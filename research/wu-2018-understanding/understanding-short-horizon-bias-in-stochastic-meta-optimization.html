<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Understanding Short-Horizon Bias in Stochastic Meta-Optimization</title>
<!--Generated on Mon Apr 10 10:50:49 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Understanding Short-Horizon Bias in Stochastic Meta-Optimization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuhuai Wu  , Mengye Ren<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span>
          <span class="ltx_tag ltx_tag_note">1</span>
          
          
        </span></span></span>  , Renjie Liao &amp; Roger B. Grosse
<br class="ltx_break">University of Toronto and Vector Institute
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">{ywu, mren, rjliao, rgrosse}@cs.toronto.edu</span>
</span><span class="ltx_author_notes">Equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes. Code available at <a href="https://github.com/renmengye/meta-optim-public" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/renmengye/meta-optim-public</a>.</p>
  
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The learning rate is one of the most important and frustrating hyperparameters to tune in deep
learning. Too small a value causes slow progress, while too large a value causes fluctuations or
even divergence. While a fixed learning rate often works well for simpler problems, good
performance on the ImageNet <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">russakovsky2015imagenet</span>)</cite> benchmark requires a carefully tuned
schedule. A variety of decay schedules have been proposed for different architectures, including
polynomial, exponential, staircase, etc. Learning rate decay is also required to achieve convergence
guarantee for stochastic gradient methods under certain conditions <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bottou1999online</span>)</cite>. Clever
learning rate heuristics have resulted in large improvements in training
efficiency <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">imagenet1hour</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">CLR</span>)</cite>. A related hyperparameter is momentum; typically fixed to a
reasonable value such as 0.9, careful tuning can also give significant performance
gains <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Sutskever13</span>)</cite>. While optimizers such as Adam <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2015adam</span>)</cite> are often described
as adapting coordinate-specific learning rates, in fact they also have global learning rate and
momentum hyperparameters analogously to SGD, and tuning at least the learning rate can be important
to good performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In light of this, it is not surprising that there have been many attempts to adapt learning rates,
either online during optimization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph1999smd</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">schaul2013nomorepesky</span>)</cite>, or offline by
fitting a learning rate schedule <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">maclaurin2015hypergrad</span>)</cite>. More ambitiously, others have
attempted to learn an optimizer
 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">l2l</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">l2o</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">MAML</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">lv2017learngd</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wichrowska2017learnopt</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">metz2017unrollgan</span>)</cite>. All of these
approaches are forms of meta-optimization, where one defines a meta-objective (typically the
expected loss after some number of optimization steps) and tunes the hyperparameters to minimize
this meta-objective. But because gradient-based meta-optimization can require thousands of updates,
each of which unrolls the entire base-level optimization procedure, the meta-optimization is
thousands of times more expensive than the base-level optimization. Therefore, the meta-objective
must be defined with a much smaller time horizon (e.g. hundreds of updates) than we are ordinarily
interested in for large-scale optimization. The hope is that the learned hyperparameters or
optimizer will generalize well to much longer time horizons. Unfortunately, we show that this is not
achieved in this paper. This is because of a strong tradeoff between short-term and long-term
performance, which we refer to as <em class="ltx_emph ltx_font_italic">short-horizon bias</em>.</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_align_floatright"><img src="x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="153" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Aggressive learning rate (red) followed by a decay schedule (yellow) wins over conservative learning rate (blue) by making more progress along the low curvature
direction (<math id="S1.F1.m2" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> direction).</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this work, we investigate the short-horizon bias both mathematically and empirically. First,
we analyze a quadratic cost function with noisy gradients based on <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">schaul2013nomorepesky</span></cite>. We
consider this a good proxy for neural net training because second-order optimization algorithms have
been shown to train neural networks in orders-of-magnitude fewer iterations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">martens_hf</span>)</cite>,
suggesting that much of the difficulty of SGD training can be explained by quadratic approximations
to the cost. In our noisy quadratic problem, the dynamics of SGD with momentum can be analyzed
exactly, allowing us to derive the greedy-optimal (i.e. 1-step horizon) learning rate and momentum in closed form,
as well as to (locally) minimize the long-horizon loss using gradient descent. We analyze the
differences between the short-horizon and long-horizon schedules.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Interestingly, when the noisy quadratic problem is <em class="ltx_emph ltx_font_italic">either</em> deterministic or spherical,
greedy schedules are optimal. However, when the problem is <em class="ltx_emph ltx_font_italic">both</em> stochastic and
badly conditioned (as is most neural net training), the greedy schedules decay the learning rate far
too quickly, leading to slow convergence towards the optimum. This is because reducing the learning
rate dampens the fluctuations along high curvature directions, giving it a large immediate reduction
in loss. But this comes at the expense of long-run performance, because the optimizer fails to make
progress along low curvature directions. This phenomenon is illustrated in
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a noisy quadratic problem in 2 dimensions, in which two learning rate
schedule are compared: a small fixed learning rate (blue), versus a larger fixed
learning rate (red) followed by exponential decay (yellow). The latter schedule initially has higher loss, but it makes more progress towards the optimum, such that it achieves an even smaller loss once the learning rate is decayed.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows this effect quantitatively for a noisy quadratic problem in 1000 dimensions (defined in Section <a href="#S2.SS3" title="2.3 Experiments ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). The solid lines show the loss after various numbers of steps of lookahead with a fixed learning rate; if this is used as the meta-objective, it favors small learning rates. The dashed curves show the loss if the same trajectories are followed by <math id="S1.p5.m1" class="ltx_Math" alttext="50" display="inline"><mn>50</mn></math> steps with an exponentially decayed learning rate; these curves favor higher learning rates, and bear little obvious relationship to the solid ones. This illustrates the difficulty of selecting learning rates based on short-horizon information.</p>
</div>
<figure id="S1.F2" class="ltx_figure ltx_align_floatright"><img src="x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="216" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Short-horizon meta-objectives for the noisy quadratic problem. Solid: loss after <math id="S1.F2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> updates with fixed learning rate. Dashed: loss after <math id="S1.F2.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> updates with fixed learning rate, followed by exponential decay.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">The second part of our paper empirically investigates gradient-based meta-optimization for neural net training. We
consider two idealized meta-optimization algorithms: an offline algorithm which fits a learning rate
decay schedule by running optimization many times from scratch, and an online algorithm which adapts
the learning rate during training. Since our interest is in studying the effect of the
meta-objective itself rather than failures of meta-optimization, we give the meta-optimizers
sufficient time to optimize their meta-objectives well. We show that short-horizon meta-optimizers,
both online and offline, dramatically underperform a hand-tuned fixed learning rate, and sometimes
cause the base-level optimization progress to slow to a crawl, even with moderately long time
horizons (e.g. 100 or 1000 steps) similar to those used in prior work on gradient-based
meta-optimization.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p">In short, we expect that any meta-objective which does not correct for short-horizon bias will
probably fail when run for a much longer time horizon than it was trained on. There are applications
where short-horizon meta-optimization is directly useful, such as few-shot
learning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mann</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ravi2017oneshot</span>)</cite>. In those settings, short-horizon bias is by definition not an
issue. But much of the appeal of meta-optimization comes from the possibility of using it to speed
up or simplify the training of large neural networks. In such settings, short-horizon bias is a
fundamental obstacle that must be addressed for meta-optimization to be practically useful.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Noisy quadratic problem</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In this section, we consider a toy problem which demonstrates the short-horizon bias and can be
analyzed analytically. In particular, we borrow the noisy quadratic model of
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">schaul2013nomorepesky</span></cite>; the true function being optimized is a quadratic, but in each iteration
we observe a noisy version with the correct curvature but a perturbed minimum.
This can be equivalently viewed as noisy observations of the gradient, which are
intended to capture the stochasticity of a mini-batch-based optimizer.
We analyze the dynamics of SGD with momentum on this example, and compare the long-horizon-optimized and
greedy-optimal learning rate schedules.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">Approximating the cost surface of a neural network with a quadratic function has led to powerful
insights and algorithms. Second-order optimization methods such as Newton-Raphson and natural
gradient <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">amari1998</span>)</cite> iteratively minimize a quadratic approximation to the cost function.
Hessian-free (H-F) optimization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">martens_hf</span>)</cite> is an approximate natural gradient method which
tries to minimize a quadratic approximation using conjugate gradient. It can often fit deep
neural networks in orders-of-magnitude fewer updates than SGD, suggesting that much of the
difficulty of neural net optimization is captured by quadratic models. In the setting of Bayesian
neural networks, quadratic approximations to the log-likelihood motivated the Laplace approximation
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">MacKay1992</span>)</cite> and variational inference <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">graves11</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">noisykfac</span>)</cite>.
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">koh2017understanding</span></cite> used quadratic approximations to analyze the
sensitivity of a neural network’s predictions to particular training labels, thereby yielding
insight into adversarial examples.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">Such quadratic approximations to the cost function have also provided insights into learning rate and momentum adaptation.
In a
deterministic setting, under certain conditions, second-order optimization algorithms can be run
with a learning rate of 1; for this reason, H-F was able to eliminate the need to tune learning rate
or momentum hyperparameters. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">K-FAC</span></cite> observed that for a deterministic quadratic cost
function, greedily choosing the learning rate and momentum to minimize the error on the next step is
equivalent to conjugate gradient (CG). Since CG achieves the minimum possible loss of any
gradient-based optimizer on each iteration, the greedily chosen learning rates and momenta are
optimal, in the sense that the greedy sequence achieves the minimum possible loss value of
<em class="ltx_emph ltx_font_italic">any</em> sequence of learning rates and momenta. This property fails to hold in the stochastic
setting, however, and as we show in this section, the greedy choice of learning rate and momentum
can do considerably worse than optimal.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">Our primary interest in this work is to adapt scalar learning rate and momentum hyperparameters
shared across all dimensions. Some optimizers based on diagonal curvature approximations
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2015adam</span>)</cite> have been motivated in terms of adapting dimension-specific learning rates,
but in practice, one still needs to tune scalar learning rate and momentum hyperparameters. Even
K-FAC <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">K-FAC</span>)</cite>, which is based on more powerful curvature approximations, has scalar learning
rate and momentum hyperparameters. Our analysis applies to all of these methods since they can be
viewed as performing SGD in a preconditioned space.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Analysis</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Notations</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">We will primarily focus on the SGD with momentum algorithm in this paper.
The update is written as follows:</p>
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1" class="ltx_Math" alttext="\displaystyle\mathbf{v}^{(t+1)}" display="inline"><msup><mi>𝐯</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\mathbf{v}^{(t)}-\alpha^{(t)}\nabla_{\bm{\theta}^{(t)}}%
\mathcal{L}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>𝐯</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0.167em">⁢</mo><mrow><msub><mo rspace="0.167em">∇</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></msub><mi class="ltx_font_mathcaligraphic">ℒ</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1" class="ltx_Math" alttext="\displaystyle\bm{\theta}^{(t+1)}" display="inline"><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m2" class="ltx_Math" alttext="\displaystyle=\bm{\theta}^{(t)}+\mathbf{v}^{(t+1)}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐯</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS2.SSS1.p1.m1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><mi class="ltx_font_mathcaligraphic">ℒ</mi></math> is the loss function, <math id="S2.SS2.SSS1.p1.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is the training step, and <math id="S2.SS2.SSS1.p1.m3" class="ltx_Math" alttext="\alpha^{(t)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> is the
learning rate. We call the gradient trace <math id="S2.SS2.SSS1.p1.m4" class="ltx_Math" alttext="\mathbf{v}^{(t)}" display="inline"><msup><mi>𝐯</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> “velocity”, and
its decay constant <math id="S2.SS2.SSS1.p1.m5" class="ltx_Math" alttext="\mu^{(t)}" display="inline"><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> “momentum”. We denote the <math id="S2.SS2.SSS1.p1.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>th coordinate of a vector <math id="S2.SS2.SSS1.p1.m7" class="ltx_Math" alttext="\mathbf{v}" display="inline"><mi>𝐯</mi></math>
as <math id="S2.SS2.SSS1.p1.m8" class="ltx_Math" alttext="v_{i}" display="inline"><msub><mi>v</mi><mi>i</mi></msub></math>. When we focus on a single dimension, we sometimes drop the dimension subscripts. We also denote <math id="S2.SS2.SSS1.p1.m9" class="ltx_Math" alttext="A(\cdot)=\mathbb{E}[\cdot]^{2}+\mathbb{V}[\cdot]" display="inline"><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">]</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></math>, where <math id="S2.SS2.SSS1.p1.m10" class="ltx_Math" alttext="\mathbb{E}" display="inline"><mi>𝔼</mi></math> and <math id="S2.SS2.SSS1.p1.m11" class="ltx_Math" alttext="\mathbb{V}" display="inline"><mi>𝕍</mi></math> denote expectation and
variance respectively.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Problem formulation</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p">We now define the noisy quadratic model, where in each iteration, the optimizer is given the gradient for a noisy version of a quadratic cost function, where the curvature is correct but the minimum is sampled stochastically from a Gaussian distribution. We assume WLOG that the Hessian is diagonal because SGD is a rotation invariant algorithm, and therefore the dynamics can be analyzed in a coordinate system corresponding to the eigenvectors of the Hessian. We make the further (nontrivial) assumption that the noise covariance is also diagonal.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
                <span class="ltx_tag ltx_tag_note">1</span>
                
                
              This amounts to assuming that the Hessian and the noise covariance are codiagonalizable. One heuristic justification for this assumption in the context of neural net optimization is that under certain assumptions, the covariance of the gradients is proportional to the Fisher information matrix, which is close to the Hessian <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">martens_natural_gradient</span>)</cite>.</span></span></span> Mathematically, the stochastic cost function is written as:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1" class="ltx_Math" alttext="\hat{\mathcal{L}}(\bm{\theta})=\frac{1}{2}\sum_{i}h_{i}(\theta_{i}-c_{i})^{2}," display="block"><mrow><mrow><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS2.SSS2.p1.m1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math> is the stochastic minimum, and each <math id="S2.SS2.SSS2.p1.m2" class="ltx_Math" alttext="c_{i}" display="inline"><msub><mi>c</mi><mi>i</mi></msub></math> follows a Gaussian
distribution with mean <math id="S2.SS2.SSS2.p1.m3" class="ltx_Math" alttext="\theta_{i}^{*}" display="inline"><msubsup><mi>θ</mi><mi>i</mi><mo>*</mo></msubsup></math> and variance <math id="S2.SS2.SSS2.p1.m4" class="ltx_Math" alttext="\sigma_{i}^{2}" display="inline"><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></math>. The expected loss is given by:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1" class="ltx_Math" alttext="\mathcal{L}(\bm{\theta})=\mathbb{E}\left[\hat{\mathcal{L}}(\bm{\theta})\right]%
=\frac{1}{2}\sum_{i}h_{i}\left((\theta_{i}-\theta_{i}^{*})^{2}+\sigma_{i}^{2}%
\right)." display="block"><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>θ</mi><mi>i</mi><mo>*</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The optimum of <math id="S2.SS2.SSS2.p1.m5" class="ltx_Math" alttext="\mathcal{L}" display="inline"><mi class="ltx_font_mathcaligraphic">ℒ</mi></math> is given by <math id="S2.SS2.SSS2.p1.m6" class="ltx_Math" alttext="\bm{\theta}^{*}=\mathbb{E}[\mathbf{c}]" display="inline"><mrow><msup><mi>𝜽</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mi>𝐜</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></math>; we assume WLOG that <math id="S2.SS2.SSS2.p1.m7" class="ltx_Math" alttext="\bm{\theta}^{*}=\mathbf{0}" display="inline"><mrow><msup><mi>𝜽</mi><mo>*</mo></msup><mo>=</mo><mn>𝟎</mn></mrow></math>.
The stochastic gradient is given by
<math id="S2.SS2.SSS2.p1.m8" class="ltx_Math" alttext="\frac{\partial\hat{\mathcal{L}}}{\partial\theta_{i}}=h_{i}(\theta_{i}-c_{i})" display="inline"><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover></mrow><mrow><mo rspace="0em">∂</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>.
Since the deterministic gradient is given by <math id="S2.SS2.SSS2.p1.m9" class="ltx_Math" alttext="\frac{\partial\mathcal{L}}{\partial\theta_{i}}=h_{i}\theta_{i}" display="inline"><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mi class="ltx_font_mathcaligraphic">ℒ</mi></mrow><mrow><mo rspace="0em">∂</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mrow></math>, the stochastic gradient can be viewed as a noisy Gaussian observation of the deterministic gradient with variance <math id="S2.SS2.SSS2.p1.m10" class="ltx_Math" alttext="h_{i}^{2}\sigma_{i}^{2}" display="inline"><mrow><msubsup><mi>h</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></math>. This interpretation motivates the use of this noisy quadratic problem as a model of SGD dynamics.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p">We treat the iterate <math id="S2.SS2.SSS2.p2.m1" class="ltx_Math" alttext="\bm{\theta}^{(t)}" display="inline"><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> as a random variable (where the randomness comes from the sampled <math id="S2.SS2.SSS2.p2.m2" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math>’s); the expected loss in each iteration is given by</p>
<table id="A1.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E5.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[\mathcal{L}(\bm{\theta}^{(t)})\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E5.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}\left[\frac{1}{2}\sum_{i}h_{i}\left((\theta^{(t)}_{i})%
^{2}+\sigma_{i}^{2}\right)\right]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S2.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E6.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}\sum_{i}h_{i}\left(\mathbb{E}\left[\theta_{i}^{(t)}%
\right]^{2}+\mathbb{V}\left[\theta_{i}^{(t)}\right]+\sigma_{i}^{2}\right)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo>[</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>]</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>]</mo></mrow></mrow><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Optimized and Greedy-Optimal Schedules</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p">We are interested in adapting a global learning rate <math id="S2.SS2.SSS3.p1.m1" class="ltx_Math" alttext="\alpha^{(t)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> and a global momentum decay parameter <math id="S2.SS2.SSS3.p1.m2" class="ltx_Math" alttext="\mu^{(t)}" display="inline"><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> for each time step <math id="S2.SS2.SSS3.p1.m3" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>. We first derive a recursive formula for the mean and variance of the iterates at each step, and then analyze the greedy-optimal schedule for <math id="S2.SS2.SSS3.p1.m4" class="ltx_Math" alttext="\alpha^{(t)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> and <math id="S2.SS2.SSS3.p1.m5" class="ltx_Math" alttext="\mu^{(t)}" display="inline"><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math>.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p">Several observations allow us to compactly model the dynamics of SGD with momentum on the
noisy quadratic model. First, <math id="S2.SS2.SSS3.p2.m1" class="ltx_Math" alttext="\mathbb{E}[\mathcal{L}(\bm{\theta}^{(t)})]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></math> can be expressed in terms of
<math id="S2.SS2.SSS3.p2.m2" class="ltx_Math" alttext="\mathbb{E}[\theta_{i}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">]</mo></mrow></mrow></math> and <math id="S2.SS2.SSS3.p2.m3" class="ltx_Math" alttext="\mathbb{V}[\theta_{i}]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">]</mo></mrow></mrow></math> using Eqn. <a href="#S2.E5" title="5 ‣ 2.2.2 Problem formulation ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Second, due to the diagonality of the Hessian and the noise
covariance matrix, each coordinate evolves independently of the others. Third, the means and variances of the parameters <math id="S2.SS2.SSS3.p2.m4" class="ltx_Math" alttext="\theta_{i}^{(t)}" display="inline"><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></math> and the velocity <math id="S2.SS2.SSS3.p2.m5" class="ltx_Math" alttext="v_{i}^{(t)}" display="inline"><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></math> are functions of
those statistics at the previous step.</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para">
<p class="ltx_p">Because each dimension evolves independently, we now drop the dimension subscripts.
Combining these observations, we model the dynamics of SGD with momentum as a <em class="ltx_emph ltx_font_italic">deterministic</em>
recurrence relation with sufficient statistics
<math id="S2.SS2.SSS3.p3.m1" class="ltx_Math" alttext="\mathbb{E}[\theta^{(t)}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math>, <math id="S2.SS2.SSS3.p3.m2" class="ltx_Math" alttext="\mathbb{E}[v^{(t)}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math>, <math id="S2.SS2.SSS3.p3.m3" class="ltx_Math" alttext="\mathbb{V}[\theta^{(t)}]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math>, <math id="S2.SS2.SSS3.p3.m4" class="ltx_Math" alttext="\mathbb{V}[v^{(t)}]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math>, and <math id="S2.SS2.SSS3.p3.m5" class="ltx_Math" alttext="\Sigma^{(t)}_{\theta,v}=\mathrm{Cov}(\theta^{(t)},v^{(t)})" display="inline"><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>,</mo><mi>v</mi></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></math>.
The dynamics are as follows:</p>
</div>
<div id="Thmtheorem1" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Mean and variance dynamics)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The expectations of the parameter <math id="Thmtheorem1.p1.m1" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math> and the velocity <math id="Thmtheorem1.p1.m2" class="ltx_Math" alttext="v" display="inline"><mi>v</mi></math> are updated as,</span></p>
<table id="A1.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex1.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[v^{(t+1)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex1.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\mathbb{E}\left[v^{(t)}\right]-(\alpha^{(t)}h)\mathbb{E%
}\left[\theta^{(t)}\right]," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[\theta^{(t+1)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}\left[\theta^{(t)}\right]+\mathbb{E}\left[v^{(t+1)}%
\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody><tr class="ltx_eqn_row ltx_align_baseline"><td class="ltx_eqn_cell ltx_align_left" style="white-space:normal;" colspan="5">
<span class="ltx_text ltx_font_italic">The variances of the parameter </span><math id="A1.EGx3.m1" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math><span class="ltx_text ltx_font_italic"> and the velocity </span><math id="A1.EGx3.m2" class="ltx_Math" alttext="v" display="inline"><mi>v</mi></math><span class="ltx_text ltx_font_italic"> are updated as</span>
</td></tr></tbody>
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex3.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[v^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex3.m2" class="ltx_Math" alttext="\displaystyle=\left(\mu^{(t)}\right)^{2}\mathbb{V}\left[v^{(t)}\right]+\left(%
\alpha^{(t)}h\right)^{2}\mathbb{V}\left[\theta^{(t)}\right]-2\mu^{(t)}\alpha^{%
(t)}h\Sigma^{(t)}_{\theta,v}+\left(\alpha^{(t)}h\sigma\right)^{2}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mrow><msup><mrow><mo>(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo>(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>−</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>,</mo><mi>v</mi></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><mo>+</mo><msup><mrow><mo>(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[\theta^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex4.m2" class="ltx_Math" alttext="\displaystyle=\left(1-2\alpha^{(t)}h\right)\mathbb{V}\left[\theta^{(t)}\right]%
+\mathbb{V}\left[v^{(t+1)}\right]+2\mu^{(t)}\Sigma^{(t)}_{\theta,v}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>,</mo><mi>v</mi></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex5.m1" class="ltx_Math" alttext="\displaystyle\Sigma^{(t+1)}_{\theta,v}" display="inline"><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>,</mo><mi>v</mi></mrow><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\Sigma^{(t)}_{\theta,v}-\alpha^{(t)}h\mathbb{V}\left[%
\theta^{(t)}\right]+\mathbb{V}\left[v^{(t+1)}\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>,</mo><mi>v</mi></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S2.SS2.SSS3.p4" class="ltx_para">
<p class="ltx_p">By applying Theorem <a href="#Thmtheorem1" title="Theorem 1 (Mean and variance dynamics). ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> recursively, we can obtain <math id="S2.SS2.SSS3.p4.m1" class="ltx_Math" alttext="\mathbb{E}[\bm{\theta}^{(t)}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math> and <math id="S2.SS2.SSS3.p4.m2" class="ltx_Math" alttext="\mathbb{V}[\bm{\theta}^{(t)}]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></math>, and hence <math id="S2.SS2.SSS3.p4.m3" class="ltx_Math" alttext="\mathbb{E}[\mathcal{L}(\bm{\theta}^{(t)})]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></math>, for every <math id="S2.SS2.SSS3.p4.m4" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>. Therefore, using gradient-based optimization, we can fit a locally optimal learning rate and momentum schedule, i.e. a sequence of values <math id="S2.SS2.SSS3.p4.m5" class="ltx_Math" alttext="\{(\alpha^{(t)},\mu^{(t)})\}_{t=1}^{T}" display="inline"><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup></math> which locally minimizes <math id="S2.SS2.SSS3.p4.m6" class="ltx_Math" alttext="\mathbb{E}[\mathcal{L}(\bm{\theta}^{(t)})]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></math> at some particular time <math id="S2.SS2.SSS3.p4.m7" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>. We refer to this as the <em class="ltx_emph ltx_font_italic">optimized</em> schedule.</p>
</div>
<div id="S2.SS2.SSS3.p5" class="ltx_para">
<p class="ltx_p">Furthermore, there is a closed-form solution for one-step lookahead, i.e., we can
solve for the optimal learning rate <math id="S2.SS2.SSS3.p5.m1" class="ltx_Math" alttext="\alpha^{(t)*}" display="inline"><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math> and momentum <math id="S2.SS2.SSS3.p5.m2" class="ltx_Math" alttext="\mu^{(t)*}" display="inline"><msup><mi>μ</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math> that minimizes
<math id="S2.SS2.SSS3.p5.m3" class="ltx_Math" alttext="\mathbb{E}[\mathcal{L}(\bm{\theta}^{(t+1)})]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝜽</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></math> given the statistics at time <math id="S2.SS2.SSS3.p5.m4" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>. We call this as the <span class="ltx_text ltx_font_italic">greedy-optimal</span>
schedule.</p>
</div>
<div id="Thmtheorem2" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2</span></span><span class="ltx_text ltx_font_bold"> </span>(Greedy-optimal learning rate and momentum)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The greedy-optimal learning rate and momentum schedule is given by</span></p>
<table id="A1.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex6.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)*}=\frac{\sum_{i}h_{i}^{2}A\left(\theta^{(t)}_{i}%
\right)\left[\sum_{j}h_{j}A\left(v^{(t)}_{j}\right)\right]-\left(\sum_{j}h_{j}%
\mathbb{E}\left[\theta^{(t)}_{j}v^{(t)}_{j}\right]\right)h_{i}^{2}\ \mathbb{E}%
\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]}{\sum_{i}h_{i}^{3}\left[A\left(\theta%
^{(t)}_{i}\right)+\sigma_{i}^{2}\right]\left[\sum_{j}h_{j}A\left(v^{(t)}_{j}%
\right)\right]-\left(\sum_{j}h_{j}^{2}\ \mathbb{E}\left[\theta^{(t)}_{j}v^{(t)%
}_{j}\right]\right)h_{i}^{2}\ \mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}%
\right]}," display="inline"><mrow><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msubsup><mi>h</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>)</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mo>(</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><msubsup><mi>h</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msubsup><mi>h</mi><mi>i</mi><mn>3</mn></msubsup><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>)</mo></mrow></mrow><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mo>(</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msubsup><mi>h</mi><mi>j</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><msubsup><mi>h</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex7.m1" class="ltx_Math" alttext="\displaystyle\mu^{(t)*}=-\frac{\sum_{i}h_{i}\left(1-\alpha^{(t)*}h_{i}\right)%
\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]}{\sum_{i}h_{i}A\left(v^{(t)%
}_{i}\right)}." display="inline"><mrow><mrow><msup><mi>μ</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S2.SS2.SSS3.p6" class="ltx_para">
<p class="ltx_p">Note that <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">schaul2013nomorepesky</span></cite> derived the greedy optimal learning rate for SGD, and Theorem <a href="#Thmtheorem2" title="Theorem 2 (Greedy-optimal learning rate and momentum). ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> extends it to the greedy optimal learning rate and momentum for SGD with momentum.
</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top"><img src="x3.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="374" height="227" alt="Refer to caption"></p>
</td>
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top"><img src="x4.png" id="S2.F3.g2" class="ltx_graphics ltx_img_landscape" width="374" height="221" alt="Refer to caption"></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top">
 <img src="x5.png" id="S2.F3.g3" class="ltx_graphics ltx_img_landscape" width="369" height="116" alt="Refer to caption"></p>
</td>
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top">  <img src="x6.png" id="S2.F3.g4" class="ltx_graphics ltx_img_landscape" width="374" height="125" alt="Refer to caption"></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top"> 
<img src="x7.png" id="S2.F3.g5" class="ltx_graphics ltx_img_landscape" width="370" height="123" alt="Refer to caption"></p>
</td>
<td class="ltx_td ltx_align_justify" style="width:195.1pt;">
<p class="ltx_p ltx_align_top">   <img src="x8.png" id="S2.F3.g6" class="ltx_graphics ltx_img_landscape" width="366" height="122" alt="Refer to caption"></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:195.1pt;"><span class="ltx_text ltx_align_top" style="font-size:90%;">                                (a)</span></td>
<td class="ltx_td ltx_align_justify" style="width:195.1pt;"><span class="ltx_text ltx_align_top" style="font-size:90%;">                                (b)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text" style="font-size:90%;">Comparisons of the optimized learning rates and momenta trained by gradient descent (red), greedy learning rates and momenta (blue), and the optimized fixed learning rate and momentum (green) in both noisy (a) and deterministic (b) quadratic settings. In the deterministic case, our optimized schedule matched the greedy one, just as the theory predicts.</span></figcaption>
</figure>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Univariate and Spherical Cases</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p class="ltx_p">As noted in Section <a href="#S2.SS1" title="2.1 Background ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">K-FAC</span></cite> found the greedy choice of <math id="S2.SS2.SSS4.p1.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> and <math id="S2.SS2.SSS4.p1.m2" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> to be optimal for gradient descent on deterministic quadratic objectives. We now show that the greedy schedule is also optimal for SGD <em class="ltx_emph ltx_font_italic">without</em> momentum in the case of univariate noisy quadratics, and hence also for multivariate ones with spherical Hessians and gradient covariances.
In particular, the following holds for SGD without momentum on a univariate noisy quadratic:</p>
</div>
<div id="Thmtheorem3" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3</span></span><span class="ltx_text ltx_font_bold"> </span>(Optimal learning rate, univariate)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For all <math id="Thmtheorem3.p1.m1" class="ltx_Math" alttext="T\in\mathbb{N}" display="inline"><mrow><mi>T</mi><mo mathvariant="normal">∈</mo><mi>ℕ</mi></mrow></math>, the sequence of learning rates <math id="Thmtheorem3.p1.m2" class="ltx_Math" alttext="\{\alpha^{(t)*}\}_{t=1}^{T-1}" display="inline"><msubsup><mrow><mo mathvariant="normal" stretchy="false">{</mo><msup><mi>α</mi><mrow><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>t</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow><mo lspace="0.222em" mathvariant="italic">⁣</mo><mo mathvariant="normal">*</mo></mrow></msup><mo mathvariant="normal" stretchy="false">}</mo></mrow><mrow><mi>t</mi><mo mathvariant="normal">=</mo><mn mathvariant="normal">1</mn></mrow><mrow><mi>T</mi><mo mathvariant="normal">−</mo><mn mathvariant="normal">1</mn></mrow></msubsup></math> that minimizes
<math id="Thmtheorem3.p1.m3" class="ltx_Math" alttext="\mathcal{L}(\theta^{(T)})" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>T</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></msup><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></math> is given by</span></p>
<table id="S2.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E7.m1" class="ltx_Math" alttext="\alpha^{(t)*}=\frac{A\left(\theta^{(t)}\right)}{h\left(A\left(\theta^{(t)}%
\right)+\sigma^{2}\right)}." display="block"><mrow><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mfrac><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo>)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Moreover, this agrees with the greedy-optimal learning rate schedule as derived by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">schaul2013nomorepesky</span></cite>.</span></p>
</div>
</div>
<div id="S2.SS2.SSS4.p2" class="ltx_para">
<p class="ltx_p">If the Hessian and the gradient covariance are both spherical, then each dimension evolves identically and independently according to the univariate dynamics. Of course, one is unlikely to encounter an optimization problem where both are exactly spherical. But some approximate second-order optimizers, such as K-FAC, can be viewed as preconditioned SGD, i.e. SGD in a transformed space where the Hessian and the gradient covariance are better conditioned <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">K-FAC</span>)</cite>. In principle, with a good enough preconditioner, the Hessian and the gradient covariance would be close enough to spherical that a greedy choice of <math id="S2.SS2.SSS4.p2.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> and <math id="S2.SS2.SSS4.p2.m2" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> would perform well. It will be interesting to investigate whether any practical optimization algorithms demonstrate this behavior.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Experiments</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">In this section, we compare the optimized and greedy-optimal schedules on a noisy quadratic problem.
We chose a 1000 dimensional quadratic cost function with the curvature distribution from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">Li05sharpnessin</span></cite>, on which CG achieves its worst-case convergence rate.
We assume that <math id="S2.SS3.p1.m1" class="ltx_Math" alttext="h_{i}=\mathbb{V}[\frac{\partial\mathcal{L}}{{\partial\theta_{i}}}]" display="inline"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mfrac><mrow><mo rspace="0em">∂</mo><mi class="ltx_font_mathcaligraphic">ℒ</mi></mrow><mrow><mo rspace="0em">∂</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">]</mo></mrow></mrow></mrow></math>, and hence <math id="S2.SS3.p1.m2" class="ltx_Math" alttext="\sigma_{i}^{2}=\frac{1}{h_{i}}" display="inline"><mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><msub><mi>h</mi><mi>i</mi></msub></mfrac></mrow></math>; this choice is motivated by the observations that under certain assumptions, the Fisher information matrix is a good approximation to the Hessian matrix, but also reflects the covariance structure of the gradient noise <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">martens_natural_gradient</span>)</cite>. We computed the greedy-optimal schedules using Theorem <a href="#Thmtheorem3" title="Theorem 3 (Optimal learning rate, univariate). ‣ 2.2.4 Univariate and Spherical Cases ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For the optimized schedules, we minimized the expected loss at time <math id="S2.SS3.p1.m3" class="ltx_Math" alttext="T=250" display="inline"><mrow><mi>T</mi><mo>=</mo><mn>250</mn></mrow></math> using Adam using Adam <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2015adam</span>)</cite>, with a learning rate 0.003 and 500 steps. We set an upper bound for the learning rate which prevented the loss component for any dimension from becoming larger than its initial value; this was needed because otherwise the optimized schedule allowed the loss to temporarily grow very large, a pathological solution which would be unstable on realistic problems. We also considered fixed learning rate and momentum, with the two hyperparameters fit using Adam. The training curves and the corresponding learning rates and momenta are shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a). The optimized schedule achieved a much lower final expected loss value (4.25) than was obtained by the greedy-optimal schedule (63.86) or fixed schedule (42.19).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">We also show the sums of the losses along the 50 highest curvature directions and 50 lowest curvature directions. We find that under the optimized schedule, the losses along the high curvature directions hardly decrease initially. However, because it maintains a high learning rate, the losses along the low curvature directions decrease significantly. After 50 iterations, it begins decaying the learning rate, at which point it achieves a large drop in both the high-curvature and total losses. On the other hand, under the greedy-optimal schedule, the learning rates and momenta become small very early on, which immediately reduces the losses on the high curvature directions, and hence also the total loss. However, in the long term, since the learning rates are too small to make substantial progress along the low curvature directions, the total loss converged to a much higher value in the end. This gives valuable insight into the nature of the short-horizon bias in meta-optimization: short-horizon objectives will often encourage the learning rate and momentum to decay quickly, so as to achieve the largest gain in the short term, but at the expense of long-run performance.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p">It is interesting to compare this behavior with the deterministic case. We repeated the above experiment for a <em class="ltx_emph ltx_font_italic">deterministic</em> quadratic cost function (i.e. <math id="S2.SS3.p3.m1" class="ltx_Math" alttext="\sigma_{i}^{2}=0" display="inline"><mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><mn>0</mn></mrow></math>) with the same Hessian; results are shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b). The greedy schedule matches the optimized one, as predicted by the analysis of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">K-FAC</span></cite>. This result illustrates that stochasticity is necessary for short-horizon bias to manifest. Interestingly, the learning rate and momentum schedules in the deterministic case are nearly flat, while the optimized schedules for the stochastic case are much more complex, suggesting that stochastic optimization raises a different set of issues for hyperparameter adaptation.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Gradient-Based Meta-Optimization</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We now turn our attention to gradient-based hyperparameter optimization. A variety of approaches
have been proposed which tune hyperparameters by doing gradient descent on a meta-objective
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph1999smd</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">maclaurin2015hypergrad</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">l2l</span>)</cite>. We empirically analyze an idealized version
of a gradient-based meta-optimization algorithm called stochastic meta-descent (SMD)
 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph1999smd</span>)</cite>. Our version of SMD is idealized in two ways: first, we drop the
algorithmic tricks used in prior work, and instead allow the meta-optimizer more memory and
computation than would be economical in practice. Second, we limit the representational power of our
meta-model: whereas <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">l2l</span></cite> aimed to learn a full optimization algorithm, we focus on the much
simpler problem of adapting learning rate and momentum hyperparameters, or schedules thereof. The
aim of these two simplifications is that we would like to do a good enough job of optimizing the
meta-objective that any base-level optimization failures can be attributed to deficiencies in the
meta-objective itself (such as short-horizon bias) rather than incomplete meta-optimization.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Despite these simplifications, we believe our experiments are relevant to practical
meta-optimization algorithms which optimize the meta-objective less thoroughly. Since the goal of
the meta-optimizer is to adapt two hyperparameters, it’s possible that poor meta-optimization could
cause the hyperparameters to get stuck in regions that happen to perform well; indeed, we observed
this phenomenon in some of our early explorations.
But
it would be dangerous to rely on poor meta-optimization, since improved meta-optimization methods
would then lead to worse base-level performance, and tuning the meta-optimizer could become a roundabout
way of tuning learning rates and momenta.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">We also believe our experiments are relevant to meta-optimization methods which aim to learn entire
algorithms. Even if the learned algorithms don’t have explicit learning rate parameters, it’s
possible for a learning rate schedule to be encoded into an algorithm itself; for instance, Adagrad
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">duchi2011adagrad</span>)</cite> implicitly uses a polynomial decay schedule because it sums rather than
averages the squared derivatives in the denominator. Hence, one would need to worry about whether
the meta-optimizer is implicitly fitting a learning rate schedule that’s optimized for short-term
performance.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background: Stochastic Meta-Descent</h3>

<figure id="S3.F4" class="ltx_figure">
<p class="ltx_p ltx_align_center"><svg version="1.1" height="156" width="419" overflow="visible"><g transform="translate(0,156) scale(1,-1)"><rect fill="none" height="112.7pt" stroke="black" stroke-width="0.4" width="302.8pt" x="0" y="0"></rect><g class="makebox" transform="translate(0,0)"><g transform="translate(0,181) scale(1, -1)"><foreignObject width="419" height="181" overflow="visible"><img src="x9.png" id="S3.F4.pic1.g1" class="ltx_graphics ltx_img_landscape" width="419" height="181" alt="Refer to caption"></foreignObject></g></g></g></svg></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Regular SGD in the form of a computation graph. The learning rate parameter <math id="S3.F4.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is
part of the differentiable computations.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">The high-level idea of stochastic meta-descent (SMD) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph1999smd</span>)</cite> is to perform
gradient descent on the learning rate, or any other differentiable hyperparameters. This is
feasible since any gradient based optimization algorithm can be unrolled as a computation graph (see
Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 Background: Stochastic Meta-Descent ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), and automatic differentiation is readily available in most deep
learning libraries.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">There are two basic types of automatic differentiation (autodiff) methods:
<span class="ltx_text ltx_font_italic">forward mode</span> and <span class="ltx_text ltx_font_italic">reverse mode</span>. In forward mode autodiff, directional derivatives are computed alongside
the forward computation. In contrast, reverse mode autodiff (a.k.a. backpropagation)
computes the gradients moving backwards through the computation graph. Meta-optimization using reverse mode can be
computationally demanding due to memory constraints, since the parameters need to be
stored at every step. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">maclaurin2015hypergrad</span></cite> got around this by cleverly exploiting approximate reversibility to
minimize the memory cost of activations. Since we are optimizing only two hyperparameters, however, forward
mode autodiff can be done cheaply. Here, we provide the forward differentiation equations
for obtaining the gradient of vanilla SGD learning rate. Let <math id="S3.SS1.p2.m1" class="ltx_Math" alttext="\frac{d\bm{\theta}_{t}}{d\alpha}" display="inline"><mfrac><mrow><mi>d</mi><mo>⁢</mo><msub><mi>𝜽</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mo>⁢</mo><mi>α</mi></mrow></mfrac></math> be
<math id="S3.SS1.p2.m2" class="ltx_Math" alttext="\bm{u}_{t}" display="inline"><msub><mi>𝒖</mi><mi>t</mi></msub></math>, and <math id="S3.SS1.p2.m3" class="ltx_Math" alttext="\frac{d\mathcal{L}_{t}}{d\alpha}" display="inline"><mfrac><mrow><mi>d</mi><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mo>⁢</mo><mi>α</mi></mrow></mfrac></math> be <math id="S3.SS1.p2.m4" class="ltx_Math" alttext="\alpha^{\prime}" display="inline"><msup><mi>α</mi><mo>′</mo></msup></math>, and the Hessian at step <math id="S3.SS1.p2.m5" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> to be
<math id="S3.SS1.p2.m6" class="ltx_Math" alttext="H_{t}" display="inline"><msub><mi>H</mi><mi>t</mi></msub></math>. By chain rule, we get,</p>
<table id="A1.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E8.m1" class="ltx_Math" alttext="\displaystyle\alpha^{\prime}" display="inline"><msup><mi>α</mi><mo>′</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E8.m2" class="ltx_Math" alttext="\displaystyle=\bm{g}_{t}\cdot\bm{u}_{t-1}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>𝒈</mi><mi>t</mi></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝒖</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1" class="ltx_Math" alttext="\displaystyle\bm{u}_{t}" display="inline"><msub><mi>𝒖</mi><mi>t</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m2" class="ltx_Math" alttext="\displaystyle=\bm{u}_{t-1}-\bm{g}_{t}-\alpha H_{t}\bm{u}_{t-1}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>𝒖</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝒈</mi><mi>t</mi></msub><mo>−</mo><mrow><mi>α</mi><mo>⁢</mo><msub><mi>H</mi><mi>t</mi></msub><mo>⁢</mo><msub><mi>𝒖</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">While the Hessian is infeasible to construct explicitly, the Hessian-vector product in Equation <a href="#S3.E9" title="9 ‣ 3.1 Background: Stochastic Meta-Descent ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> can be computed efficiently using reverse-on-reverse <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">werbos1988backprop</span>)</cite> or forward-on-reverse automatic differentiation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">pearlmutter1994hessian</span>)</cite>, in time linear in the cost of the forward pass. See <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph2002smdmv</span></cite> for more details.</p>
</div>
<figure id="S3.SS1.fig1" class="ltx_figure">
<figure id="algorithm1" class="ltx_float ltx_minipage ltx_align_middle" style="width:225.5pt;">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline">       <span class="ltx_text" style="font-size:90%;color:#000000;"><span class="ltx_text ltx_font_bold">Input:</span> </span><math id="algorithm1.m1" class="ltx_Math" alttext="\alpha_{0},\eta,\bm{\theta},T,M" display="inline"><mrow><msub><mi mathsize="90%">α</mi><mn mathsize="90%">0</mn></msub><mo mathsize="90%">,</mo><mi mathsize="90%">η</mi><mo mathsize="90%">,</mo><mi mathsize="90%">𝜽</mi><mo mathsize="90%">,</mo><mi mathsize="90%">T</mi><mo mathsize="90%">,</mo><mi mathsize="90%">M</mi></mrow></math>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text" style="font-size:90%;color:#000000;"><span class="ltx_text ltx_font_bold">Output:</span> </span><math id="algorithm1.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi mathsize="90%">α</mi></math>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m3" class="ltx_Math" alttext="\bm{\theta}_{0}\leftarrow\bm{\theta}" display="inline"><mrow><msub><mi mathsize="90%">𝜽</mi><mn mathsize="90%">0</mn></msub><mo mathsize="90%" stretchy="false">←</mo><mi mathsize="90%">𝜽</mi></mrow></math><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m4" class="ltx_Math" alttext="\alpha\leftarrow\alpha_{0}" display="inline"><mrow><mi mathsize="90%">α</mi><mo mathsize="90%" stretchy="false">←</mo><msub><mi mathsize="90%">α</mi><mn mathsize="90%">0</mn></msub></mrow></math><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span class="ltx_text" style="font-size:90%;"> </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;"><math id="algorithm1.m5" class="ltx_Math" alttext="m\leftarrow 1" display="inline"><mrow><mi>m</mi><mo mathvariant="normal" stretchy="false">←</mo><mn mathvariant="normal">1</mn></mrow></math> … <math id="algorithm1.m6" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math></em><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m7" class="ltx_Math" alttext="\bm{u}\leftarrow\bm{0}" display="inline"><mrow><mi mathsize="90%">𝒖</mi><mo mathsize="90%" stretchy="false">←</mo><mn mathsize="90%">𝟎</mn></mrow></math><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span class="ltx_text" style="font-size:90%;"> </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;"><math id="algorithm1.m8" class="ltx_Math" alttext="t\leftarrow 1" display="inline"><mrow><mi>t</mi><mo mathvariant="normal" stretchy="false">←</mo><mn mathvariant="normal">1</mn></mrow></math> … <math id="algorithm1.m9" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math></em><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">do</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m10" class="ltx_Math" alttext="X,\bm{y}\leftarrow" display="inline"><mrow><mrow><mi mathsize="90%">X</mi><mo mathsize="90%">,</mo><mi mathsize="90%">𝒚</mi></mrow><mo mathsize="90%" stretchy="false">←</mo><mi></mi></mrow></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">GetMiniBatch(<em class="ltx_emph ltx_font_serif ltx_font_italic"></em>)</span><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m11" class="ltx_Math" alttext="\bm{g}\leftarrow" display="inline"><mrow><mi mathsize="90%">𝒈</mi><mo mathsize="90%" stretchy="false">←</mo><mi></mi></mrow></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">BGrad(<em class="ltx_emph ltx_font_serif ltx_font_italic">L(<math id="algorithm1.m12" class="ltx_math_unparsed" alttext="X,\bm{y},\bm{\theta}),\bm{\theta}" display="inline"><mrow><mi>X</mi><mo mathvariant="normal">,</mo><mi>𝐲</mi><mo mathvariant="normal">,</mo><mi>𝛉</mi><mo mathvariant="normal" stretchy="false">)</mo><mo mathvariant="normal">,</mo><mi>𝛉</mi></mrow></math>, <math id="algorithm1.m13" class="ltx_Math" alttext="1" display="inline"><mn mathvariant="normal">1</mn></math></em>)</span><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m14" class="ltx_Math" alttext="\bm{\theta}_{\textit{new}}\leftarrow" display="inline"><mrow><msub><mi mathsize="90%">𝜽</mi><mtext mathsize="90%">𝑛𝑒𝑤</mtext></msub><mo mathsize="90%" stretchy="false">←</mo><mi></mi></mrow></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">Step(<em class="ltx_emph ltx_font_serif ltx_font_italic"><math id="algorithm1.m15" class="ltx_Math" alttext="\bm{\theta},\bm{g},\alpha" display="inline"><mrow><mi>𝛉</mi><mo mathvariant="normal">,</mo><mi>𝐠</mi><mo mathvariant="normal">,</mo><mi>α</mi></mrow></math></em>)</span><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m16" class="ltx_Math" alttext="\alpha^{\prime}\leftarrow\bm{g}\cdot\bm{u}" display="inline"><mrow><msup><mi mathsize="90%">α</mi><mo mathsize="90%">′</mo></msup><mo mathsize="90%" stretchy="false">←</mo><mrow><mi mathsize="90%">𝒈</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em">⋅</mo><mi mathsize="90%">𝒖</mi></mrow></mrow></math><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m17" class="ltx_Math" alttext="\bm{u}\leftarrow" display="inline"><mrow><mi mathsize="90%">𝒖</mi><mo mathsize="90%" stretchy="false">←</mo><mi></mi></mrow></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">FGrad(<em class="ltx_emph ltx_font_serif ltx_font_italic"><math id="algorithm1.m18" class="ltx_Math" alttext="\bm{\theta}_{\text{new}},[\alpha,\bm{\theta}],[1,\bm{u}]" display="inline"><mrow><msub><mi>𝛉</mi><mtext><em class="ltx_emph" style="font-size:70%;">new</em></mtext></msub><mo mathvariant="normal">,</mo><mrow><mo mathvariant="normal" stretchy="false">[</mo><mi>α</mi><mo mathvariant="normal">,</mo><mi>𝛉</mi><mo mathvariant="normal" stretchy="false">]</mo></mrow><mo mathvariant="normal">,</mo><mrow><mo mathvariant="normal" stretchy="false">[</mo><mn mathvariant="normal">1</mn><mo mathvariant="normal">,</mo><mi>𝐮</mi><mo mathvariant="normal" stretchy="false">]</mo></mrow></mrow></math></em>)</span><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m19" class="ltx_Math" alttext="\bm{\theta}\leftarrow\bm{\theta}_{\textit{new}}" display="inline"><mrow><mi mathsize="90%">𝜽</mi><mo mathsize="90%" stretchy="false">←</mo><msub><mi mathsize="90%">𝜽</mi><mtext mathsize="90%">𝑛𝑒𝑤</mtext></msub></mrow></math><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><math id="algorithm1.m20" class="ltx_Math" alttext="\alpha\leftarrow" display="inline"><mrow><mi mathsize="90%">α</mi><mo mathsize="90%" stretchy="false">←</mo><mi></mi></mrow></math><span class="ltx_text" style="font-size:90%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">MetaStep(<em class="ltx_emph ltx_font_serif ltx_font_italic"><math id="algorithm1.m21" class="ltx_Math" alttext="\alpha,\alpha^{\prime},\eta" display="inline"><mrow><mi>α</mi><mo mathvariant="normal">,</mo><msup><mi>α</mi><mo mathvariant="normal">′</mo></msup><mo mathvariant="normal">,</mo><mi>η</mi></mrow></math></em>)</span><span class="ltx_text" style="font-size:90%;">;</span>
</div>
<div class="ltx_listingline">
<span class="ltx_text" style="font-size:90%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span class="ltx_text" style="font-size:90%;">   </span><span class="ltx_text" style="font-size:90%;"> </span><math id="algorithm1.m22" class="ltx_Math" alttext="\bm{\theta}\leftarrow\bm{\theta}_{0}" display="inline"><mrow><mi mathsize="90%">𝜽</mi><mo mathsize="90%" stretchy="false">←</mo><msub><mi mathsize="90%">𝜽</mi><mn mathsize="90%">0</mn></msub></mrow></math><span class="ltx_text" style="font-size:90%;"> </span>
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold" style="font-size:90%;">return</span><span class="ltx_text" style="font-size:90%;"> </span><em class="ltx_emph ltx_font_italic" style="font-size:90%;"><math id="algorithm1.m23" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math></em><span class="ltx_text" style="font-size:90%;"> </span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Stochastic Meta-Descent</figcaption>
</figure>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">Using the gradients with respect to hyperparameters, as given in Eq. <a href="#S3.E9" title="9 ‣ 3.1 Background: Stochastic Meta-Descent ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we can
apply gradient based meta-optimization, just like optimizing regular parameters. It is worth noting
that, although SMD was originally proposed for optimizing vanilla SGD, in practice it can be applied
to other optimization algorithms such as SGD with momentum or
Adam <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2015adam</span>)</cite>. Moreover, gradient-based optimizers other than SGD can be used for the
meta-optimization as well.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">The basic SMD algorithm is given as Algorithm <a href="#algorithm1" title="1 ‣ 3.1 Background: Stochastic Meta-Descent ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here, <math id="S3.SS1.p4.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is a set of
hyperparameters (e.g. learning rate), and <math id="S3.SS1.p4.m2" class="ltx_Math" alttext="\alpha_{0}" display="inline"><msub><mi>α</mi><mn>0</mn></msub></math> are inital hyperparameter values; <math id="S3.SS1.p4.m3" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math>
is a set of optimization intermediate variables, such as weights and velocities; <math id="S3.SS1.p4.m4" class="ltx_Math" alttext="\eta" display="inline"><mi>η</mi></math> is a set of
meta-optimizer hyperparameters (e.g. meta learning rate). <span class="ltx_text ltx_font_typewriter">BGrad<math id="S3.SS1.p4.m5" class="ltx_Math" alttext="(y,x,dy)" display="inline"><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>y</mi><mo mathvariant="normal">,</mo><mi>x</mi><mo mathvariant="normal">,</mo><mrow><mi>d</mi><mo mathvariant="monospace">⁢</mo><mi>y</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></math></span> is the backward
gradient function that computes the gradients of the loss function wrt. <math id="S3.SS1.p4.m6" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math>, and
<span class="ltx_text ltx_font_typewriter">FGrad<math id="S3.SS1.p4.m7" class="ltx_Math" alttext="(y,x,dx)" display="inline"><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>y</mi><mo mathvariant="normal">,</mo><mi>x</mi><mo mathvariant="normal">,</mo><mrow><mi>d</mi><mo mathvariant="monospace">⁢</mo><mi>x</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></math></span> is the forward gradient function that accumulates the gradients of
<math id="S3.SS1.p4.m8" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math> with respect to <math id="S3.SS1.p4.m9" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>. <span class="ltx_text ltx_font_typewriter">Step</span> and <span class="ltx_text ltx_font_typewriter">MetaStep</span> optimize regular
parameters and hyperparameters, respectively, for one step using gradient-based methods.
Additionally, <math id="S3.SS1.p4.m10" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is the lookahead window size, and <math id="S3.SS1.p4.m11" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> is the number of meta updates.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simplifications from the original SMD algorithm.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">The original SMD
algorithm <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schraudolph1999smd</span>)</cite> fit coordinate-wise adaptive learning rates with intermediate
gradients (<math id="S3.SS1.SSS0.Px1.p1.m1" class="ltx_Math" alttext="\bm{u}_{t}" display="inline"><msub><mi>𝒖</mi><mi>t</mi></msub></math>) accumulated throughout the process of training. Since computing separate
directional derivatives for each coordinate using forward mode autodiff is computationally prohibitive,
the algorithm used approximate updates. Both features introduced bias into the meta-gradients.
We make several changes to the original algorithm. First, we tune only a global learning rate
parameter. Second, we use exact forward mode accumulation because this is feasible for a single
learning rate. Third, rather than accumulate directional derivatives during training, we compute the
meta-updates on separate SGD trajectories simulated using fixed network parameters. Finally, we
compute multiple meta-updates in order to ensure that the meta-objective is optimized sufficiently
well. Together, these changes ensure unbiased meta-gradients, as well as careful optimization of the
meta-objective, at the cost of high computational overhead. We do not recommend this approach as a
practical SMD implementation, but rather as a way of understanding the biases in the meta-objective
itself.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Offline meta-optimization</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">To understand the sensitivity of the optimized hyperparameters to the horizon, we
first carried out an offline experiment on a multi-layered perceptron (MLP) on MNIST <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mnist</span>)</cite>.
Specifically, we fit learning rate decay schedules offline by repeatedly training the network, and
a single meta-gradient was obtained from each training run. </p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="x10.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="814" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Meta-objective surfaces and SMD trajectories (red) optimizing initial effective learning rate and
decay exponent with horizons of
{100, 1k, 5k, 20k} steps. 2.5k random samples with Gaussian interpolation are used to
illustrate the meta-objective surface.</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure ltx_align_floatright"><img src="x11.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="357" height="455" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Training curves using meta-optimized learning rate schedules with {100, 1k, 5k, 20k} step horizons.</figcaption>
</figure>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Learnable decay schedule.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We used a
parametric learning rate decay schedule known as <em class="ltx_emph ltx_font_italic">inverse time decay</em> <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">welling2011sgld</span>)</cite>:
<math id="S3.SS2.SSS0.Px1.p1.m1" class="ltx_Math" alttext="\alpha_{t}=\frac{\alpha_{0}}{(1+\frac{t}{K})^{\beta}}" display="inline"><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mfrac><msub><mi>α</mi><mn>0</mn></msub><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mi>t</mi><mi>K</mi></mfrac></mrow><mo stretchy="false">)</mo></mrow><mi>β</mi></msup></mfrac></mrow></math>, where <math id="S3.SS2.SSS0.Px1.p1.m2" class="ltx_Math" alttext="\alpha_{0}" display="inline"><msub><mi>α</mi><mn>0</mn></msub></math> is the
initial learning rate, <math id="S3.SS2.SSS0.Px1.p1.m3" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is the number of training steps,
<math id="S3.SS2.SSS0.Px1.p1.m4" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> is the learning rate decay exponent, and <math id="S3.SS2.SSS0.Px1.p1.m5" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> is the time constant. We
jointly optimized <math id="S3.SS2.SSS0.Px1.p1.m6" class="ltx_Math" alttext="\alpha_{0}" display="inline"><msub><mi>α</mi><mn>0</mn></msub></math> and <math id="S3.SS2.SSS0.Px1.p1.m7" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>.
We fixed <math id="S3.SS2.SSS0.Px1.p1.m8" class="ltx_Math" alttext="\mu=0.9" display="inline"><mrow><mi>μ</mi><mo>=</mo><mn>0.9</mn></mrow></math>, <math id="S3.SS2.SSS0.Px1.p1.m9" class="ltx_Math" alttext="K=5000" display="inline"><mrow><mi>K</mi><mo>=</mo><mn>5000</mn></mrow></math> for simplicity.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Experimental details.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">The network had two layers of 100 hidden units, with ReLU
activations. Weights were initialized with a zero-mean Gaussian with standard deviation 0.1. We used
a warm start from a network trained for 50 SGD with momentum steps, using <math id="S3.SS2.SSS0.Px2.p1.m1" class="ltx_Math" alttext="\alpha=0.1,\mu=0.9" display="inline"><mrow><mrow><mi>α</mi><mo>=</mo><mn>0.1</mn></mrow><mo>,</mo><mrow><mi>μ</mi><mo>=</mo><mn>0.9</mn></mrow></mrow></math>. (We
used a warm start because the dynamics are generally different at the very start of training.) We
trained all hyperparameters in log space using Adam optimizer, with 5k meta steps.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 Offline meta-optimization ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows SMD optimization trajectories on the meta-objective surfaces,
initialized with multiple random hyperparameter settings. The SMD trajectories appear to have
converged to the global optimum.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">Importantly, the meta-objectives with longer horizons favored a much smaller learning rate decay
exponent <math id="S3.SS2.SSS0.Px2.p3.m1" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>, leading to a more gradual decay schedule. The meta-objective surfaces were very
different depending on the time horizon, and the final <math id="S3.SS2.SSS0.Px2.p3.m2" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> value differed by over two orders of
magnitude between 100 and 20k step horizons.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p4" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 Offline meta-optimization ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> plots the training curves of a network using the optimized learning rate
schedules. The resulting training loss at 20k steps with the 100 step horizon
was over <span class="ltx_text ltx_font_italic">three</span> orders of magnitude larger than with the 20k step horizon. In general, short
horizons gave better performance initially,
but were surpassed by longer horizons. The differences in <em class="ltx_emph ltx_font_italic">error</em> were less drastic, but
we see that the 100 step network was severely undertrained, and the 1k step network achieved
noticeably worse test error than the longer-horizon ones.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Online Meta-Optimization</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">In this section, we study whether online adaptation also suffers from short-horizon bias. Specifically, we used Algorithm <a href="#algorithm1" title="1 ‣ 3.1 Background: Stochastic Meta-Descent ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) to adapt the learning rate and momentum
hyperparameters online while a network is trained. We experimented with an MLP on MNIST and a CNN
on CIFAR-10 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Krizhevsky09learningmultiple</span>)</cite>.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Experimental details.</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">For the MNIST experiments, we used an MLP network with two hidden
layers of 100 units, with ReLU activations. Weights were initialized with a zero-mean Gaussian with
standard deviation 0.1. For CIFAR-10 experiments, we used a CNN network adapted from Caffe
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">caffe</span>)</cite>, with 3 convolutional layers of filter size 3 <math id="S3.SS3.SSS0.Px1.p1.m1" class="ltx_Math" alttext="\times" display="inline"><mo>×</mo></math> 3 and depth [32, 32, 64], and
<math id="S3.SS3.SSS0.Px1.p1.m2" class="ltx_Math" alttext="2\times 2" display="inline"><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow></math> max pooling with stride 2 after every convolution layer, and follwed by a fully
connected hidden layer of 100 units. Meta-optimization was done with 100 steps of Adam for every 10
steps of regular training. We adapted the learning rate <math id="S3.SS3.SSS0.Px1.p1.m3" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> and momentum <math id="S3.SS3.SSS0.Px1.p1.m4" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math>. After 25k
steps, adaptation was stopped, and we trained for another 25k steps with an exponentially decaying
learning rate such that it reached 1e-4 on the last time step. We re-parameterized the learning rate
with the effective learning rate <math id="S3.SS3.SSS0.Px1.p1.m5" class="ltx_Math" alttext="\alpha_{\text{eff}}=\frac{\alpha}{1-\mu}" display="inline"><mrow><msub><mi>α</mi><mtext>eff</mtext></msub><mo>=</mo><mfrac><mi>α</mi><mrow><mn>1</mn><mo>−</mo><mi>μ</mi></mrow></mfrac></mrow></math>, and the momentum with
<math id="S3.SS3.SSS0.Px1.p1.m6" class="ltx_Math" alttext="1-\mu" display="inline"><mrow><mn>1</mn><mo>−</mo><mi>μ</mi></mrow></math>, so that they can be optimized more smoothly in the log space.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F7" title="Figure 7 ‣ Experimental details. ‣ 3.3 Online Meta-Optimization ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows training curves both with online SMD and with hand-tuned fixed
learning rate and momentum hyperparameters. We show several SMD runs initialized from widely varying
hyperparameters; all the SMD runs behaved similarly, suggesting it optimized the meta-objective
efficiently enough. Under SMD, learning rates were quickly decreased to very small values, leading
to slow progress in the long term, consistent with the noisy quadratic and offline adaptation
experiments.
</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2"><img src="x12.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_square" width="349" height="305" alt="Refer to caption"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2"><img src="x13.png" id="S3.F7.g2" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_square" width="349" height="305" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Training cuurves and learning rates from online SMD with lookahead of 5 steps (blue), and
hand-tuned fixed learning rate (red). Each blue curve corresponds to a different initial learning
rate.</figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2"><img src="x14.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_square" width="349" height="305" alt="Refer to caption"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2"><img src="x15.png" id="S3.F8.g2" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_square" width="349" height="305" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Online SMD with deterministic lookahead of 5 steps (blue), compared with a manually tuned
fixed learning rate (red). Other settings are the same as Figure <a href="#S3.F7" title="Figure 7 ‣ Experimental details. ‣ 3.3 Online Meta-Optimization ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</figcaption>
</figure>
<div id="S3.SS3.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p">As online SMD can be too conservative in the choice of learning rate, it is natural to ask whether
removing the stochasticity in the lookahead sequence can fix the problem. We therefore considered
online SMD where the entire lookahead trajectory used a <em class="ltx_emph ltx_font_italic">single</em> mini-batch, hence removing the
stochasticity. As shown in Figure <a href="#S3.F8" title="Figure 8 ‣ Experimental details. ‣ 3.3 Online Meta-Optimization ‣ 3 Gradient-Based Meta-Optimization ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, this deterministic lookahead scheme led
to the opposite problem: the adapted learning rates were very large, leading to instability. We
conclude that the stochasticity of mini-batch training cannot be simply ignored in
meta-optimization.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this paper, we analyzed the problem of short-horizon bias in meta-optimization. We presented a
noisy quadratic toy problem which we analyzed mathematically, and observed that the optimal learning rate schedule differs greatly from a
greedy schedule that minimizes training loss one step ahead.
While the greedy schedule tends to decay the learning rate drastically to reduce the loss on high curvature
directions, the optimal schedule keeps a high learning rate in order to make steady progress on low curvature
directions, and eventually achieves far lower loss.
We showed that this bias stems from the combination of stochasticity and ill-conditioning: when the
problem is <em class="ltx_emph ltx_font_italic">either</em> deterministic or spherical, the greedy learning rate
schedule is globally optimal; however, when the problem is both stochastic and ill-conditioned (as is most neural net training), the greedy schedule performs poorly.
We empirially verified the short-horizon bias in the context of neural net training by applying gradient
based meta-optimization, both offline and online. We found the same pathological behaviors as in the noisy quadratic problem — a fast learning rate decay and poor long-run performance.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">While our results suggest that meta-optimization should not be applied blindly, our noisy quadratic analysis also provides grounds for optimism: by removing ill-conditioning (by using a good preconditioner) and/or stochasticity (with large batch sizes or variance reduction techniques), it may be possible to enter the regime where short-horizon meta-optimization works well. It remains to be seen whether this is achievable with existing optimization algorithms.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Acknowledgement</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">YW is supported by a Google PhD Fellowship. RL is supported by Connaught International Scholarships.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Proofs of Theorems</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">The proofs are organized as follows; we provide a proof to Theorem <a href="#Thmtheorem1" title="Theorem 1 (Mean and variance dynamics). ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in <a href="#A1.SS1" title="A.1 Model Dynamics ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>, a proof to Theorem <a href="#Thmtheorem2" title="Theorem 2 (Greedy-optimal learning rate and momentum). ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> in <a href="#A1.SS2" title="A.2 Greedy optimality ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> and a proof to Theorem <a href="#Thmtheorem3" title="Theorem 3 (Optimal learning rate, univariate). ‣ 2.2.4 Univariate and Spherical Cases ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in <a href="#A1.SS3" title="A.3 Univariate Optimality in SGD ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Model Dynamics</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p class="ltx_p">Recall the stochastic gradient descent with momentum is defined as follows,</p>
<table id="A1.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex9.m1" class="ltx_Math" alttext="\displaystyle v^{(t+1)}" display="inline"><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex9.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}v^{(t)}-\alpha^{(t)}(h\theta^{(t)}+h\sigma\xi),\quad\xi%
\sim\mathcal{N}(0,1)" display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>h</mi><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mi>h</mi><mo>⁢</mo><mi>σ</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>ξ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex10.m1" class="ltx_Math" alttext="\displaystyle\theta^{(t+1)}" display="inline"><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex10.m2" class="ltx_Math" alttext="\displaystyle=\theta^{(t)}+v^{(t+1)}=\theta^{(t)}+\mu^{(t)}v^{(t)}-\alpha^{(t)%
}(h\theta^{(t)}+h\sigma\xi)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow><mo>=</mo><mrow><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>h</mi><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mi>h</mi><mo>⁢</mo><mi>σ</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex11.m1" class="ltx_Math" alttext="\displaystyle=(1-\alpha^{(t)}h)\theta^{(t)}+\mu^{(t)}v^{(t)}+h\sigma\xi." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mi>h</mi><mo>⁢</mo><mi>σ</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<section id="A1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Dynamics of the expectation</h4>

<div id="A1.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p">We calculate the mean of the velocity <math id="A1.SS1.SSS1.p1.m1" class="ltx_Math" alttext="v^{(t+1)}" display="inline"><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math>,</p>
<table id="A1.EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex12.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[v^{(t+1)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex12.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}\left[\mu^{(t)}v^{(t)}-\alpha^{(t)}h\theta^{(t)}\right]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.E10.m1" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\mathbb{E}\left[v^{(t)}\right]-\alpha^{(t)}h\mathbb{E}%
\left[\theta^{(t)}\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We calculate the mean of the parameter <math id="A1.SS1.SSS1.p1.m2" class="ltx_Math" alttext="\theta^{(t+1)}" display="inline"><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math>,</p>
<table id="A1.EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.E11.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[\theta^{(t+1)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.E11.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}\left[\theta^{(t)}\right]+\mathbb{E}\left[v^{(t+1)}%
\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let’s assume the following initial conditions:</p>
<table id="A1.EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex13.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[v^{(0)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex13.m2" class="ltx_Math" alttext="\displaystyle=0" display="inline"><mrow><mi></mi><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex14.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}\left[\theta^{(0)}\right]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex14.m2" class="ltx_Math" alttext="\displaystyle=E_{0}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><msub><mi>E</mi><mn>0</mn></msub></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Then Eq.(<a href="#A1.E10" title="10 ‣ A.1.1 Dynamics of the expectation ‣ A.1 Model Dynamics ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>) and Eq.(<a href="#A1.E11" title="11 ‣ A.1.1 Dynamics of the expectation ‣ A.1 Model Dynamics ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) describes how <math id="A1.SS1.SSS1.p1.m3" class="ltx_Math" alttext="\mathbb{E}\left[{\theta^{(t)}}\right],\mathbb{E}\left[v^{(t)}\right]" display="inline"><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></math> changes over time <math id="A1.SS1.SSS1.p1.m4" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>.</p>
</div>
</section>
<section id="A1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Dynamics of the variance</h4>

<div id="A1.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p">We calculate the variance of the velocity <math id="A1.SS1.SSS2.p1.m1" class="ltx_Math" alttext="v^{(t+1)}" display="inline"><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math>,</p>
<table id="A1.EGx10" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex15.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[v^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex15.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{V}\left[\mu^{(t)}v^{(t)}-\alpha^{(t)}h\theta^{(t)}\right%
]+(\alpha^{(t)}h\sigma)^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.E12.m1" class="ltx_Math" alttext="\displaystyle=(\mu^{(t)})^{2}\mathbb{V}\left[v^{(t)}\right]+(\alpha^{(t)}h)^{2%
}\mathbb{V}\left[\theta^{(t)}\right]-2\mu^{(t)}\alpha^{(t)}h\cdot\mathrm{Cov}%
\left(\theta^{(t)},v^{(t)}\right)+(\alpha^{(t)}h\sigma)^{2}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>Cov</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The variance of the parameter <math id="A1.SS1.SSS2.p1.m2" class="ltx_Math" alttext="\theta^{(t+1)}" display="inline"><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math> is given by,
</p>
<table id="A1.EGx11" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.E13.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[\theta^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.E13.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{V}\left[\theta^{(t)}\right]+\mathbb{V}\left[v^{(t+1)}%
\right]+2\left(\mu^{(t)}\mathrm{Cov}\left(\theta^{(t)},v^{(t)}\right)-\alpha^{%
(t)}h\mathbb{V}\left[\theta^{(t)}\right]\right)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We also need to derive how the covariance of <math id="A1.SS1.SSS2.p1.m3" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math> and <math id="A1.SS1.SSS2.p1.m4" class="ltx_Math" alttext="v" display="inline"><mi>v</mi></math> changes over time:</p>
<table id="A1.EGx12" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex16.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Cov}\left(\theta^{(t+1)},v^{(t+1)}\right)" display="inline"><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex16.m2" class="ltx_Math" alttext="\displaystyle=\mathrm{Cov}\left((\theta^{(t)}+v^{(t+1)}),v^{(t+1)}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex17.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Cov}\left(\theta^{(t)},v^{(t+1)}\right)+\mathbb{V}\left[%
v^{(t+1)}\right]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.E14.m1" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\mathrm{Cov}\left(\theta^{(t)},v^{(t)}\right)-\alpha^{(%
t)}h\mathbb{V}\left[\theta^{(t)}\right]+\mathbb{V}\left[v^{(t+1)}\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let’s assume the following initial conditions:</p>
<table id="A1.EGx13" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex18.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[v^{(0)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex18.m2" class="ltx_Math" alttext="\displaystyle=0" display="inline"><mrow><mi></mi><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex19.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[\theta^{(0)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex19.m2" class="ltx_Math" alttext="\displaystyle=V_{0}" display="inline"><mrow><mi></mi><mo>=</mo><msub><mi>V</mi><mn>0</mn></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex20.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Cov}\left(\theta^{(0)},v^{(0)}\right)" display="inline"><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex20.m2" class="ltx_Math" alttext="\displaystyle=0." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Combining Eq.(<a href="#A1.E12" title="12 ‣ A.1.2 Dynamics of the variance ‣ A.1 Model Dynamics ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-<a href="#A1.E14" title="14 ‣ A.1.2 Dynamics of the variance ‣ A.1 Model Dynamics ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>), we obtain the following dynamics (from <math id="A1.SS1.SSS2.p1.m5" class="ltx_Math" alttext="t=0,\dots,T-1" display="inline"><mrow><mi>t</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mrow></mrow></math>):</p>
<table id="A1.EGx14" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex21.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[v^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex21.m2" class="ltx_Math" alttext="\displaystyle=(\mu^{(t)})^{2}\mathbb{V}\left[v^{(t)}\right]+(\alpha^{(t)}h)^{2%
}\mathbb{V}\left[\theta^{(t)}\right]-2\mu^{(t)}\alpha^{(t)}h\cdot\mathrm{Cov}%
\left(\theta^{(t)},v^{(t)}\right)+(\alpha^{(t)}h\sigma)^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>Cov</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex22.m1" class="ltx_Math" alttext="\displaystyle\mathbb{V}\left[\theta^{(t+1)}\right]" display="inline"><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex22.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{V}\left[\theta^{(t)}\right]+\mathbb{V}\left[v^{(t+1)}%
\right]+2\left(\mu^{(t)}\mathrm{Cov}\left(\theta^{(t)},v^{(t)}\right)-\alpha^{%
(t)}h\mathbb{V}\left[\theta^{(t)}\right]\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex23.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Cov}\left(\theta^{(t+1)},v^{(t+1)}\right)" display="inline"><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex23.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}\mathrm{Cov}\left(\theta^{(t)},v^{(t)}\right)-\alpha^{(%
t)}h\mathbb{V}\left[\theta^{(t)}\right]+\mathbb{V}\left[v^{(t+1)}\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Greedy optimality</h3>

<section id="A1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Univariate case</h4>

<div id="A1.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">The loss at time step <math id="A1.SS2.SSS1.p1.m1" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is,</p>
<table id="A1.EGx15" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex24.m1" class="ltx_Math" alttext="\displaystyle\mathcal{L}^{(t+1)}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex24.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}h\left(\mathbb{E}\left[\theta^{(t+1)}\right]^{2}+%
\mathbb{V}\left[\theta^{(t+1)}\right]\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex25.m1" class="ltx_math_unparsed" alttext="\displaystyle=\frac{1}{2}h\Big{[}\left(\mathbb{E}\left[\theta^{(t)}\right]+\mu%
^{(t)}\mathbb{E}\left[v^{(t)}\right]-(\alpha^{(t)}h)\mathbb{E}\left[\theta^{(t%
)}\right]\right)^{2}+\mathbb{V}\left[\theta^{(t)}\right]+(\mu^{(t)})^{2}%
\mathbb{V}\left[v^{(t)}\right]+(\alpha^{(t)}h)^{2}\mathbb{V}\left[\theta^{(t)}\right]" display="inline"><mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mi>h</mi><mrow><mo maxsize="160%" minsize="160%">[</mo><msup><mrow><mo>(</mo><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>−</mo><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex26.m1" class="ltx_math_unparsed" alttext="\displaystyle-2\mu^{(t)}\alpha^{(t)}h\cdot\mathrm{Cov}\left(\theta^{(t)},v^{(t%
)}\right)+(\alpha^{(t)}h\sigma)^{2}+2\left(\mu^{(t)}\mathrm{Cov}\left(\theta^{%
(t)},v^{(t)}\right)-\alpha^{(t)}h\mathbb{V}\left[\theta^{(t)}\right]\right)%
\Big{]}" display="inline"><mrow><mo>−</mo><mn>2</mn><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>Cov</mi><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mn>2</mn><mrow><mo>(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>Cov</mi><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>)</mo></mrow><mo maxsize="160%" minsize="160%">]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex27.m1" class="ltx_math_unparsed" alttext="\displaystyle=\frac{1}{2}h\Big{[}\left((1-\alpha^{(t)}h)\mathbb{E}\left[\theta%
^{(t)}\right]+\mu^{(t)}\mathbb{E}\left[v^{(t)}\right]\right)^{2}+(1-\alpha^{(t%
)}h)^{2}\mathbb{V}\left[\theta^{(t)}\right]+(\mu^{(t)})^{2}\mathbb{V}\left[v^{%
(t)}\right]" display="inline"><mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mi>h</mi><mrow><mo maxsize="160%" minsize="160%">[</mo><msup><mrow><mo>(</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex28.m1" class="ltx_math_unparsed" alttext="\displaystyle+2\mu^{(t)}(1-\alpha^{(t)}h)\mathrm{Cov}\left(\theta^{(t)},v^{(t)%
}\right)+(\alpha^{(t)}h\sigma)^{2}\Big{]}" display="inline"><mrow><mo>+</mo><mn>2</mn><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mi>Cov</mi><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo maxsize="160%" minsize="160%">]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex29.m1" class="ltx_math_unparsed" alttext="\displaystyle=\frac{1}{2}h\Big{[}(1-\alpha^{(t)}h)^{2}\left(\mathbb{E}\left[%
\theta^{(t)}\right]^{2}+\mathbb{V}\left[\theta^{(t)}\right]\right)+(\mu^{(t)})%
^{2}\left(\mathbb{E}\left[v^{(t)}\right]^{2}+\mathbb{V}\left[v^{(t)}\right]\right)" display="inline"><mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mi>h</mi><mrow><mo maxsize="160%" minsize="160%">[</mo><msup><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mrow><mo>(</mo><mi>𝔼</mi><msup><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mrow><mo>(</mo><mi>𝔼</mi><msup><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>𝕍</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex30.m1" class="ltx_math_unparsed" alttext="\displaystyle+2\mu^{(t)}(1-\alpha^{(t)}h)\left(\mathbb{E}\left[\theta^{(t)}%
\right]\mathbb{E}\left[v^{(t)}\right]+\mathrm{Cov}\left(\theta^{(t)},v^{(t)}%
\right)\right)+(\alpha^{(t)}h\sigma)^{2}\Big{]}." display="inline"><mrow><mo>+</mo><mn>2</mn><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mo stretchy="false">)</mo></mrow><mrow><mo>(</mo><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mi>𝔼</mi><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>+</mo><mi>Cov</mi><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow><mo>)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mi>h</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo maxsize="160%" minsize="160%">]</mo><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">For simplicity, we denote <math id="A1.SS2.SSS1.p1.m2" class="ltx_Math" alttext="A(\cdot)=\mathbb{E}\left[\cdot\right]^{2}+\mathbb{V}\left[\cdot\right]" display="inline"><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>]</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>]</mo></mrow></mrow></mrow></mrow></math>, and notice that <math id="A1.SS2.SSS1.p1.m3" class="ltx_Math" alttext="\mathbb{E}\left[\theta^{(t)}v^{(t)}\right]=\mathbb{E}\left[\theta^{(t)}\right]%
\mathbb{E}\left[v^{(t)}\right]+\mathrm{Cov}\left(\theta^{(t)},v^{(t)}\right)" display="inline"><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>Cov</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow></mrow></math>, hence,</p>
<table id="A1.E15" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.E15.m1" class="ltx_Math" alttext="\mathcal{L}^{(t+1)}=\frac{1}{2}h\Big{[}(1-\alpha^{(t)}h)^{2}A(\theta^{(t)})+(%
\mu^{(t)})^{2}A(v^{(t)})+2\mu^{(t)}(1-\alpha^{(t)}h)\mathbb{E}\left[\theta^{(t%
)}v^{(t)}\right]+(\alpha^{(t)}h\sigma)^{2}\Big{]}." display="block"><mrow><mrow><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">[</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo maxsize="160%" minsize="160%">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In order to find the optimal learning rate and momentum for minimizing <math id="A1.SS2.SSS1.p1.m4" class="ltx_Math" alttext="\mathcal{L}^{(t+1)}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math>, we take the derivative with respect to <math id="A1.SS2.SSS1.p1.m5" class="ltx_Math" alttext="\alpha^{(t)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> and <math id="A1.SS2.SSS1.p1.m6" class="ltx_Math" alttext="\mu^{(t)}" display="inline"><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math>, and set it to <math id="A1.SS2.SSS1.p1.m7" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>:</p>
<table id="A1.EGx16" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex31.m1" class="ltx_Math" alttext="\displaystyle\nabla_{\alpha^{(t)}}\mathcal{L}^{(t+1)}" display="inline"><mrow><msub><mo>∇</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></msub><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex31.m2" class="ltx_Math" alttext="\displaystyle=(1-\alpha^{(t)}h)A(\theta^{(t)})\cdot(-h)-\mu^{(t)}h\mathbb{E}%
\left[\theta^{(t)}v^{(t)}\right]+\alpha^{(t)}(h\sigma)^{2}=0" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mi>h</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex32.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)}h(A(\theta^{(t)})+\sigma^{2})" display="inline"><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex32.m2" class="ltx_Math" alttext="\displaystyle=A(\theta^{(t)})+\mu^{(t)}\mathbb{E}\left[\theta^{(t)}v^{(t)}\right]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex33"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex33.m1" class="ltx_Math" alttext="\displaystyle\nabla_{\mu^{(t)}}\mathcal{L}^{(t+1)}" display="inline"><mrow><msub><mo>∇</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></msub><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex33.m2" class="ltx_Math" alttext="\displaystyle=\mu^{(t)}A(v^{(t)})+(1-\alpha^{(t)}h)\mathbb{E}\left[\theta^{(t)%
}v^{(t)}\right]=0" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex34"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex34.m1" class="ltx_Math" alttext="\displaystyle\mu^{(t)*}" display="inline"><msup><mi>μ</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex34.m2" class="ltx_Math" alttext="\displaystyle=-\frac{(1-\alpha^{(t)}h)\mathbb{E}\left[\theta^{(t)}v^{(t)}%
\right]}{A(v^{(t)})}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex35.m1" class="ltx_Math" alttext="\displaystyle\quad\alpha^{(t)}h(A(\theta^{(t)})+\sigma^{2})" display="inline"><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex35.m2" class="ltx_Math" alttext="\displaystyle=A(\theta^{(t)})-\frac{(1-\alpha^{(t)}h)\mathbb{E}\left[\theta^{(%
t)}v^{(t)}\right]}{A(v^{(t)})}\mathbb{E}\left[\theta^{(t)}v^{(t)}\right]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex36.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)*}" display="inline"><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex36.m2" class="ltx_Math" alttext="\displaystyle=\frac{A(\theta^{(t)})A(v^{(t)})-\mathbb{E}\left[\theta^{(t)}v^{(%
t)}\right]^{2}}{h\left(A(v^{(t)})(A(\theta^{(t)})+\sigma^{2})-\mathbb{E}\left[%
\theta^{(t)}v^{(t)}\right]^{2}\right)}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow><mn>2</mn></msup></mrow></mrow><mrow><mi>h</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>]</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>high dimension case</h4>

<div id="A1.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p">The loss is the sum of losses along all directions:</p>
<table id="A1.Ex37" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.Ex37.m1" class="ltx_Math" alttext="\mathcal{L}^{(t+1)}=\sum_{i}\frac{1}{2}h_{i}\Big{[}(1-\alpha^{(t)}h_{i})^{2}A(%
\theta^{(t)}_{i})+(\mu^{(t)})^{2}A(v^{(t)}_{i})+2\mu^{(t)}(1-\alpha^{(t)}h_{i}%
)\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]+(\alpha^{(t)}h_{i}\sigma_{%
i})^{2}\Big{]}" display="block"><mrow><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">[</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>σ</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo maxsize="160%" minsize="160%">]</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Now we obtain optimal learning rate and momentum by setting the derivative to <math id="A1.SS2.SSS2.p1.m1" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>,</p>
<table id="A1.EGx17" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex38.m1" class="ltx_Math" alttext="\displaystyle\nabla_{\alpha^{(t)}}\mathcal{L}^{(t+1)}=\sum_{i}h_{i}\Big{[}(1-%
\alpha^{(t)}h_{i})A(\theta^{(t)}_{i})\cdot(-h_{i})-\mu^{(t)}h_{i}\mathbb{E}%
\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]+\alpha^{(t)}(h_{i}\sigma_{i})^{2}\Big%
{]}=0" display="inline"><mrow><mrow><msub><mo>∇</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></msub><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">[</mo><mrow><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>σ</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo maxsize="160%" minsize="160%">]</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex39.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)}\sum_{i}\left((h_{i})^{3}(A(\theta^{(t)}_{i})+(\sigma%
_{i})^{2})\right)=\sum_{i}\left((h_{i})^{2}A(\theta^{(t)}_{i})+\mu^{(t)}(h_{i}%
)^{2}\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]\right)" display="inline"><mrow><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>3</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo>(</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex40"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex40.m1" class="ltx_Math" alttext="\displaystyle\nabla_{\mu^{(t)}}\mathcal{L}^{(t+1)}=\sum_{i}h_{i}\mu^{(t)}A(v^{%
(t)}_{i})+h_{i}(1-\alpha^{(t)}h_{i})\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i%
}\right]=0" display="inline"><mrow><mrow><msub><mo>∇</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></msub><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><msup><mi>μ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex41.m1" class="ltx_Math" alttext="\displaystyle\mu^{(t)*}=-\frac{\sum_{i}h_{i}(1-\alpha^{(t)}h_{i})\mathbb{E}%
\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]}{\sum_{i}h_{i}A(v^{(t)}_{i})}" display="inline"><mrow><msup><mi>μ</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex42"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex42.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)}\sum_{i}\left(\left((h_{i})^{3}(A(\theta^{(t)}_{i})+(%
\sigma_{i})^{2})\right)\Big{(}\sum_{j}h_{j}A(v^{(t)}_{j})\Big{)}-\Big{(}\sum_{%
j}(h_{j})^{2}\mathbb{E}\left[\theta^{(t)}_{j}v^{(t)}_{j}\right]\Big{)}(h_{i})^%
{2}\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]\right)=" display="inline"><mrow><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo>(</mo><mrow><mrow><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>3</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder></mstyle><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mi></mi></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex43"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex43.m1" class="ltx_Math" alttext="\displaystyle\qquad\qquad\sum_{i}\left((h_{i})^{2}A(\theta^{(t)}_{i})\Big{(}%
\sum_{j}h_{j}A(v^{(t)}_{j})\Big{)}-\Big{(}\sum_{j}h_{j}\mathbb{E}\left[\theta^%
{(t)}_{j}v^{(t)}_{j}\right]\Big{)}(h_{i})^{2}\mathbb{E}\left[\theta^{(t)}_{i}v%
^{(t)}_{i}\right]\right)" display="inline"><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo>(</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder></mstyle><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex44"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex44.m1" class="ltx_Math" alttext="\displaystyle\alpha^{(t)*}=\frac{\sum_{i}\left((h_{i})^{2}A(\theta^{(t)}_{i})%
\Big{(}\sum_{j}h_{j}A(v^{(t)}_{j})\Big{)}-\Big{(}\sum_{j}h_{j}\mathbb{E}\left[%
\theta^{(t)}_{j}v^{(t)}_{j}\right]\Big{)}(h_{i})^{2}\mathbb{E}\left[\theta^{(t%
)}_{i}v^{(t)}_{i}\right]\right)}{\sum_{i}\left(\left((h_{i})^{3}(A(\theta^{(t)%
}_{i})+(\sigma_{i})^{2})\right)\Big{(}\sum_{j}h_{j}A(v^{(t)}_{j})\Big{)}-\Big{%
(}\sum_{j}(h_{j})^{2}\mathbb{E}\left[\theta^{(t)}_{j}v^{(t)}_{j}\right]\Big{)}%
(h_{i})^{2}\mathbb{E}\left[\theta^{(t)}_{i}v^{(t)}_{i}\right]\right)}." display="inline"><mrow><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo lspace="0em">(</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo lspace="0em">(</mo><mrow><mrow><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>3</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>σ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><msub><mo lspace="0em">∑</mo><mi>j</mi></msub><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><msub><mo lspace="0em" rspace="0em">∑</mo><mi>j</mi></msub><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mi>θ</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Univariate Optimality in SGD</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p class="ltx_p">We now consider a dynamic programming approach to solve the problem. We formalize the optimization problem of <math id="A1.SS3.p1.m1" class="ltx_Math" alttext="\{\alpha_{i}\}" display="inline"><mrow><mo stretchy="false">{</mo><msub><mi>α</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></math> as follows. We first denote <math id="A1.SS3.p1.m2" class="ltx_Math" alttext="\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math> as the minimum expected loss at the last time step <math id="A1.SS3.p1.m3" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> (i.e., under the optimal learning rate).
</p>
<table id="A1.EGx18" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex45"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex45.m1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\min}=\min_{\alpha^{(t)},\alpha^{(t+1)},\dots,\alpha%
^{(T-1)}}\mathbb{E}_{\xi^{(t)},\xi^{(t+1)},\dots,\xi^{(T-1)}}\left[\mathcal{L}%
(\theta^{(T)})\right]." display="inline"><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub><mo>=</mo><mrow><mrow><munder><mi>min</mi><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>𝔼</mi><mrow><msup><mi>ξ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>ξ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>ξ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></msub></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Recall that the loss can be expressed in terms of the expectation and variance of <math id="A1.SS3.p1.m4" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>. Denote <math id="A1.SS3.p1.m5" class="ltx_Math" alttext="A^{(t)}=(\mathbb{E}\left[\theta^{(t)}\right])^{2}+\mathbb{V}\left[\theta^{(t)}\right]" display="inline"><mrow><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow></mrow></mrow></math>.
The final loss can be expressed in terms of <math id="A1.SS3.p1.m6" class="ltx_Math" alttext="A_{\min}^{(T)}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></math>, obtained by using optimal learning rate schedule:</p>
<table id="A1.EGx19" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex46"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex46.m1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex46.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}hA_{\min}^{(T)}+\sigma^{2}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mi>h</mi><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p class="ltx_p">As in Theorem <a href="#Thmtheorem1" title="Theorem 1 (Mean and variance dynamics). ‣ 2.2.3 Optimized and Greedy-Optimal Schedules ‣ 2.2 Analysis ‣ 2 Noisy quadratic problem ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we derive the dynamics for SGD <em class="ltx_emph ltx_font_italic">without</em> momentum:</p>
<table id="A1.EGx20" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex47"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex47.m1" class="ltx_Math" alttext="\displaystyle\theta^{(t)}" display="inline"><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex47.m2" class="ltx_Math" alttext="\displaystyle=(1-\alpha^{(t-1)}h)\theta^{(t-1)}+\alpha^{(t-1)}h\sigma\xi^{(t-1)}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi><mo>⁢</mo><msup><mi>ξ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex48"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex48.m1" class="ltx_Math" alttext="\displaystyle\Rightarrow\left(\mathbb{E}\left[\theta^{(t)}\right],\mathbb{V}%
\left[\theta^{(t)}\right]\right)" display="inline"><mrow><mi></mi><mo stretchy="false">⇒</mo><mrow><mo>(</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex48.m2" class="ltx_Math" alttext="\displaystyle=\left((1-\alpha^{(t-1)}h)\mathbb{E}\left[\theta^{(t-1)}\right],(%
1-\alpha^{(t-1)}h)^{2}\mathbb{V}\left[\theta^{(t-1)}\right]+(\alpha^{(t-1)}h%
\sigma)^{2}\right)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mo>(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>𝕍</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we can find a recurrence relation of the sequence <math id="A1.SS3.p2.m1" class="ltx_Math" alttext="A^{(t)}" display="inline"><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math>:</p>
<table id="A1.Ex49" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.Ex49.m1" class="ltx_Math" alttext="A^{(t)}=(1-\alpha^{(t-1)}h)^{2}\left((\mathbb{E}\left[\theta^{(t-1)}\right])^{%
2}+\mathbb{V}\left[\theta^{(t-1)}\right]^{2}\right)+(\alpha^{(t-1)}h\sigma)^{2%
}=(1-\alpha^{(t-1)}h)^{2}A_{\min}^{(T-1)}+(\alpha^{(t-1)}h\sigma)^{2}." display="block"><mrow><mrow><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mi>𝕍</mi><mo>⁢</mo><msup><mrow><mo>[</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>]</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>σ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Since <math id="A1.SS3.p2.m2" class="ltx_Math" alttext="A_{\min}^{(T)}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></math> is a function of <math id="A1.SS3.p2.m3" class="ltx_Math" alttext="\alpha^{(T-1)*}" display="inline"><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math>. We can obtain optimal learning rate <math id="A1.SS3.p2.m4" class="ltx_Math" alttext="\alpha^{(T-1)*}" display="inline"><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math> by taking the derivative of <math id="A1.SS3.p2.m5" class="ltx_Math" alttext="\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math> w.r.t. <math id="A1.SS3.p2.m6" class="ltx_Math" alttext="\alpha^{(T-1)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math> and setting it to zero:</p>
<table id="A1.EGx21" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex50"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex50.m1" class="ltx_Math" alttext="\displaystyle\frac{d\mathcal{L}_{\min}}{d\alpha^{(T-1)}}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></mrow><mrow><mi>d</mi><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mstyle></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex50.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}h\frac{dA_{\min}^{(T-1)}}{d\alpha^{(T-2)}}=0" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow><mrow><mi>d</mi><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mstyle></mrow><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex51"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex51.m1" class="ltx_Math" alttext="\displaystyle\Rightarrow" display="inline"><mo stretchy="false">⇒</mo></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex51.m2" class="ltx_Math" alttext="\displaystyle\quad\frac{dA_{\min}^{(T)}}{d\alpha^{(T-1)}}=0" display="inline"><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mrow><mi>d</mi><mo>⁢</mo><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mstyle><mo>=</mo><mn>0</mn></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex52"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex52.m1" class="ltx_Math" alttext="\displaystyle\Rightarrow" display="inline"><mo stretchy="false">⇒</mo></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex52.m2" class="ltx_Math" alttext="\displaystyle\quad\alpha^{(T-1)*}=\frac{A_{\min}^{(T-1)}}{h(A_{\min}^{(T-1)}+%
\sigma^{2})}." display="inline"><mrow><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus we can write <math id="A1.SS3.p2.m7" class="ltx_Math" alttext="A_{\min}^{(T)}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></math> in terms of <math id="A1.SS3.p2.m8" class="ltx_Math" alttext="A_{\min}^{(T-1)}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></math> and the optimal <math id="A1.SS3.p2.m9" class="ltx_Math" alttext="\alpha^{(T-1)*}" display="inline"><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup></math>:</p>
<table id="A1.EGx22" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex53"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex53.m1" class="ltx_Math" alttext="\displaystyle A^{(T)}_{\min}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex53.m2" class="ltx_Math" alttext="\displaystyle=\left(1-\frac{A_{\min}^{(T-1)}}{A_{\min}^{(T-1)}+\sigma^{2}}%
\right)^{2}A_{\min}^{(T-1)}+\left(\frac{A_{\min}^{(T-1)}}{A_{\min}^{(T-1)}+%
\sigma^{2}}\sigma\right)^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mi>σ</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex54"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex54.m1" class="ltx_Math" alttext="\displaystyle=\left(\frac{\sigma^{2}}{A_{\min}^{(T-1)}+\sigma^{2}}\right)^{2}A%
_{\min}^{(T-1)}+\left(\frac{A_{\min}^{(T-1)}}{A_{\min}^{(T-1)}+\sigma^{2}}%
\sigma\right)^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><msup><mi>σ</mi><mn>2</mn></msup><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mi>σ</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex55"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex55.m1" class="ltx_Math" alttext="\displaystyle=\frac{A_{\min}^{(T-1)}\sigma^{2}}{A_{\min}^{(T-1)}+\sigma^{2}}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore,</p>
<table id="A1.EGx23" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex56"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.Ex56.m1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex56.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}hA_{\min}^{(T)}+\sigma^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mi>h</mi><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex57"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A1.Ex57.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}h\left(\frac{A_{\min}^{(T-1)}\sigma^{2}}{A_{\min}^{(T%
-1)}+\sigma^{2}}\right)+\sigma^{2}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We now generalize the above derivation. First rewrite <math id="A1.SS3.p2.m10" class="ltx_Math" alttext="\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math> in terms of <math id="A1.SS3.p2.m11" class="ltx_Math" alttext="A^{T-k}_{\min}" display="inline"><msubsup><mi>A</mi><mi>min</mi><mrow><mi>T</mi><mo>−</mo><mi>k</mi></mrow></msubsup></math> and calculate the optimal learning rate at time step <math id="A1.SS3.p2.m12" class="ltx_Math" alttext="T-k" display="inline"><mrow><mi>T</mi><mo>−</mo><mi>k</mi></mrow></math>.</p>
</div>
<div id="Thmtheorem4" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For all <math id="Thmtheorem4.p1.m1" class="ltx_Math" alttext="T\in\mathbb{N}" display="inline"><mrow><mi>T</mi><mo mathvariant="normal">∈</mo><mi>ℕ</mi></mrow></math>, and <math id="Thmtheorem4.p1.m2" class="ltx_Math" alttext="k\in\mathbb{N}" display="inline"><mrow><mi>k</mi><mo mathvariant="normal">∈</mo><mi>ℕ</mi></mrow></math>, <math id="Thmtheorem4.p1.m3" class="ltx_Math" alttext="1\leq k\leq T" display="inline"><mrow><mn mathvariant="normal">1</mn><mo mathvariant="normal">≤</mo><mi>k</mi><mo mathvariant="normal">≤</mo><mi>T</mi></mrow></math>, we have,</span></p>
<table id="A1.E16" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.E16.m1" class="ltx_Math" alttext="\mathcal{L}_{\min}=\frac{1}{2}h\left(\frac{A_{\min}^{(T-k)}\sigma^{2}}{kA_{%
\min}^{(T-k)}+\sigma^{2}}\right)+\sigma^{2}." display="block"><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mrow><mo>(</mo><mfrac><mrow><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>A</mi><mi>min</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>)</mo></mrow></mrow><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Therefore, the optimal learning <math id="Thmtheorem4.p1.m4" class="ltx_Math" alttext="\alpha^{(t)}" display="inline"><msup><mi>α</mi><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>t</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></msup></math> at timestep <math id="Thmtheorem4.p1.m5" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is given as,</span></p>
<table id="A1.E17" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.E17.m1" class="ltx_Math" alttext="\alpha^{(t)*}=\frac{A^{(t)}}{h(A^{(t)}+\sigma^{2})}." display="block"><mrow><mrow><msup><mi>α</mi><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.222em">⁣</mo><mo>*</mo></mrow></msup><mo>=</mo><mfrac><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mrow><mi>h</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A1.SS3.p3" class="ltx_para">
<p class="ltx_p">The form of <math id="A1.SS3.p3.m1" class="ltx_Math" alttext="\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math> can be easily proven by induction on <math id="A1.SS3.p3.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>, and use the identity that,</p>
<table id="A1.Ex58" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.Ex58.m1" class="ltx_Math" alttext="\frac{(\frac{ab}{a+b})b}{k(\frac{ab}{a+b})+b}=\frac{ab}{(k+1)a+b}." display="block"><mrow><mrow><mfrac><mrow><mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>a</mi><mo>⁢</mo><mi>b</mi></mrow><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>b</mi></mrow><mrow><mrow><mi>k</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>a</mi><mo>⁢</mo><mi>b</mi></mrow><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>a</mi><mo>⁢</mo><mi>b</mi></mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>a</mi></mrow><mo>+</mo><mi>b</mi></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The optimal learning rate then follows immediately by taking the derivative of <math id="A1.SS3.p3.m3" class="ltx_Math" alttext="\mathcal{L}_{\min}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>min</mi></msub></math> w.r.t. <math id="A1.SS3.p3.m4" class="ltx_Math" alttext="\alpha^{(T-k-1)}" display="inline"><msup><mi>α</mi><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mi>k</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></math> and setting it to zero. Note that the subscript <math id="A1.SS3.p3.m5" class="ltx_Math" alttext="\min" display="inline"><mi>min</mi></math> is omitted from <math id="A1.SS3.p3.m6" class="ltx_Math" alttext="A^{(t)}" display="inline"><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> in Eq.(<a href="#A1.E17" title="17 ‣ Theorem 4. ‣ A.3 Univariate Optimality in SGD ‣ Appendix A Proofs of Theorems ‣ Understanding Short-Horizon Bias in Stochastic Meta-Optimization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>) as we assume all <math id="A1.SS3.p3.m7" class="ltx_Math" alttext="A^{(t)}" display="inline"><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></math> are obtained using optimal <math id="A1.SS3.p3.m8" class="ltx_Math" alttext="\alpha^{*}" display="inline"><msup><mi>α</mi><mo>*</mo></msup></math>, and hence minimum.
∎</p>
</div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Apr 10 10:50:49 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
