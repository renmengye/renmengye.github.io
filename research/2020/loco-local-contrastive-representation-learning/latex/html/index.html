<!DOCTYPE html><html><head>
<title>LoCo: Local Contrastive Representation Learning</title>
<!--Generated by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="index.css" type="text/css">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LoCo: Local Contrastive Representation Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuwen Xiong, Mengye Ren, Raquel Urtasun
<br class="ltx_break">
<br class="ltx_break">Uber ATG, University of Toronto
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">{yuwen,mren3,urtasun}@uber.com</span>

</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that
creates synchronization constraints in the weight update step across layers and is not biologically
plausible. Recent advances in unsupervised contrastive representation learning invite the question
of whether a learning algorithm can also be made local, that is, the updates of lower layers do not
directly depend on the computation of upper layers. While Greedy InfoMax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> separately
learns each block with a local objective, we found that it consistently hurts readout accuracy in
state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective
as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking
on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly
send feedbacks to lower blocks. This simple design closes the performance gap between local learning
and end-to-end contrastive learning algorithms for the first time. Aside from standard ImageNet
experiments, we also show results on complex downstream tasks such as object detection and instance
segmentation directly using readout features.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Most deep learning algorithms nowadays are trained using backpropagation in an end-to-end fashion:
training losses are computed at the top layer and weight updates are computed based on the gradient
that flows from the very top. Such an algorithm requires lower layers to “wait” for upper layers,
a synchronization constraint that seems very unnatural in truly parallel distributed processing.
Indeed, there are evidences that weight synapse updates in the human brain are achieved through
local learning, without waiting for neurons in other parts of the brain to finish their jobs
<cite class="ltx_cite ltx_citemacro_cite">Caporale and Dan (<a href="#bib.bib60" title="Spike timing–dependent plasticity: a hebbian learning rule" class="ltx_ref">2008</a>); Bengio<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib54" title="Towards biologically plausible deep learning" class="ltx_ref">2015</a>)</cite>. In addition to biological plausibility aims, local learning
algorithms can also significantly reduce memory footprint during training, as they do not require
saving the intermediate activations after each local module finish its calculation. With these synchronization constraints removed, one can further
enable model parallelism in many deep network architectures <cite class="ltx_cite ltx_citemacro_cite">Narayanan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib22" title="PipeDream: generalized pipeline parallelism for dnn training" class="ltx_ref">2019</a>)</cite> for faster parallel
training and inference.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">One main objection against local learning algorithms has always been the need for supervision from
the top layer. This belief has recently been challenged by the success of numerous self-supervised
contrastive learning algorithms&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Tian<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib25" title="Contrastive multiview coding" class="ltx_ref">2019</a>); He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>); Misra and van der Maaten (<a href="#bib.bib31" title="Self-supervised learning of pretext-invariant representations" class="ltx_ref">2019</a>); Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>, some of which can achieve matching
performance compared to supervised counterparts, meanwhile using zero class labels during the
representation learning phase. Indeed, Löwe et al. <cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> show that they can separately
learn each block of layers using local contrastive learning by putting gradient stoppers in between
blocks. While the authors show matching or even sometimes superior performance using local
algorithms, we found that their gradient isolation blocks still result in degradation in accuracy in
state-of-the-art self-supervised learning frameworks, such as SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>. We hypothesize
that, due to gradient isolation, lower layers are unaware of the existence of upper layers, and thus
failing to deliver the full capacity of a deep network when evaluating on large scale datasets such
as ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Deng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Imagenet: a large-scale hierarchical image database" class="ltx_ref">2009</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">To bridge the gradient isolation blocks and allow upper layers to influence lower layers while
maintaining localism, we propose to group two blocks into one local unit and share the middle block
simultaneously by two units. As shown in the right part of Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Thus, the
middle blocks will receive gradients from both the lower portion and the upper portion, acting like
a gradient “bridge”. We found that such a simple scheme significantly bridges the performance gap
between Greedy InfoMax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> and the original end-to-end algorithm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">On ImageNet unsupervised representation learning benchmark, we evaluate our new local learning
algorithm, named LoCo, on both ResNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11" title="Deep residual learning for image recognition" class="ltx_ref">2016</a>)</cite> and ShuffleNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ma<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib12" title="Shufflenet v2: practical guidelines for efficient cnn architecture design" class="ltx_ref">2018</a>)</cite>
architectures and found the conclusion to be the same. Aside from ImageNet object classification, we
further validate the generalizability of locally learned features on other downstream tasks such as
object detection and semantic segmentation, by only training the readout headers. On all benchmarks,
our local learning algorithm once again closely matches the more costly end-to-end trained models.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">We first review related literature in local learning rules and unsupervised representation learning
in Section&nbsp;<a href="#S2" title="2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and further elaborate the background and the two main baselines
SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite> and Greedy InfoMax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> in Section&nbsp;<a href="#S3.SS2" title="3.2 Greedy InfoMax ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Section&nbsp;<a href="#S4" title="4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes our LoCo algorithm in detail. Finally, in
Section&nbsp;<a href="#S5" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we present ImageNet-1K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Deng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Imagenet: a large-scale hierarchical image database" class="ltx_ref">2009</a>)</cite> results, followed by instance
segmentation results on MS-COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib62" title="Microsoft COCO: common objects in context" class="ltx_ref">2014</a>)</cite> and Cityscapes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cordts<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib63" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">2016</a>)</cite>.

</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Neural network local learning rules:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Early neural networks literature, inspired by
biological neural networks, makes use of local associative learning rules, where the change in
synapse weights only depends on the pre- and post-activations. One classic example is the Hebbian
rule&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hebb (<a href="#bib.bib55" title="The organization of behavior: a neuropsychological theory" class="ltx_ref">1949</a>)</cite>, which strengthens the connection whenever two neurons fire together. As this can
result in numerical instability, various modifications were also proposed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Oja (<a href="#bib.bib58" title="Simplified neuron model as a principal component analyzer" class="ltx_ref">1982</a>); Bienenstock<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib57" title="Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex" class="ltx_ref">1982</a>)</cite>. These
classic learning rules can be empirically observed through long-term potentiation (LTP) and long
term depression (LTD) events during spike-timing-dependent plasticity
(STDP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Abbott and Nelson (<a href="#bib.bib56" title="Synaptic plasticity: taming the beast" class="ltx_ref">2000</a>); Caporale and Dan (<a href="#bib.bib60" title="Spike timing–dependent plasticity: a hebbian learning rule" class="ltx_ref">2008</a>)</cite>, and various computational learning models have also been
proposed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib54" title="Towards biologically plausible deep learning" class="ltx_ref">2015</a>)</cite>. Local learning rules are also seen in learning algorithms such as restricted
Boltzmann machines (RBM) &nbsp;<cite class="ltx_cite ltx_citemacro_cite">Smolensky (<a href="#bib.bib53" title="Information processing in dynamical systems: foundations of harmony theory" class="ltx_ref">1986</a>); Hinton (<a href="#bib.bib52" title="A practical guide to training restricted boltzmann machines" class="ltx_ref">2012</a>); Hinton<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib61" title="A fast learning algorithm for deep belief nets" class="ltx_ref">2006</a>)</cite>, greedy layer-wise
training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib59" title="Greedy layer-wise training of deep networks" class="ltx_ref">2006</a>); Belilovsky<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Greedy layerwise learning can scale to imagenet" class="ltx_ref">2018</a>)</cite> and TargetProp&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio (<a href="#bib.bib51" title="How auto-encoders could provide credit assignment in deep networks via target propagation" class="ltx_ref">2014</a>)</cite>. More recently,
it is also shown to be possible to use a network to predict the weight changes of another
network&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jaderberg<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib17" title="Decoupled neural interfaces using synthetic gradients" class="ltx_ref">2017</a>); Metz<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib30" title="Meta-learning update rules for unsupervised representation learning" class="ltx_ref">2019</a>); Xiong<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib28" title="Learning to remember from a multi-task teacher" class="ltx_ref">2019</a>)</cite>, as well as to learn the meta-parameters of a
plasticity rule&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Miconi<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib27" title="Differentiable plasticity: training plastic neural networks with backpropagation" class="ltx_ref">2018</a>, <a href="#bib.bib29" title="Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity" class="ltx_ref">2019</a>)</cite>. Direct feedback alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Nøkland (<a href="#bib.bib69" title="Direct feedback alignment provides learning in deep neural networks" class="ltx_ref">2016</a>)</cite> on the
other hand proposed to directly learn the weights from the loss to each layer by using a random
backward layer. Despite numerous attempts at bringing biological plausibility to deep neural
networks, the performances of these learning algorithms are still far behind state-of-the-art
networks that are trained via end-to-end backpropagation on large scale datasets. A major difference
from prior literature is that, both GIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> and our LoCo use an entire downsampling stage
as a unit of local computation, instead of a single convolutional layer. In fact, different
downsampling stages have been found to have rough correspondence with the primate visual
cortex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schrimpf<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib68" title="Brain-score: which artificial neural network for object recognition is most brain-like?" class="ltx_ref">2018</a>); Zhuang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib67" title="Unsupervised neural network models of the ventral visual stream" class="ltx_ref">2020</a>)</cite>, and therefore they can probably be viewed as better
modeling tools for local learning. Nevertheless, we do not claim to have solved the local learning
problem on a more granular level.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Unsupervised &amp; self-supervised representation learning:</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">Since the success of
AlexNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Krizhevsky<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib19" title="ImageNet classification with deep convolutional neural networks" class="ltx_ref">2012</a>)</cite>, tremendous progress has been made in terms of learning representations
without class label supervision. One of such examples is self-supervised training
objectives&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Goyal<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib38" title="Scaling and benchmarking self-supervised visual representation learning" class="ltx_ref">2019</a>)</cite>, such as predicting context&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Doersch<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib37" title="Unsupervised visual representation learning by context prediction" class="ltx_ref">2015</a>); Noroozi and Favaro (<a href="#bib.bib35" title="Unsupervised learning of visual representations by solving jigsaw puzzles" class="ltx_ref">2016</a>)</cite>, predicting
rotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gidaris<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib36" title="Unsupervised representation learning by predicting image rotations" class="ltx_ref">2018</a>)</cite>, colorization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib39" title="Colorful image colorization" class="ltx_ref">2016</a>)</cite> and counting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Noroozi<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib40" title="Representation learning by learning to count" class="ltx_ref">2017</a>)</cite>.
Representations learned from these tasks can be further decoded into class labels by just training a
linear layer. Aside from predicting parts of input data, clustering objectives are also
considered&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhuang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib34" title="Local aggregation for unsupervised learning of visual embeddings" class="ltx_ref">2019</a>); Caron<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib41" title="Deep clustering for unsupervised learning of visual features" class="ltx_ref">2018</a>)</cite>. Unsupervised contrastive learning has recently emerged as a
promising direction for representation learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>); Tian<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib25" title="Contrastive multiview coding" class="ltx_ref">2019</a>); He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>); Misra and van der Maaten (<a href="#bib.bib31" title="Self-supervised learning of pretext-invariant representations" class="ltx_ref">2019</a>); Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>, achieving
state-of-the-art performance on ImageNet, closing the gap between supervised training and
unsupervised training with wider networks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>. Building on top of the InfoMax contrastive
learning rule&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite>, Greedy InfoMax (GIM)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> proposes to learn each local stage with
gradient blocks in the middle, effectively removing the backward dependency. This is similar to
block-wise greedy training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Belilovsky<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Greedy layerwise learning can scale to imagenet" class="ltx_ref">2018</a>)</cite> but in an unsupervised fashion.
</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Memory saving and model parallel computation:</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">By removing the data dependency in the
backward pass, our method can perform model parallel learning, and activations do not need to be
stored all the time to wait from the top layer. GPU memory can be saved by recomputing the
activations at the cost of longer training time&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib15" title="Training deep nets with sublinear memory cost" class="ltx_ref">2016</a>); Gruslys<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib16" title="Memory-efficient backpropagation through time" class="ltx_ref">2016</a>); Gomez<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib14" title="The reversible residual network: backpropagation without storing activations" class="ltx_ref">2017</a>)</cite>, whereas local learning
algorithms do not have such trade-off. Most parallel trainings of deep neural networks are achieved
by using data parallel training, with each GPU taking a portion of the input examples and then the
gradients are averaged. Although in the past model parallelism has also been used to vertically
split the network&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Krizhevsky<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib19" title="ImageNet classification with deep convolutional neural networks" class="ltx_ref">2012</a>); Krizhevsky (<a href="#bib.bib20" title="One weird trick for parallelizing convolutional neural networks" class="ltx_ref">2014</a>)</cite>, it soon went out of favor since the forward pass needs
to be synchronized. Data parallel training, on the other hand, can reach generalization bottleneck
with an extremely large batch size&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Shallue<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib21" title="Measuring the effects of data parallelismon neural network training" class="ltx_ref">2019</a>)</cite>. Recently, <cite class="ltx_cite ltx_citemacro_cite">Huang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib42" title="Gpipe: efficient training of giant neural networks using pipeline parallelism" class="ltx_ref">2019</a>); Narayanan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib22" title="PipeDream: generalized pipeline parallelism for dnn training" class="ltx_ref">2019</a>)</cite>
proposed to make a pipeline design among blocks of neural networks, to allow more forward passes
while waiting for the top layers to send gradients back. However, since they use end-to-end
backpropagation, they need to save previous activations in a data buffer to avoid numerical errors
when computing the gradients. By contrast, our local learning algorithm is a natural fit for model
parallelism, without the need for extra activation storage and wait time.

</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background: Unsupervised Contrastive Learning</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this section, we introduce relevant background on unsupervised contrastive learning using the
InfoNCE loss&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite>, as well as Greed InfoMax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite>, a local learning algorithm that
aims to learn each neural network stage with a greedy objective.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Unsupervised Contrastive Learning &amp; SimCLR</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Contrastive learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite> learns representations from data organized in similar or dissimilar
pairs. During learning, an encoder is used to learn meaningful representations and a decoder is
used to distinguish the positives from the negatives through the InfoNCE loss function&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite>,</p>
<div class="ltx_engrafo_equation_container"><table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S3.E1.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \mathcal{L}_{q,k^{+},\{k^{-}\}}=-\log\frac{\exp(q{\cdot}k^{+}/\tau)}{\exp(q{%
\cdot}k^{+}/\tau)+{\displaystyle\sum_{k^{-}}}\exp(q{\cdot}k^{-}/\tau)}. "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">L</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.296em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 13.368em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 13.368em; top: -1.608em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">exp</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span><span class="mjx-denominator" style="width: 13.368em; bottom: -2.345em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">exp</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-texatom MJXc-space2"><span class="mjx-mrow"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">∑</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.416em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">exp</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 13.368em;" class="mjx-line"></span></span><span style="height: 3.953em; vertical-align: -2.345em;" class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</tbody></table></div>
<p class="ltx_p">As shown above, the InfoNCE loss is essentially cross-entropy loss for classification with a
temperature scale factor <span id="S3.SS1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tau"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span></span></span></span></span></span>, where <span id="S3.SS1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span></span></span></span></span></span> and <span id="S3.SS1.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\{k\}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span></span></span></span></span></span> are normalized representation vectors from
the encoder. The positive pair <span id="S3.SS1.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(q,k^{+})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> needs to be classified among all <span id="S3.SS1.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(q,k)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> pairs. Note that
since the positive samples are defined as augmented version of the same example, this learning
objective does not need any class label information. After learning is finished, the decoder part
will be discarded and the encoder’s outputs will be served as learned representations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Recently, Chen et al. proposed SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite>, a state-of-the-art framework for contrastive
learning of visual representations. It proposes many useful techniques for closing the gap between
unsupervised and supervised representation learning. First, the learning benefits from a larger
batch size (~2k to 8k) and stronger data augmentation. Second, it uses a non-linear
MLP projection head instead of a linear layer as the decoder, making the representation more general
as it is further away from the contrastive loss function. With 4<span id="S3.SS1.p2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> the channel size, it is
able to match the performance of a fully supervised ResNet-50. In this paper, we use the SimCLR
algorithm as our end-to-end baseline as it is the current state-of-the-art. We believe that our
modifications can transfer to other contrastive learning algorithms as well.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Greedy InfoMax</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">As unsupervised learning has achieved tremendous progress, it is natural to ask whether we can
achieve the same from a local learning algorithm. Greedy InfoMax (GIM)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> proposed to
learn representation locally in each stage of the network, shown in the middle part of
Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It divides the encoder into several stacked modules, each with a
contrastive loss at the end. The input is forward-propagated in the usual way, but the gradients do
not propagate backward between modules. Instead, each module is trained greedily using a local
contrastive loss. This work was proposed prior to SimCLR and achieved comparable results to
CPC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite>, an earlier work, on a small scale dataset STL-10&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Coates<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib6" title="An analysis of single-layer networks in unsupervised feature learning" class="ltx_ref">2011</a>)</cite>. In this
paper, we used SimCLR as our main baseline, since it has superior performance on ImageNet, and we
apply the changes proposed in GIM on top of SimCLR as our local learning baseline. In our
experiments, we find that simply applying GIM on SimCLR results in a significant loss in performance
and in the next section we will explain our techniques to bridge the performance gap.

</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>LoCo: Local Contrastive Representation Learning</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we will introduce our approach to close the gap between local contrastive learning and state-of-the-art end-to-end learning.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="figures/mainfig_res.png" id="S4.F1.g1" class="ltx_graphics ltx_centering" width="810" height="212" alt="Comparison between End-to-End, Greedy InfoMax (GIM) and LoCo">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison between End-to-End, Greedy InfoMax (GIM) and LoCo</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">In the left part of Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show a regular end-to-end network using
backpropagation, where each rectangle denotes a downsample stage. In ResNet-50, they are <span class="ltx_text ltx_font_italic">conv1+res2</span>, <span class="ltx_text ltx_font_italic">res3</span>, <span class="ltx_text ltx_font_italic">res4</span>, <span class="ltx_text ltx_font_italic">res5</span>. In the middle we show GIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite>, where an
InfoNCE loss is added at the end of each local stage, and gradients do not flow back from upper
stages to lower stages. Our experimental results will show that such practice results in much worse
performance on large-scale datasets such as ImageNet. We hypothesize that it may be due to a lack of
feedback from upper layers and a lack of depth in terms of the decoders of lower layers, as they are
trying to greedily solve the classification problem. Towards fixing these two potential problems, on
the right hand side of Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we show our design: we group two stages into a
unit, and each middle stage is simultaneously shared by two units. Next, we will go into details
explaining our reasonings behind these design choices.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Bridging the Gap between Gradient Isolation Blocks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">First, in GIM, the feedback from high-level features is absent. When the difficulty of the
contrastive learning task increases (e.g., learning on a large-scale dataset such as ImageNet), the
quality of intermediate representations from lower layers will largely affect the final performance
of upper layers. However, such demand cannot be realized because lower layers are unaware of what
kind of representations are required from above.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">To overcome this issue, we hypothesize that it is essential to build a “bridge” between a lower
stage and its upper stage so that it can receive feedback that would otherwise be lost. As shown in
Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, instead of cutting the encoder into several non-overlapping parts, we can
overlap the adjacent local stages. Each stage now essentially performs a “look-ahead” when
performing local gradient descent. By chaining these overlapped blocks together, it is now possible
to send feedback from the very top.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">It is worth noting that, our method does not change the forward pass, even though <span class="ltx_text ltx_font_italic">res3</span> and
<span class="ltx_text ltx_font_italic">res4</span> appear twice in Fig.&nbsp;<a href="#S4.F1" title="Figure 1 ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, they receive the same inputs (from <span class="ltx_text ltx_font_italic">res2</span>
and <span class="ltx_text ltx_font_italic">res3</span>, respectively). Therefore the forward pass only needs to be done once in these
stages, and only the backward pass is doubled.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Deeper Decoder</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Second, we hypothesize that the receptive field of early stages in the encoder might be too small to
effectively solve the contrastive learning problem. As the same InfoNCE function is applied to all
local learning blocks (both early and late stages), it is difficult for the decoder to use
intermediate representation from the early stages to successful classify the positive sample,
because of the limitation of their receptive fields. For example, in the first stage, we need to
perform a global average pooling on the entire feature map with a spatial dimension of <span id="S4.SS2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="56\times 56"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">56</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">56</span></span></span></span></span></span></span>
before we send it to the decoder for classification.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">In Section&nbsp;<a href="#S5" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we empirically verify our hypothesis by showing that adding convolutional
layers into the decoder to enlarge the receptive field is essential for local algorithms. However,
this change does not show any difference in the end-to-end version with a single loss, since the
receptive field of the final stage is already large enough. Importantly, by having an overlapped
stage shared between local units, we effectively make decoders deeper without introducing extra cost
in the forward pass, simultaneously solving both issues described in this section.

</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this section, we conduct experiments to test the hypotheses we made in Section&nbsp;<a href="#S4" title="4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
and verify our design choices. Following previous works&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib39" title="Colorful image colorization" class="ltx_ref">2016</a>); van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>); Bachman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib9" title="Learning representations by maximizing mutual information across views" class="ltx_ref">2019</a>); Kolesnikov<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib5" title="Revisiting self-supervised visual representation learning" class="ltx_ref">2019</a>); He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>, we first evaluate the quality of the learned
representation using ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Deng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Imagenet: a large-scale hierarchical image database" class="ltx_ref">2009</a>)</cite>, followed by results on MS-COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib62" title="Microsoft COCO: common objects in context" class="ltx_ref">2014</a>)</cite>
and Cityscapes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cordts<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib63" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">2016</a>)</cite>. We use SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">2020a</a>)</cite> and GIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Löwe<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Putting an end to end-to-end: gradient-isolated learning of representations" class="ltx_ref">2019</a>)</cite> as our main
baselines, and consider both ResNet-50&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11" title="Deep residual learning for image recognition" class="ltx_ref">2016</a>)</cite> and ShuffleNet
v2-50&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ma<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib12" title="Shufflenet v2: practical guidelines for efficient cnn architecture design" class="ltx_ref">2018</a>)</cite> backbone architectures as the encoder network.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>ImageNet-1K</h3>

<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation details:</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Unless otherwise specified, we train with a batch size of 4096
using the LARS optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">You<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib43" title="Large batch training of convolutional networks" class="ltx_ref">2017</a>)</cite>. We train models 800 epochs to show that LoCo can
perform well on very long training schedules and match state-of-the-art performance; we use a
learning rate of 4.8 with a cosine decay schedule without restart&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib44" title="Sgdr: stochastic gradient descent with warm restarts" class="ltx_ref">2016</a>)</cite>; linear
warm-up is used for the first 10 epochs. Standard data augmentations such as random cropping, random
color distortion, and random Gaussian blurring are used. For local learning algorithms (i.e., GIM
and LoCo), 2-layer MLPs with global average pooling are used to project the intermediate features
into a 128-dim latent space, unless otherwise specified in ablation studies.
Following&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib39" title="Colorful image colorization" class="ltx_ref">2016</a>); van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>); Bachman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib9" title="Learning representations by maximizing mutual information across views" class="ltx_ref">2019</a>); Kolesnikov<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib5" title="Revisiting self-supervised visual representation learning" class="ltx_ref">2019</a>); He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>, we evaluate
the quality of the learned representation by freezing the encoder and training a linear classifier
on top of the trained encoders. SGD without momentum is used as the optimizer for 100 training
epochs with a learning rate of 30 and decayed by a factor of 10 at epoch 30, 60 and 90, the same
procedure done in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Main results:</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">As shown in Table&nbsp;<a href="#S5.T1" title="Table 1 ‣ Main results: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, SimCLR achieves favorable
results compared to other previous contrastive learning methods. For instance, CPC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">van den Oord<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Representation learning with contrastive predictive coding" class="ltx_ref">2018</a>)</cite>, the
contrastive learning algorithm which Greedy InfoMax (GIM) was originally based on, performs much
worse. By applying GIM on top of SimCLR, we see a significant drop of 5% on the top 1 accuracy. Our
method clearly outperforms GIM by a large margin, and is even slightly better than the end-to-end
SimCLR baseline, possibly caused by the fact that better representations are obtained via multiple
training losses applied at different local decoders.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Method</th>
<td class="ltx_td ltx_align_center ltx_border_tt">Architecture</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Local</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Local Agg.</th>
<td class="ltx_td ltx_align_center ltx_border_t">ResNet-50</td>
<td class="ltx_td ltx_align_center ltx_border_t">60.2</td>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">MoCo</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">60.6</td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">PIRL</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">63.6</td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CPC v2</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">63.8</td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">SimCLR*</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">69.3</td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<td class="ltx_td ltx_align_center ltx_border_t">ResNet-50</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">69.8</span></td>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">64.7</td>
<td class="ltx_td ltx_align_center">✓</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">LoCo (Ours)</th>
<td class="ltx_td ltx_align_center">ResNet-50</td>
<td class="ltx_td ltx_align_center">69.5</td>
<td class="ltx_td ltx_align_center">✓</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<td class="ltx_td ltx_align_center ltx_border_t">ShuffleNet v2-50</td>
<td class="ltx_td ltx_align_center ltx_border_t">69.1</td>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<td class="ltx_td ltx_align_center">ShuffleNet v2-50</td>
<td class="ltx_td ltx_align_center">63.5</td>
<td class="ltx_td ltx_align_center">✓</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LoCo (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb">ShuffleNet v2-50</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>ImageNet accuracies of linear classifiers trained on representations learned with different
unsupervised methods, SimCLR* is the result from the SimCLR paper with 1000 training epochs. </figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2">Arch</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">COCO</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Cityscapes</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">AP<span id="S5.T2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\text{bb}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em; width: 1em;">bb</span></span></span></span></span></span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td class="ltx_td ltx_align_center">AP<span id="S5.T2.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\text{bb}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em; width: 1em;">bb</span></span></span></span></span></span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center">AP</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Supervised</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">R-50</th>
<td class="ltx_td ltx_align_left ltx_border_t">33.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">31.3</td>
<td class="ltx_td ltx_align_left ltx_border_t">33.2</td>
<td class="ltx_td ltx_align_left ltx_border_t">27.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="6">Backbone weights with 100 Epochs</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">R-50</th>
<td class="ltx_td ltx_align_left ltx_border_t">32.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">29.9</td>
<td class="ltx_td ltx_align_left ltx_border_t">33.2</td>
<td class="ltx_td ltx_align_left ltx_border_t">28.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R-50</th>
<td class="ltx_td ltx_align_left">27.7 <span class="ltx_text" style="color:#FF0000;">(-4.5)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">25.7 <span class="ltx_text" style="color:#FF0000;">(-4.2)</span>
</td>
<td class="ltx_td ltx_align_left">30.0 <span class="ltx_text" style="color:#FF0000;">(-3.2)</span>
</td>
<td class="ltx_td ltx_align_left">24.6 <span class="ltx_text" style="color:#FF0000;">(-4.0)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R-50</th>
<td class="ltx_td ltx_align_left">32.6 <span class="ltx_text" style="color:#000000;">(+0.4)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">30.1 <span class="ltx_text" style="color:#000000;">(+0.2)</span>
</td>
<td class="ltx_td ltx_align_left">33.2 <span class="ltx_text" style="color:#000000;">(+0.0)</span>
</td>
<td class="ltx_td ltx_align_left">28.4 <span class="ltx_text" style="color:#000000;">(-0.2)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Sh-50</th>
<td class="ltx_td ltx_align_left ltx_border_t">32.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">30.1</td>
<td class="ltx_td ltx_align_left ltx_border_t">33.3</td>
<td class="ltx_td ltx_align_left ltx_border_t">28.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Sh-50</th>
<td class="ltx_td ltx_align_left">27.3 <span class="ltx_text" style="color:#FF0000;">(-5.2)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">25.4 <span class="ltx_text" style="color:#FF0000;">(-4.7)</span>
</td>
<td class="ltx_td ltx_align_left">29.1 <span class="ltx_text" style="color:#FF0000;">(-4.2)</span>
</td>
<td class="ltx_td ltx_align_left">23.9 <span class="ltx_text" style="color:#FF0000;">(-4.1)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Sh-50</th>
<td class="ltx_td ltx_align_left">31.8 <span class="ltx_text" style="color:#000000;">(-0.7)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">29.4 <span class="ltx_text" style="color:#000000;">(-0.7)</span>
</td>
<td class="ltx_td ltx_align_left">33.1 <span class="ltx_text" style="color:#000000;">(-0.2)</span>
</td>
<td class="ltx_td ltx_align_left">27.7 <span class="ltx_text" style="color:#000000;">(-0.3)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="6">Backbone weights with 800 Epochs</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">R-50</th>
<td class="ltx_td ltx_align_left ltx_border_t">34.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">32.2</td>
<td class="ltx_td ltx_align_left ltx_border_t">34.8</td>
<td class="ltx_td ltx_align_left ltx_border_t">30.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R-50</th>
<td class="ltx_td ltx_align_left">29.3 <span class="ltx_text" style="color:#FF0000;">(-5.5)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.0 <span class="ltx_text" style="color:#FF0000;">(-5.2)</span>
</td>
<td class="ltx_td ltx_align_left">30.7 <span class="ltx_text" style="color:#FF0000;">(-4.1)</span>
</td>
<td class="ltx_td ltx_align_left">26.0 <span class="ltx_text" style="color:#FF0000;">(-4.1)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R-50</th>
<td class="ltx_td ltx_align_left">34.5 <span class="ltx_text" style="color:#000000;">(-0.3)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">32.0 <span class="ltx_text" style="color:#000000;">(-0.2)</span>
</td>
<td class="ltx_td ltx_align_left">34.2 <span class="ltx_text" style="color:#000000;">(-0.6)</span>
</td>
<td class="ltx_td ltx_align_left">29.5 <span class="ltx_text" style="color:#000000;">(-0.6)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Sh-50</th>
<td class="ltx_td ltx_align_left ltx_border_t">33.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">30.9</td>
<td class="ltx_td ltx_align_left ltx_border_t">33.9</td>
<td class="ltx_td ltx_align_left ltx_border_t">28.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GIM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Sh-50</th>
<td class="ltx_td ltx_align_left">28.9 <span class="ltx_text" style="color:#FF0000;">(-4.5)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">26.9 <span class="ltx_text" style="color:#FF0000;">(-4.0)</span>
</td>
<td class="ltx_td ltx_align_left">29.6 <span class="ltx_text" style="color:#FF0000;">(-4.3)</span>
</td>
<td class="ltx_td ltx_align_left">23.9 <span class="ltx_text" style="color:#FF0000;">(-4.8)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Sh-50</th>
<td class="ltx_td ltx_align_left ltx_border_bb">33.6 <span class="ltx_text" style="color:#000000;">(+0.2)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">31.2 <span class="ltx_text" style="color:#000000;">(+0.3)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb">33.0 <span class="ltx_text" style="color:#000000;">(-0.9)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb">28.1 <span class="ltx_text" style="color:#000000;">(-0.6)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Mask R-CNN results on COCO and Cityscapes. Backbone networks are frozen. “R-50” denotes
ResNet-50 and “Sh-50” denotes ShuffleNet v2-50.</figcaption>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Performance on Downstream Tasks</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">In order to further verify the quality and generalizability of the learned representations, we use
the trained encoder from previous section as pre-trained models to perform downstream tasks, We use
Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib64" title="Mask R-CNN" class="ltx_ref">2017</a>)</cite> on Cityscapes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cordts<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib63" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">2016</a>)</cite> and COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib62" title="Microsoft COCO: common objects in context" class="ltx_ref">2014</a>)</cite> to evaluate object
detection and instance segmentation performance. Unlike what has been done in MoCo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>,
where the whole network is finetuned on downstream task, here we freeze the pretrained backbone
network, so that we better distinguish the differences in quality of different unsupervised learning
methods.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation details:</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">To mitigate the distribution gap between features from the
supervised pre-training model and contrastive learning model, and reuse the same hyperparameters
that are selected for the supervised pre-training model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>, we add
SyncBN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Peng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib7" title="Megdet: a large mini-batch object detector" class="ltx_ref">2018</a>)</cite> after all newly added layers in FPN and bbox/mask heads. The two-layer
MLP box head is replaced with a <span class="ltx_text ltx_font_italic">4conv-1fc</span> box head to better leverage
SyncBN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Wu and He (<a href="#bib.bib48" title="Group normalization" class="ltx_ref">2018</a>)</cite>. We conduct the downstream task experiments using
mmdetection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib49" title="MMDetection: open mmlab detection toolbox and benchmark" class="ltx_ref">2019</a>)</cite>. Following&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">2019a</a>)</cite>, we use the same hyperparameters as the
ImageNet supervised counterpart for all experiments, with <span id="S5.SS2.SSS0.Px1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> (<span id="S5.SS2.SSS0.Px1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span></span></span></span></span>12 epochs) schedule for
COCO and 64 epochs for Cityscapes, respectively. Besides SimCLR and GIM, we provide one more
baseline using weights pretrained on ImageNet via supervised learning provided by
PyTorch<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="https://download.pytorch.org/models/resnet50-19c8e357.pth" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://download.pytorch.org/models/resnet50-19c8e357.pth</a></span></span></span> for reference.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results:</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">From the Table&nbsp;<a href="#S5.T2" title="Table 2 ‣ Main results: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we can clearly see that the conclusion is
consistent on downstream tasks. Better accuracy on ImageNet linear evaluation also translates to
better instance segmentation quality on both COCO and Cityscapes. LoCo not only closes the gap
with end-to-end baselines on object classification in the training domain but also on downstream
tasks in new domains.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">Surprisingly, even though SimCLR and LoCo cannot exactly match “Supervised” on ImageNet, they
are 1 – 2 points AP better than “Supervised” on downstream tasks. This shows unsupervised
representation learning can learn more generalizable features that are more transferable to new
domains.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">Pretrain</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">COCO-10K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">COCO-1K</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Method</th>
<td class="ltx_td ltx_align_center">AP<span id="S5.T3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\text{bb}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em; width: 1em;">bb</span></span></span></span></span></span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td class="ltx_td ltx_align_center">AP<span id="S5.T3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\text{bb}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-font-inherit" style="font-size: 100%; padding-bottom: 0.3em; width: 1em;">bb</span></span></span></span></span></span></span></span></span></span></span></span>
</td>
<td class="ltx_td ltx_align_center">AP</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Random Init</th>
<td class="ltx_td ltx_align_left ltx_border_t">23.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">22.0</td>
<td class="ltx_td ltx_align_left ltx_border_t">2.5</td>
<td class="ltx_td ltx_align_left ltx_border_t">2.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Supervised</th>
<td class="ltx_td ltx_align_left">26.0</td>
<td class="ltx_td ltx_align_left ltx_border_r">23.8</td>
<td class="ltx_td ltx_align_left">10.4</td>
<td class="ltx_td ltx_align_left">10.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5">Pretrained weights with 100 Epochs</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SimCLR</th>
<td class="ltx_td ltx_align_left ltx_border_t">25.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">23.9</td>
<td class="ltx_td ltx_align_left ltx_border_t">11.3</td>
<td class="ltx_td ltx_align_left ltx_border_t">11.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GIM</th>
<td class="ltx_td ltx_align_left">22.6 <span class="ltx_text" style="color:#FF0000;">(-3.0)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">20.8 <span class="ltx_text" style="color:#FF0000;">(-3.1)</span>
</td>
<td class="ltx_td ltx_align_left">&nbsp;&nbsp; 9.7 <span class="ltx_text" style="color:#FF0000;">(-1.6)</span>
</td>
<td class="ltx_td ltx_align_left">&nbsp;&nbsp; 9.6 <span class="ltx_text" style="color:#FF0000;">(-1.8)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_left">26.1 <span class="ltx_text" style="color:#000000;">(+0.3)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">24.2 <span class="ltx_text" style="color:#000000;">(+0.5)</span>
</td>
<td class="ltx_td ltx_align_left">11.7 <span class="ltx_text" style="color:#000000;">(+0.4)</span>
</td>
<td class="ltx_td ltx_align_left">11.8 <span class="ltx_text" style="color:#000000;">(+0.4)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5">Pretrained weights with 800 Epochs</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SimCLR</th>
<td class="ltx_td ltx_align_left ltx_border_t">27.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25.2</td>
<td class="ltx_td ltx_align_left ltx_border_t">13.9</td>
<td class="ltx_td ltx_align_left ltx_border_t">14.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GIM</th>
<td class="ltx_td ltx_align_left">24.4 <span class="ltx_text" style="color:#FF0000;">(-2.8)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r">22.4 <span class="ltx_text" style="color:#FF0000;">(-2.8)</span>
</td>
<td class="ltx_td ltx_align_left">11.5 <span class="ltx_text" style="color:#FF0000;">(-2.4)</span>
</td>
<td class="ltx_td ltx_align_left">11.7 <span class="ltx_text" style="color:#FF0000;">(-2.4)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_left ltx_border_bb">27.8 <span class="ltx_text" style="color:#000000;">(+0.6)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">25.6 <span class="ltx_text" style="color:#000000;">(+0.4)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb">13.9 <span class="ltx_text" style="color:#000000;">(+0.0)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb">13.8 <span class="ltx_text" style="color:#000000;">(-0.3)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Mask R-CNN results on 10K COCO images and 1K COCO images</figcaption>
</figure>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Downstream Tasks with Limited Labeled Data</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">With the power of unsupervised representation learning, one can learn a deep model with much less
amount of labeled data on downstream tasks. Following&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="Rethinking imagenet pre-training" class="ltx_ref">2019b</a>)</cite>, we randomly sample
10k and 1k COCO images for training, namely COCO-10K and COCO-1K. These are 10% and 1% of the
full COCO train2017 set. We report AP on the official val2017 set. Besides SimCLR and GIM, we also
provide two baselines for reference: “Supervised” as mentioned in previous subsection, and
“Random Init” does not use any pretrained weight but just uses random initialization for all
layers and trains from scratch.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">Hyperparameters are kept the same as &nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="Rethinking imagenet pre-training" class="ltx_ref">2019b</a>)</cite> with multi-scale training except for
adjusted learning rate and decay schedules. We train models for 60k iterations (96 epochs) on
COCO-10K and 15k iterations (240 epochs) on COCO-1K with a batch size of 16. All models use
ResNet-50 as the backbone and are finetuned with SyncBN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Peng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib7" title="Megdet: a large mini-batch object detector" class="ltx_ref">2018</a>)</cite>, <span class="ltx_text ltx_font_italic">conv1</span> and <span class="ltx_text ltx_font_italic">res2</span> are frozen except “Random Init" entry. We make 5 random splits for both COCO-10K/1K and run
all entries on these 5 splits and take the average. The results are very stable and the variance is
very small (<span id="S5.SS3.p2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="<0.2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.2</span></span></span></span></span></span></span>).</p>
</div>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results:</h4>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Experimental results are shown in Table&nbsp;<a href="#S5.T3" title="Table 3 ‣ Results: ‣ 5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Random initialization is
significantly worse than other models that are pretrained on ImageNet, in agreement with the results
reported by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="Rethinking imagenet pre-training" class="ltx_ref">2019b</a>)</cite>. With weights pretrained for 100 epochs, both SimCLR and LoCo
get sometimes better performance compared to supervised pre-training, especially toward the regime
of limited labels (i.e., COCO-1K). This shows that the unsupervised features are more general as
they do not aim to solve the ImageNet classification problem. Again, GIM does not perform well and
cannot match the randomly initialized baseline. Since we do not finetune early stages, this suggests
that GIM does not learn generalizable features in its early stages. We conclude that our proposed
LoCo algorithm is able to learn generalizable features for downstream tasks, and is especially
beneficial when limited labeled data are available.</p>
</div>
<div id="S5.SS3.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">Similar to the previous subsection, we run pretraining longer until 800 epochs, and observe
noticeable improvements on both tasks and datasets. This results seem different from the one
reported in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib50" title="Improved baselines with momentum contrastive learning" class="ltx_ref">2020b</a>)</cite> that longer iterations help improve the ImageNet accuracy but do
not improve downstream VOC detection performance. Using 800 epoch pretraining, both LoCo and
SimCLR can outperform the supervised baseline by 2 points AP on COCO-10K and 4 points AP on COCO-1K.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Influence of the Decoder Depth</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">In this section, we study the influence of the decoder depth. First, we investigate the
effectiveness of the convolutional layers we add in the decoder. The results are shown in
Table&nbsp;<a href="#S5.T4" title="Table 4 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. As we can see from the “1 conv block without local and
sharing property” entry in the table, adding one more residual convolution block at the end of the
encoder, i.e. the beginning of the decoder, in the original SimCLR does not help. One possible
reason is that the receptive field is large enough at the very end of the encoder. However, adding
one convolution block with downsampling before the global average pooling operation in the decoder
will significantly improve the performance of local contrastive learning. We argue that such a
convolution block will enlarge the receptive field as well as the capacity of the local decoders and
lead to better representation learning even with gradient isolation. If the added convolution block
has no downsampling factor (denoted as “w/o ds”), the improvement is not be as significant.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p class="ltx_p">We also try adding more convolution layers in the decoder, including adding two convolution
blocks (denoted as “2 conv blocks”), adding one stage to make the decoder as deep as the
next residual stage of the encoder (denoted as “one stage”), as well as adding layers to make each
decoder as deep as the full Res-50 encoder (denoted as “full network”). The results of these
entries show that adding more convolution layers helps, but the improvement will eventually diminish
and these entries achieve the same performance as SimCLR.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p class="ltx_p">Lastly, we show that by adding two more layers in the MLP decoders, i.e. four layers in total, we
can observe the same amount of performance boost on all of methods, as shown in the 4th to 6th row
of Table&nbsp;<a href="#S5.T4" title="Table 4 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. However, increasing MLP decoder depth cannot help us bridge
the gap between local and end-to-end contrastive learning.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p class="ltx_p">To reduce the overhead we introduce in the decoder, we decide to add one residual convolution block
only and keep the MLP depth to 2, as was done the original SimCLR. It is also worth noting that by
sharing one stage of the encoder, our method can already closely match SimCLR without deeper
decoders, as shown in the third row of Table&nbsp;<a href="#S5.T4" title="Table 4 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Extra Layers before MLP Decoder</th>
<td class="ltx_td ltx_align_center ltx_border_tt">Local</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Sharing</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Acc.</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">None</th>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">65.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">None</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">60.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">1 conv block</th>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">65.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">1 conv block (w/o ds)</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">63.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">1 conv block</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">65.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">2 conv blocks</th>
<td class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">65.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">1 stage</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">65.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">full network</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">65.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">2-layer MLP</th>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">67.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">2-layer MLP</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">62.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t">66.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours + 2-layer MLP</th>
<td class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">67.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>ImageNet accuracies of models with different decoder architecture. All entries are trained
with 100 epochs. </figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Sharing description</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No sharing</td>
<td class="ltx_td ltx_align_center ltx_border_t">65.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Upper layer grad only</td>
<td class="ltx_td ltx_align_center">65.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">L2 penalty (1e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_t">65.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">L2 penalty (1e-3)</td>
<td class="ltx_td ltx_align_center">66.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">L2 penalty (1e-2)</td>
<td class="ltx_td ltx_align_center">65.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Sharing 1 block</td>
<td class="ltx_td ltx_align_center ltx_border_t">64.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Sharing 2 blocks</td>
<td class="ltx_td ltx_align_center">65.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Sharing 1 stage</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">66.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>ImageNet accuracies of models with different sharing strategies. All entries are trained
with 100 epochs.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Influence of the Sharing Strategy</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p class="ltx_p">As we argued in Sec.&nbsp;<a href="#S4.SS1" title="4.1 Bridging the Gap between Gradient Isolation Blocks ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> that local contrastive learning may suffer from
gradient isolation, it is important to verify this situation and know how to build a feedback
mechanism properly. In Table&nbsp;<a href="#S5.T5" title="Table 5 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we explore several sharing strategies to
show their impact of the performance. All entries are equipped with 1 residual convolution block +
2-layer MLP decoders.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p class="ltx_p">We would like to study what kind of sharing can build implicit feedback. In LoCo the shared stage
between two local learning modules is updated by gradients associated with losses from both lower
and upper local learning modules. Can implicit feedback be achieved by another way? To answer this
question, we try to discard part of the gradients of a block shared in both local and upper local
learning modules. Only the gradients calculated from the loss associated with the upper module will
be kept to update the weights. This control is denoted as “Upper layer grad only” in
Table&nbsp;<a href="#S5.T5" title="Table 5 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and the result indicates that although the performance is slightly
improved compared to not sharing any encoder blocks, it is worse than taking gradients from both
sides.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p class="ltx_p">We also investigate soft sharing, i.e. weights are not directly shared in different local learning
modules but are instead softly tied using L2 penalty on the differences. For each layer in the
shared stage, e.g., layers in <span class="ltx_text ltx_font_italic">res3</span>, the weights are identical in different local learning
modules upon initialization, and they will diverge as the training progress goes on. We add an L2
penalty on the difference of the weights in each pair of local learning modules, similar to L2
regularization on weights during neural network training. We try three different coefficients from
1e-2 to 1e-4 to control the strength of soft sharing. The results in
Table&nbsp;<a href="#S5.T5" title="Table 5 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show that soft sharing also brings improvements but it is slightly
worse than hard sharing. Note that with this strategy the forward computation cannot be shared and
the computation cost is increased. Thus we believe that soft sharing is not an ideal way to achieve
good performance.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p class="ltx_p">Finally, we test whether sharing can be done with fewer residual convolution blocks between local
learning modules rather than a whole stage, in other words, we vary the size of the local learning
modules to observe any differences. We try to make each module contain only one stage plus a few
residual blocks at the beginning of the next stage instead of two entire stages. Therefore, only the
blocks at the beginning of stages are shared between different modules. This can be seen as a smooth
transition between GIM and LoCo. We try only sharing the first block or first two blocks of each
stage, leading to “Sharing 1 block” and “Sharing 2 blocks” entries in
Table&nbsp;<a href="#S5.T5" title="Table 5 ‣ 5.4 Influence of the Decoder Depth ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The results show that sharing fewer blocks of each stage will not
improve performance and sharing only 1 block will even hurt.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Memory Saving</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p class="ltx_p">Although local learning saves GPU memory, we find that the original ResNet-50 architecture prevents
LoCo to further benefit from local learning, since ResNet-50 was designed with balanced
computation cost at each stage and memory footprint was not taken into consideration. In ResNet,
when performing downsampling operations at the beginning of each stage, the spatial dimension is
reduced by <span id="S5.SS6.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1/4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span> but the number of channels only doubles, therefore the memory usage of the lower
stage will be twice as much as the upper stage. Such design choice makes <span class="ltx_text ltx_font_italic">conv1</span> and <span class="ltx_text ltx_font_italic">res2</span>
almost occupy 50% of the network memory footprint. When using ResNet-50, the memory saving ratio
of GIM is <span id="S5.SS6.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1.81\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1.81</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> compared to the original, where the memory saving ratio is defined as the
reciprocal of peak memory usage between two models. LoCo can achieve <span id="S5.SS6.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1.28\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1.28</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> memory saving
ratio since it needs to store one extra stage.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p class="ltx_p">We also show that by properly designing the network architecture, we can make training benefit more
from local learning. We change the 4-stage ResNet to a 6-stage variant with a more progressive
downsampling mechanism. In particular, each stage has 3 residual blocks, leading to a Progressive
ResNet-50 (PResNet-50). Table&nbsp;<a href="#S5.T6" title="Table 6 ‣ 5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares memory footprint and computation of each
stage for PResNet-56 and ResNet-50 in detail. The number of base channels for each stage are 56, 96,
144, 256, 512, 1024, respectively. After <span class="ltx_text ltx_font_italic">conv1</span> and <span class="ltx_text ltx_font_italic">pool1</span>, we gradually downsample the
feature map resolution from 56x56 to 36x36, 24x24, 16x16, 12x12, 8x8 at each stage with bilinear
interpolation instead of strided convolution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib11" title="Deep residual learning for image recognition" class="ltx_ref">2016</a>)</cite>. Grouped convolution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Krizhevsky<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib19" title="ImageNet classification with deep convolutional neural networks" class="ltx_ref">2012</a>)</cite>
with 2, 16, 128 groups is used in the last three stages respectively to reduce the computation cost.
The difference between PResNet-56 and ResNet-50 and block structures are illustrated in
appendix.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p class="ltx_p">By simply making this modification without other new techniques&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib4" title="Bag of tricks for image classification with convolutional neural networks" class="ltx_ref">2019c</a>); Hu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib3" title="Squeeze-and-excitation networks" class="ltx_ref">2018</a>); Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib2" title="Selective kernel networks" class="ltx_ref">2019</a>)</cite>, we can get a network that matches the ResNet-50 performance with similar
computation costs. More importantly, it has balanced memory footprint at each stage. As shown in
Table&nbsp;<a href="#S5.T7" title="Table 7 ‣ 5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, SimCLR using PResNet-50 gets 66.8% accuracy, slightly better
compared to the ResNet-50 encoder. Using PResNet-50, our method performs on par with SimCLR while
still achieving remarkable memory savings of 2.76 <span id="S5.SS6.p3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>. By contrast, GIM now has an even larger
gap (14 points behind SimCLR) compared to before with ResNet-50, possibly due to the receptive field
issue we mentioned in Sec.&nbsp;<a href="#S4.SS2" title="4.2 Deeper Decoder ‣ 4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2">Stage</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">PResNet-50</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">ResNet-50</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Mem.</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">(%)</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">FLOPS</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">(%)</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Mem.</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">(%)</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">FLOPS</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">(%)</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">res2</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.64</td>
<td class="ltx_td ltx_align_center ltx_border_t">19.39</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">res3</th>
<td class="ltx_td ltx_align_center ltx_border_r">10.96</td>
<td class="ltx_td ltx_align_center ltx_border_r">14.63</td>
<td class="ltx_td ltx_align_center ltx_border_r">29.09</td>
<td class="ltx_td ltx_align_center">25.09</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">res4</th>
<td class="ltx_td ltx_align_center ltx_border_r">19.48</td>
<td class="ltx_td ltx_align_center ltx_border_r">14.77</td>
<td class="ltx_td ltx_align_center ltx_border_r">21.82</td>
<td class="ltx_td ltx_align_center">35.80</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">res5</th>
<td class="ltx_td ltx_align_center ltx_border_r">17.31</td>
<td class="ltx_td ltx_align_center ltx_border_r">16.62</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.45</td>
<td class="ltx_td ltx_align_center">19.73</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">res6</th>
<td class="ltx_td ltx_align_center ltx_border_r">19.48</td>
<td class="ltx_td ltx_align_center ltx_border_r">20.45</td>
<td class="ltx_td ltx_align_center ltx_border_r">-</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">res7</th>
<td class="ltx_td ltx_align_center ltx_border_r">17.31</td>
<td class="ltx_td ltx_align_center ltx_border_r">20.04</td>
<td class="ltx_td ltx_align_center ltx_border_r">-</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">FLOPs</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" colspan="2">4.16G</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="2">4.14G</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Memory footprint and computation percentages for PResNet-50 and ResNet-50 on stage level.</figcaption>
</figure>
<figure id="S5.T7" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Memory Saving Ratio</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SimCLR</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">1<span id="S5.T7.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GIM</th>
<td class="ltx_td ltx_align_center ltx_border_r">52.6</td>
<td class="ltx_td ltx_align_center">4.56<span id="S5.T7.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">LoCo</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">66.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb">2.76<span id="S5.T7.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>ImageNet accuracies and memory saving ratio of Progressive ResNet-50 with balanced memory
footprint at each stage. All entries are trained with 100 epochs.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We have presented LoCo, a local learning algorithm for unsupervised contrastive learning. We show
that by introducing implicit gradient feedback between the gradient isolation blocks and properly
deepening the decoders, we can largely close the gap between local contrastive learning and
state-of-the-art end-to-end contrastive learning frameworks. Experiments on ImageNet and downstream
tasks show that LoCo can learn good visual representations for both object recognition and
instance segmentation just like end-to-end approaches can. Meanwhile, it can benefit from nice
properties of local learning, such as lower peak memory footprint and faster model parallel
training.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib56" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. F. Abbott and S. B. Nelson (2000)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Synaptic plasticity: taming the beast</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature neuroscience</span> <span class="ltx_text ltx_bib_volume">3</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1178–1183</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Bachman, R. D. Hjelm, and W. Buchwalter (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning representations by maximizing mutual information across views</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NeurIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. Belilovsky, M. Eickenberg, and E. Oyallon (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Greedy layerwise learning can scale to imagenet</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1812.11446</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Greedy layer-wise training of deep networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NIPS</span>,  <span class="ltx_text ltx_bib_editor">B. Schölkopf, J. C. Platt, and T. Hofmann (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Bengio, D. Lee, J. Bornschein, and Z. Lin (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards biologically plausible deep learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1502.04156</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Bengio (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How auto-encoders could provide credit assignment in deep networks via target propagation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1407.7906</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. L. Bienenstock, L. N. Cooper, and P. W. Munro (1982)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Neuroscience</span> <span class="ltx_text ltx_bib_volume">2</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;32–48</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Caporale and Y. Dan (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spike timing–dependent plasticity: a hebbian learning rule</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Annu. Rev. Neurosci.</span> <span class="ltx_text ltx_bib_volume">31</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;25–46</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Caron, P. Bojanowski, A. Joulin, and M. Douze (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep clustering for unsupervised learning of visual features</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">15th European Conference on Computer Vision, ECCV</span>,  <span class="ltx_text ltx_bib_editor">V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li, X. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C. Loy, and D. Lin (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MMDetection: open mmlab detection toolbox and benchmark</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1906.07155</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.SSS0.Px1.p1" title="Implementation details: ‣ 5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Chen, B. Xu, C. Zhang, and C. Guestrin (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Training deep nets with sublinear memory cost</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1604.06174</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton (2020a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A simple framework for contrastive learning of visual representations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/2002.05709</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS1.p2" title="3.1 Unsupervised Contrastive Learning &amp; SimCLR ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Chen, H. Fan, R. Girshick, and K. He (2020b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved baselines with momentum contrastive learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2003.04297</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS3.SSS0.Px1.p2" title="Results: ‣ 5.3 Downstream Tasks with Limited Labeled Data ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Coates, A. Ng, and H. Lee (2011)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An analysis of single-layer networks in unsupervised feature learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">14th International Conference on Artificial Intelligence and Statistics, AISTATS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p1" title="3.2 Greedy InfoMax ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The cityscapes dataset for semantic urban scene understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition,
CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.SS2.p1" title="5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Imagenet: a large-scale hierarchical image database</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Doersch, A. Gupta, and A. A. Efros (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised visual representation learning by context prediction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Gidaris, P. Singh, and N. Komodakis (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised representation learning by predicting image rotations</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">6th International Conference on Learning Representations, ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The reversible residual network: backpropagation without storing activations</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Goyal, D. Mahajan, A. Gupta, and I. Misra (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scaling and benchmarking self-supervised visual representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2019 IEEE/CVF International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Gruslys, R. Munos, I. Danihelka, M. Lanctot, and A. Graves (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Memory-efficient backpropagation through time</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick (2019a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Momentum contrast for unsupervised visual representation learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1911.05722</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.SS2.SSS0.Px1.p1" title="Implementation details: ‣ 5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a href="#S5.SS2.p1" title="5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. He, R. Girshick, and P. Dollár (2019b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking imagenet pre-training</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS3.SSS0.Px1.p1" title="Results: ‣ 5.3 Downstream Tasks with Limited Labeled Data ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>,
<a href="#S5.SS3.p1" title="5.3 Downstream Tasks with Limited Labeled Data ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>,
<a href="#S5.SS3.p2" title="5.3 Downstream Tasks with Limited Labeled Data ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. He, G. Gkioxari, P. Dollár, and R. B. Girshick (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mask R-CNN</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p1" title="5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. He, X. Zhang, S. Ren, and J. Sun (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep residual learning for image recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.SS6.p2" title="5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.6</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li (2019c)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bag of tricks for image classification with convolutional neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS6.p3" title="5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.6</span></a>.
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. O. Hebb (1949)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The organization of behavior: a neuropsychological theory</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">J. Wiley; Chapman &amp; Hall</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. E. Hinton, S. Osindero, and Y. W. Teh (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A fast learning algorithm for deep belief nets</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Neural Computation</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;1527–1554</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. E. Hinton (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A practical guide to training restricted boltzmann machines</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Neural networks: Tricks of the trade</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;599–619</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Hu, L. Shen, and G. Sun (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Squeeze-and-excitation networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS6.p3" title="5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.6</span></a>.
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, <span class="ltx_text ltx_bib_etal">et al.</span> (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gpipe: efficient training of giant neural networks using pipeline parallelism</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NeurIPS</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;103–112</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and K. Kavukcuoglu (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Decoupled neural interfaces using synthetic gradients</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">34th International Conference on Machine Learning, ICML</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Kolesnikov, X. Zhai, and L. Beyer (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Revisiting self-supervised visual representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Krizhevsky, I. Sutskever, and G. E. Hinton (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ImageNet classification with deep convolutional neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS6.p2" title="5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.6</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Krizhevsky (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">One weird trick for parallelizing convolutional neural networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1404.5997</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Li, W. Wang, X. Hu, and J. Yang (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Selective kernel networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS6.p3" title="5.6 Memory Saving ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.6</span></a>.
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Microsoft COCO: common objects in context</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">13th European Conference on Computer Vision, ECCV</span>,  <span class="ltx_text ltx_bib_editor">D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.SS2.p1" title="5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">I. Loshchilov and F. Hutter (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sgdr: stochastic gradient descent with warm restarts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1608.03983</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Löwe, P. O’Connor, and B. S. Veeling (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Putting an end to end-to-end: gradient-isolated learning of representations</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems, NeurIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">LoCo: Local Contrastive Representation Learning</span></span>,
<a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS2.p1" title="3.2 Greedy InfoMax ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a href="#S3.p1" title="3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>,
<a href="#S4.p2" title="4 LoCo: Local Contrastive Representation Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Ma, X. Zhang, H. Zheng, and J. Sun (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Shufflenet v2: practical guidelines for efficient cnn architecture design</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">15th European Conference on Computer Vision, ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. v. d. Maaten and G. Hinton (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Visualizing data using t-sne</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of machine learning research</span> <span class="ltx_text ltx_bib_volume">9</span> (<span class="ltx_text ltx_bib_number">Nov</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;2579–2605</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A3.p1" title="Appendix C Representation visualization ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>.
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-Dickstein (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Meta-learning update rules for unsupervised representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">7th International Conference on Learning Representations, ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Miconi, A. Rawal, J. Clune, and K. O. Stanley (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">7th International Conference on Learning Representations, ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Miconi, K. O. Stanley, and J. Clune (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Differentiable plasticity: training plastic neural networks with backpropagation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">35th International Conference on Machine Learning, ICML</span>,  <span class="ltx_text ltx_bib_editor">J. G. Dy and A. Krause (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">I. Misra and L. van der Maaten (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Self-supervised learning of pretext-invariant representations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1912.01991</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PipeDream: generalized pipeline parallelism for dnn training</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">27th ACM Symposium on Operating Systems Principles, SOSP</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Nøkland (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Direct feedback alignment provides learning in deep neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems, NeurIPS</span>,  <span class="ltx_text ltx_bib_editor">D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Noroozi and P. Favaro (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised learning of visual representations by solving jigsaw puzzles</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">14th European Conference on Computer Vision - ECCV</span>,  <span class="ltx_text ltx_bib_editor">B. Leibe, J. Matas, N. Sebe, and M. Welling (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Noroozi, H. Pirsiavash, and P. Favaro (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Representation learning by learning to count</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. Oja (1982)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Simplified neuron model as a principal component analyzer</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of mathematical biology</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;267–273</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Peng, T. Xiao, Z. Li, Y. Jiang, X. Zhang, K. Jia, G. Yu, and J. Sun (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Megdet: a large mini-batch object detector</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.SSS0.Px1.p1" title="Implementation details: ‣ 5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a href="#S5.SS3.p2" title="5.3 Downstream Tasks with Limited Labeled Data ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Schrimpf, J. Kubilius, H. Hong, N. J. Majaj, R. Rajalingham, E. B. Issa, K. Kar, P. Bashivan, J. Prescott-Roy, K. Schmidt, D. L. K. Yamins, and J. J. DiCarlo (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Brain-score: which artificial neural network for object recognition is most brain-like?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">bioRxiv</span> <span class="ltx_text ltx_bib_volume">10.1101/407007</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Measuring the effects of data parallelismon neural network training</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">20</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px3.p1" title="Memory saving and model parallel computation: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_report">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Smolensky (1986)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Information processing in dynamical systems: foundations of harmony theory</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Colorado Univ at Boulder Dept of Computer Science</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Tian, D. Krishnan, and P. Isola (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contrastive multiview coding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1906.05849</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. van den Oord, Y. Li, and O. Vinyals (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Representation learning with contrastive predictive coding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1807.03748</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Unsupervised Contrastive Learning &amp; SimCLR ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S3.SS2.p1" title="3.2 Greedy InfoMax ‣ 3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a href="#S3.p1" title="3 Background: Unsupervised Contrastive Learning ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>,
<a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.SS1.SSS0.Px2.p1" title="Main results: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Wu and K. He (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Group normalization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">15th European Conference on Computer Vision, ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.SSS0.Px1.p1" title="Implementation details: ‣ 5.2 Performance on Downstream Tasks ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Xiong, M. Ren, and R. Urtasun (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to remember from a multi-task teacher</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1910.04650</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. You, I. Gitman, and B. Ginsburg (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Large batch training of convolutional networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1708.03888</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Zhang, P. Isola, and A. A. Efros (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Colorful image colorization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">14th European Conference on Computer Vision, ECCV</span>,  <span class="ltx_text ltx_bib_editor">B. Leibe, J. Matas, N. Sebe, and M. Welling (Eds.)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS1.SSS0.Px1.p1" title="Implementation details: ‣ 5.1 ImageNet-1K ‣ 5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.p1" title="5 Experiments ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Zhuang, S. Yan, A. Nayebi, M. Schrimpf, M. C. Frank, J. J. DiCarlo, and D. L. K. Yamins (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised neural network models of the ventral visual stream</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">bioRxiv</span> <span class="ltx_text ltx_bib_volume">10.1101/2020.06.16.155556</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px1.p1" title="Neural network local learning rules: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Zhuang, A. L. Zhai, and D. Yamins (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Local aggregation for unsupervised learning of visual embeddings</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE/CVF International Conference on Computer Vision, ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.Px2.p1" title="Unsupervised &amp; self-supervised representation learning: ‣ 2 Related Work ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training Curves</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">We provide training loss curves of SimCLR, GIM and LoCo for a better understanding of the
performance gap between them. Contrastive losses computed using outputs from the full ResNet-50
encoder are shown in Fig.&nbsp;<a href="#A1.F2" title="Figure 2 ‣ Appendix A Training Curves ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For GIM and LoCo, losses from other decoders,
including <span class="ltx_text ltx_font_italic">res2</span>, <span class="ltx_text ltx_font_italic">res3</span>, <span class="ltx_text ltx_font_italic">res4</span>, are also provided. As we can see in
Fig.&nbsp;<a href="#A1.F2" title="Figure 2 ‣ Appendix A Training Curves ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the losses of different decoders in LoCo closely match the loss of
the decoder in SimCLR during training, with the exception of <span class="ltx_text ltx_font_italic">res2</span>, while for GIM this is not
the case.</p>
</div>
<figure id="A1.F2" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="x1.png" id="A1.F2.g1" class="ltx_graphics" width="405" height="270" alt="Training loss curves for SimCLR, GIM and LoCo"></td>
<td class="ltx_subgraphics"><img src="x2.png" id="A1.F2.g2" class="ltx_graphics" width="405" height="270" alt="Training loss curves for SimCLR, GIM and LoCo"></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Training loss curves for SimCLR, GIM and LoCo</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Architecture of Progressive ResNet-50</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">In this section we show the block structure of each stage in Progressive ResNet-50 in
Table&nbsp;<a href="#A2.T8" title="Table 8 ‣ Appendix B Architecture of Progressive ResNet-50 ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The block structure of ResNet-50 is also shown here for reference. We
downsample the feature map size progressively using bilinear interpolation, and use basic blocks to
reduce the memory footprint of earlier stages, and group convolution to reduce the computation cost
of later stages to get a model with more balanced computation and memory footprint at each stage. We
use this model to show the great potential of LoCo in terms of both memory saving and computation
for model parallelism. As it is designed to have 15~20 memory footprint and
computation cost per stage, the peak memory usage will be significantly reduced in local learning,
and no worker that handles a stage of the encoder will become a computation bottleneck in model
parallelism.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2">layer</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">PResNet-50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">ResNet-50</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">output size</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">block structure</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">output size</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">block structure</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">conv1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">112<span id="A2.T8.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>112</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7x7, 32, stride 2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">112<span id="A2.T8.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>112</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7<span id="A2.T8.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>7, 64, stride 2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2">res2_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2">56<span id="A2.T8.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3<span id="A2.T8.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3 max pool, stride 2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2">56<span id="A2.T8.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3<span id="A2.T8.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3 max pool, stride 2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m8" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m10" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m11" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">res3_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36<span id="A2.T8.m12" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>36</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m13" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m14" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28<span id="A2.T8.m15" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m16" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m17" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">res4_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24<span id="A2.T8.m18" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m19" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m20" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14<span id="A2.T8.m21" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m22" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m23" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">res5_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16<span id="A2.T8.m24" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m25" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m26" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m27" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m28" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>2</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7<span id="A2.T8.m29" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A2.T8.m30" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m31" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">res6_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12<span id="A2.T8.m32" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m33" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m34" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m35" class="ltx_Math"><span class="mjpage"></span></span><span id="A2.T8.m36" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>2</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">res7_x</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8<span id="A2.T8.m37" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m38" class="ltx_Math"><span class="mjpage"></span></span> <span id="A2.T8.m39" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<span id="A2.T8.m40" class="ltx_Math"><span class="mjpage"></span></span> <span id="A2.T8.m41" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>2</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_b ltx_border_l ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1<span id="A2.T8.m42" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">average pool, 1000-d fc</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1<span id="A2.T8.m43" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">average pool, 1000-d fc</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Architectural details of Progressive ResNet-50 and ResNet-50. Output sizes for both models are specified individually</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Representation visualization</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">In this section we show some visualization results of the learned representation
of SimCLR, GIM and
LoCo. We subsample images belonging to the first 10 classes of ImageNet-1K from
the validation set (500 images in total) and use t-SNE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Maaten and Hinton (<a href="#bib.bib1" title="Visualizing data using t-sne" class="ltx_ref">2008</a>)</cite> to visualize the
4096-d vector representation from the PResNet-50 encoder. The results are shown in
Fig&nbsp;<a href="#A3.F3" title="Figure 3 ‣ Appendix C Representation visualization ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We can see LoCo learns image
embedding vectors that can form more compact clusters compared to GIM.</p>
</div>
<figure id="A3.F3" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subgraphics"><img src="x3.png" id="A3.F3.g1" class="ltx_graphics" width="270" height="180" alt="t-SNE visualization results for SimCLR, GIM and LoCo"></td>
<td class="ltx_subgraphics"><img src="x4.png" id="A3.F3.g2" class="ltx_graphics" width="270" height="180" alt="t-SNE visualization results for SimCLR, GIM and LoCo"></td>
<td class="ltx_subgraphics"><img src="x5.png" id="A3.F3.g3" class="ltx_graphics" width="270" height="180" alt="t-SNE visualization results for SimCLR, GIM and LoCo"></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>t-SNE visualization results for SimCLR, GIM and LoCo</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Qualitative results for downstream tasks</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p">Last, we show qualitative results of detection and instance segmentation tasks on COCO in Fig.&nbsp;<a href="#A4.F4" title="Figure 4 ‣ Appendix D Qualitative results for downstream tasks ‣ LoCo: Local Contrastive Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A4.F4" class="ltx_figure"><img src="figures/coco_vis.png" id="A4.F4.g1" class="ltx_graphics ltx_centering" width="810" height="877" alt="Qualitative results on COCO-10K, LoCo trained on 800 epochs with ResNet-50 is used to
initialize the Mask R-CNN model">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative results on COCO-10K, LoCo trained on 800 epochs with ResNet-50 is used to
initialize the Mask R-CNN model</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>


<script src="index.js" type="text/javascript"></script></body></html>