<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="../style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4CCK7L860L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4CCK7L860L');
</script>
<body>
    <div class="ribbon">
 
</div>
<h1 id="publications-by-year">Publications by Year</h1>
<p>[<a
href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google
Scholar</a>] [<a
href="https://dblp.org/pers/hd/r/Ren:Mengye">dblp</a>]</p>
<hr />
<h2 id="preprints">Preprints</h2>
<ul>
<li><p><span class="paper-title">Self-supervised learning of video
representations from a child’s perspective.</span> Emin Orhan, Wentao
Wang, Alex N. Wang, Mengye Ren, Brenden M. Lake. <em>arXiv preprint
2402.00300</em>, 2024. [<a
href="https://arxiv.org/abs/2402.00300">arxiv</a>]</p></li>
<li><p><span class="paper-title">Learning and forgetting unsafe examples
in large language models.</span> Jiachen Zhao, Zhun Deng, David Madras,
James Zou, Mengye Ren. <em>arXiv preprint 2312.12736</em>, 2023. [<a
href="https://arxiv.org/abs/2312.12736">arxiv</a>]</p></li>
<li><p><span class="paper-title">LifelongMemory: Leveraging LLMs for
answering queries in egocentric videos.</span> Ying Wang, Yanlai Yang,
Mengye Ren. <em>arXiv preprint 2312.05269</em>, 2023. [<a
href="https://lifelongmemory.github.io/">webpage</a>] [<a
href="https://arxiv.org/abs/2312.05269">arxiv</a>]</p></li>
<li><p><span class="paper-title">BIM: Block-wise self-supervised
learning with masked image modeling.</span> Yixuan Luo, Mengye Ren, Sai
Qian Zhang. <em>arXiv preprint 2311.17218</em>, 2023. [<a
href="https://arxiv.org/abs/2311.17218">arxiv</a>]</p></li>
<li><p><span class="paper-title">Learning to reason with relational
abstractions.</span> Andrew J. Nam<code>*</code>, Mengye
Ren<code>*</code>, Chelsea Finn, James L. McClelland. <em>arXiv preprint
2210.02615</em>, 2022. [<a
href="https://arxiv.org/abs/2210.02615">arxiv</a>] [<a
href="2022/learning-to-reason-with-relational-abstractions/nam-2022-learning.pdf">pdf</a>]
[<a
href="https://github.com/renmengye/grade-school-math-relational">dataset</a>]</p></li>
<li><p><span class="paper-title">Gaussian-Bernoulli RBMs without
tears.</span> Renjie Liao, Simon Kornblith, Mengye Ren, David J. Fleet,
Geoffrey Hinton. <em>arXiv preprint 2210.10318</em>, 2022. [<a
href="https://arxiv.org/abs/2210.10318">arxiv</a>]</p></li>
</ul>
<hr />
<h2 id="section">2023</h2>
<ul>
<li><p><span class="paper-title"><a
href="2023/scaling-forward-gradient-with-local-losses">Scaling forward
gradient with local losses</a>.</span> Mengye Ren, Simon Kornblith,
Renjie Liao, Geoffrey Hinton. <em>ICLR</em>, 2023. [<a
href="https://arxiv.org/abs/2210.03310">arxiv</a>] [<a
href="2023/scaling-forward-gradient-with-local-losses/ren-2023-scaling.pdf">pdf</a>]
[<a
href="https://github.com/google-research/google-research/tree/master/local_forward_gradient">code</a>]
[<a
href="2023/scaling-forward-gradient-with-local-losses">html</a>]</p></li>
<li><p><span class="paper-title">Learning in temporally structured
environments.</span> Matt Jones, Tyler R. Scott, Mengye Ren, Gamaleldin
F. Elsayed, Katherine Hermann, David Mayo, Michael C. Mozer.
<em>ICLR</em>, 2023. [<a
href="2023/learning-in-temporally-structured-environments/jones-2023-learning.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title">Multitask learning via interleaving: A
neural network investigation.</span> David Mayo, Tyler Scott, Mengye
Ren, Gamaleldin Elsayed, Katherine Hermann, Matt Jones, Michael Mozer.
<em>CogSci</em>, 2023. [<a
href="2023/multitask-learning-via-interleaving-a-neural-network-investigation/mayo-2023-multitask.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title">Towards unsupervised object detection
from LiDAR point clouds.</span> Lunjun Zhang, Anqi Joyce Yang, Yuwen
Xiong, Sergio Casas, Bin Yang, Mengye Ren, Raquel Urtasun.
<em>CVPR</em>, 2023. [<a
href="https://arxiv.org/abs/2311.02007">arxiv</a>] [<a
href="2023/towards-unsupervised-object-detection-from-lidar-point-clouds/zhang-2023-towards.pdf">pdf</a>]
[<a
href="https://research-assets.waabi.ai/wp-content/uploads/2023/05/oyster_video-2.mp4">video</a>]
[<a href="https://waabi.ai/oyster">website</a>]</p></li>
<li><p><span class="paper-title">Egocentric video comprehension via
large language model inner speech.</span> Ying Wang, Dongdong Sun, Rui
Chen, Yanlai Yang, Mengye Ren. <em>3rd International Ego4D Workshop at
CVPR</em>, 2023. [<a
href="2023/egocentric-video-comprehension-via-large-language-model-inner-speech/wang-2023-egocentric.pdf">pdf</a>]</p></li>
</ul>
<hr />
<h2 id="section-1">2022</h2>
<ul>
<li><p><span class="paper-title">Learning to reason with relational
abstractions.</span> Andrew J. Nam<code>*</code>, Mengye
Ren<code>*</code>, Chelsea Finn, James L. McClelland. <em>NeurIPS
MATH-AI Worshop</em>, 2022. [<a
href="https://arxiv.org/abs/2210.02615">arxiv</a>] [<a
href="2022/learning-to-reason-with-relational-abstractions/nam-2022-learning.pdf">pdf</a>]
[<a
href="https://github.com/renmengye/grade-school-math-relational">dataset</a>]</p></li>
<li><p><span class="paper-title">Neural network online training with
sensitivity to multiscale temporal structure.</span> Matt Jones, Tyler
R. Scott, Gamaleldin F. Elsayed, Mengye Ren, Katherine Hermann, David
Mayo, Michael C. Mozer. <em>NeurIPS MemARI Workshop</em>, 2022. [<a
href="2022/neural-network-online-training-with-sensitivity-to-multiscale-temporal-structure/jones-2022-neural.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title">Rethinking closed-loop training for
autonomous driving.</span> Chris Zhang<code>*</code>, Runsheng
Guo<code>*</code>, Wenyuan Zeng<code>*</code>, Yuwen Xiong, Binbin Dai,
Rui Hu, Mengye Ren, Raquel Urtasun. <em>ECCV</em>, 2022.
<!-- [[pdf](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990259.pdf)] -->
[<a href="https://arxiv.org/abs/2306.15713">arxiv</a>] [<a
href="2022/rethinking-closed-loop-training-for-autonomous-driving/zhang-2022-rethinking.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title">Open-world machine learning with
limited labeled data.</span> Mengye Ren. <em>Ph.D. Thesis, University of
Toronto</em>, 2022.
<!-- [[pdf](https://tspace.library.utoronto.ca/bitstream/1807/123215/2/Ren_Mengye_202206_PhD_thesis.pdf)] -->
[<a
href="2022/phd-thesis/Ren_Mengye_202206_PhD_thesis.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title"><a
href="2022/probing-few-shot-generalization-with-attributes">Probing
few-shot generalization with attributes</a>.</span> Mengye
Ren<code>*</code>, Eleni Triantafillou<code>*</code>, Kuan-Chieh
Wang<code>*</code>, James Lucas<code>*</code>, Jake Snell, Xaq Pitkow,
Andreas S. Tolias, Richard Zemel. <em>arXiv preprint 2012.05895</em>,
2020. [<a href="https://arxiv.org/abs/2012.05895">arxiv</a>] [<a
href="2022/probing-few-shot-generalization-with-attributes/ren-2022-probing.pdf">pdf</a>]
[<a
href="https://slideslive.at/38941548/flexible-fewshot-learning-of-contextual-similarities">video</a>]
[<a
href="2022/probing-few-shot-generalization-with-attributes">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="2022/online-unsupervised-learning-of-visual-representations-and-categories">Online
unsupervised learning of visual representations and
categories</a>.</span> Mengye Ren, Tyler R. Scott, Michael L. Iuzzolino,
Michael C. Mozer, Richard Zemel. <em>arXiv preprint 2109.05675</em>,
2021. [<a href="https://arxiv.org/abs/2109.05675">arxiv</a>] [<a
href="2022/online-unsupervised-learning-of-visual-representations-and-categories/ren-2022-online.pdf">pdf</a>]
[<a href="https://github.com/renmengye/online-unsup-proto-net">code</a>]
[<a
href="2022/online-unsupervised-learning-of-visual-representations-and-categories">html</a>]</p></li>
</ul>
<hr />
<h2 id="section-2">2021</h2>
<ul>
<li><p><span class="paper-title"><a
href="2021/self-supervised-representation-learning-from-flow-equivariance">Self-supervised
representation learning from flow equivariance</a>.</span> Yuwen Xiong,
Mengye Ren, Wenyuan Zeng, Raquel Urtasun. <em>ICCV</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06553">arxiv</a>] [<a
href="2021/self-supervised-representation-learning-from-flow-equivariance/xiong-2021-self.pdf">pdf</a>]
[<a
href="2021/self-supervised-representation-learning-from-flow-equivariance">html</a>]</p></li>
<li><p><span class="paper-title">Adversarial attacks on multi-agent
communication.</span> James Tu<code>*</code>, Tsunhsuan
Wang<code>*</code>, Jingkang Wang, Sivabalan Manivasagam, Mengye Ren,
Raquel Urtasun. <em>ICCV</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06560">arxiv</a>]</p></li>
<li><p><span class="paper-title">Just label what you need: Fine-grained
active selection for perception and prediction through partially labeled
scenes.</span> Sean Segal, Nishanth Kumar, Sergio Casas, Wenyuan Zeng,
Mengye Ren, Jingkang Wang, Raquel Urtasun. <em>CoRL</em>, 2021. [<a
href="https://arxiv.org/abs/2104.03956">arxiv</a>]</p></li>
<li><p><span class="paper-title">Exploring adversarial robustness of
multi-sensor perception systems in self driving.</span> James Tu,
Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar,
Ersin Yumer, Raquel Urtasun. <em>CoRL</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06784">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings">SketchEmbedNet:
Learning novel concepts by imitating drawings</a>.</span> Alexander
Wang<code>*</code>, Mengye Ren<code>*</code>, Richard Zemel.
<em>ICML</em>, 2021. [<a
href="https://arxiv.org/abs/2009.04806">arxiv</a>] [<a
href="2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings/wang-2021-sketch.pdf">pdf</a>]
[<a
href="2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="2021/wandering-within-a-world-online-contextualized-few-shot-learning">Wandering
within a world: Online contextualized few-shot learning</a>.</span>
Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel.
<em>ICLR</em>, 2021. [<a
href="https://arxiv.org/abs/2007.04546">arxiv</a>] [<a
href="2021/wandering-within-a-world-online-contextualized-few-shot-learning/ren-2021-wandering.pdf">pdf</a>]
[<a href="https://github.com/renmengye/oc-fewshot-public">code</a>] [<a
href="https://slideslive.com/38931573/wandering-within-a-world-online-contextualized-fewshot-learning">video</a>]
[<a
href="2021/wandering-within-a-world-online-contextualized-few-shot-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="2021/theoretical-bounds-on-estimation-error-for-meta-learning">Theoretical
bounds on estimation error for meta-learning</a>.</span> James Lucas,
Mengye Ren, Irene Kameni, Toniann Pitassi, Richard Zemel. <em>ICLR</em>,
2021. [<a href="https://arxiv.org/abs/2010.07140">arxiv</a>] [<a
href="2021/theoretical-bounds-on-estimation-error-for-meta-learning/lucas-2021-theoretical.pdf">pdf</a>]
[<a
href="https://slideslive.com/38954221/theoretical-bounds-on-estimation-error-for-metalearning">video</a>]
[<a
href="2021/theoretical-bounds-on-estimation-error-for-meta-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="2021/perceive-attend-and-drive-learning-spatial-attention-for-safe-self-driving">Perceive,
attend, and drive: Learning spatial attention for safe
self-driving</a>.</span> Bob Wei<code>*</code>, Mengye
Ren<code>*</code>, Wenyuan Zeng, Ming Liang, Bin Yang, Raquel Urtasun.
<em>ICRA</em>, 2021. [<a
href="https://arxiv.org/abs/2011.01153">arxiv</a>] [<a
href="2021/perceive-attend-and-drive-learning-spatial-attention-for-safe-self-driving/wei-2021-perceive.pdf">pdf</a>]
[<a href="https://youtu.be/3ffaQ2PIQCM">video</a>] [<a
href="2021/perceive-attend-and-drive-learning-spatial-attention-for-safe-self-driving">html</a>]</p></li>
<li><p><span class="paper-title">AdvSim: Generating safety-critical
scenarios for self-driving vehicles.</span> Jingkang Wang, Ava Pun,
James Tu, Sivabalan Manivasagam, Abbas Sadat, Sergio Casas, Mengye Ren,
Raquel Urtasun. <em>CVPR</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06549">arxiv</a>] [<a
href="2021/advsim-generating-safety-critical-scenarios-for-self-driving-vehicles/wang-2021-advsim.pdf">pdf</a>]</p></li>
<li><p><span class="paper-title">SceneGen: Learning to generate
realistic traffic scenes.</span> Shuhan Tan<code>*</code>, Kelvin
Wong<code>*</code>, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren,
Raquel Urtasun. <em>CVPR</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06541">arxiv</a>] [<a
href="2021/scenegen-learning-to-generate-realistic-traffic-scenes/tan-2021-scenegen.pdf">pdf</a>]</p></li>
</ul>
<hr />
<h2 id="section-3">2020</h2>
<ul>
<li><p><span class="paper-title"><a
href="2020/loco-local-contrastive-representation-learning">LoCo: Local
contrastive representation learning</a>.</span> Yuwen Xiong, Mengye Ren,
Raquel Urtasun. <em>NeurIPS</em>, 2020. [<a
href="https://arxiv.org/abs/2008.01342">arxiv</a>] [<a
href="2020/loco-local-contrastive-representation-learning/xiong-2020-loco.pdf">pdf</a>]
[<a
href="https://slideslive.com/38936405/loco-local-contrastive-representation-learning">video</a>]
[<a
href="2020/loco-local-contrastive-representation-learning">html</a>]</p></li>
<li><p><span class="paper-title">Multi-label incremental few-shot
learning for medical image pathology classifiers.</span> Laleh
Seyyed-Kalantari, Karsten Roth, Mengye Ren, Parsa Torabian, Joseph P.
Cohen, Marzyeh Ghassemi. <em>Medical Imaging Meets NeurIPS
Workshop</em>, 2020. [<a
href="https://slideslive.com/38942997/multilabel-incremental-fewshot-learning-for-medical-image-pathology-classifiers">video</a>]</p></li>
<li><p><span class="paper-title">Flexible Few-Shot Learning of
Contextual Similarities.</span> Mengye Ren<code>*</code>, Eleni
Triantafillou<code>*</code>, Kuan-Chieh Wang<code>*</code>, James
Lucas<code>*</code>, Jake Snell, Xaq Pitkow, Andreas S. Tolias, Richard
Zemel. <em>NeurIPS Meta-Learning Workshop</em>, 2020. [<a
href="https://meta-learn.github.io/2020/papers/49_paper.pdf">pdf</a>]
[<a
href="https://slideslive.at/38941548/flexible-fewshot-learning-of-contextual-similarities">video</a>]</p></li>
<li><p><span class="paper-title">Learning to communicate and correct
pose errors.</span> Nicholas Vadivelu, Mengye Ren, James Tu, Jingkang
Wang, Raquel Urtasun. <em>CoRL</em>, 2020. [<a
href="https://arxiv.org/abs/2011.05289">arxiv</a>] [<a
href="https://www.youtube.com/watch?v=Qu5KJofLRvs">video</a>]</p></li>
<li><p><span class="paper-title"><a
href="2020/multi-agent-routing-value-iteration-network">Multi-agent
routing value iteration network</a>.</span> Quinlan
Sykora<code>*</code>, Mengye Ren<code>*</code>, Raquel Urtasun.
<em>ICML</em>, 2020. [<a
href="https://arxiv.org/abs/2007.05096">arxiv</a>] [<a
href="2020/multi-agent-routing-value-iteration-network/sykora-2020-multi.pdf">pdf</a>]
[<a href="https://github.com/uber-research/MARVIN">code</a>] [<a
href="https://slideslive.com/38927801/multiagent-routing-value-iteration-network-marvin">video</a>]
[<a
href="2020/multi-agent-routing-value-iteration-network">html</a>]</p></li>
<li><p><span class="paper-title">Cost-efficient online hyperparameter
optimization.</span> Jingkang Wang<code>*</code>, Mengye
Ren<code>*</code>, Ilija Bogunovic, Yuwen Xiong, Raquel Urtasun.
<em>ICML RealML Workshop</em>, 2020. [<a
href="https://arxiv.org/abs/2101.06590">arxiv</a>] [<a
href="2020/cost-efficient-online-hyperparameter-optimization/wang-2020-cost.pdf">pdf</a>]
[<a
href="https://realworldml.github.io/files/slides/28_slide.pdf">slide</a>]</p></li>
<li><p><span class="paper-title">Perceive, predict, and plan: Safe
motion planning through interpretable semantic representations.</span>
Abbas Sadat<code>*</code>, Sergio Casas Romero<code>*</code>, Mengye
Ren, Xinyu Wu, Pranaab Dhawan, Raquel Urtasun. <em>ECCV</em>, 2020. [<a
href="https://arxiv.org/abs/2008.05930">arxiv</a>]</p></li>
<li><p><span class="paper-title">End-to-end contextual perception and
prediction with interaction transformer.</span> Lingyun (Luke) Li, Bin
Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal, Raquel Urtasun.
<em>IROS</em>, 2020. [<a
href="https://arxiv.org/abs/2008.05927">arxiv</a>]</p></li>
<li><p><span class="paper-title">Physically realizable adversarial
examples for LiDAR object detection.</span> James Tu, Mengye Ren,
Sivabalan Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng,
Raquel Urtasun. <em>CVPR</em>, 2020. [<a
href="https://arxiv.org/abs/2004.00543">arxiv</a>] [<a
href="https://www.youtube.com/watch?v=yP47irVRGZI">video</a>]</p></li>
</ul>
<hr />
<h2 id="section-4">2019</h2>
<ul>
<li><p><span class="paper-title">Learning to remember from a multi-task
teacher.</span> Yuwen Xiong<code>*</code>, Mengye Ren<code>*</code>,
Raquel Urtasun. <em>arXiv preprint 1910.04650</em>, 2019. [<a
href="https://arxiv.org/abs/1910.04650">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="2019/incremental-few-shot-learning-with-attention-attractor-networks">Incremental
few-shot learning with attention attractor networks</a>.</span> Mengye
Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel. <em>NeurIPS</em>,
2019. [<a href="https://arxiv.org/abs/1810.07218">arxiv</a>] [<a
href="https://github.com/renmengye/inc-few-shot-attractor-public">code</a>]
[<a
href="2019/incremental-few-shot-learning-with-attention-attractor-networks">html</a>]</p></li>
<li><p><span class="paper-title">Information-theoretic limitations on
novel task generalization.</span> James Lucas, Mengye Ren, Richard S.
Zemel. <em>NeurIPS Workshop on Machine Learning with Guarantees</em>,
2019. [<a
href="https://drive.google.com/file/d/10pNGWyVYruL-yjjF3NVoKp6GjLZDWj8U/view">pdf</a>]</p></li>
<li><p><span class="paper-title">Deformable filter convolution for point
cloud reasoning.</span> Yuwen Xiong<code>*</code>, Mengye
Ren<code>*</code>, Renjie Liao, Kelvin Wong, Raquel Urtasun. <em>NeurIPS
Workshop on Sets &amp; Partitions</em>, 2019. [<a
href="https://arxiv.org/abs/1907.13079">arxiv</a>]</p></li>
<li><p><span class="paper-title">Identifying unknown instances for
autonomous driving.</span> Kelvin Wong, Shenlong Wang, Mengye Ren, Ming
Liang, Raquel Urtasun. <em>CoRL</em>, 2019. [<a
href="https://arxiv.org/abs/1910.11296">arxiv</a>]</p></li>
<li><p><span class="paper-title">Jointly learnable behavior and
trajectory planning for self-driving vehicles.</span> Abbas
Sadat<code>*</code>, Mengye Ren<code>*</code>, Andrei Pokrovsky,
Yen-Chen Lin, Ersin Yumer, Raquel Urtasun. <em>IROS</em>, 2019. [<a
href="https://arxiv.org/abs/1910.04586">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="2019/graph-hypernetworks-for-neural-architecture-search">Graph
hypernetworks for neural architecture search</a>.</span> Chris Zhang,
Mengye Ren, Raquel Urtasun. <em>ICLR</em>, 2019. [<a
href="https://arxiv.org/abs/1810.05749">arxiv</a>] [<a
href="2019/graph-hypernetworks-for-neural-architecture-search">html</a>]</p></li>
</ul>
<hr />
<h2 id="section-5">2018</h2>
<ul>
<li><p><span class="paper-title"><a
href="2018/learning-to-reweight-examples-for-robust-deep-learning">Learning
to reweight examples for robust deep learning</a>.</span> Mengye Ren,
Wenyuan Zeng, Bin Yang, Raquel Urtasun. <em>ICML</em>, 2018. [<a
href="https://arxiv.org/abs/1803.09050">arxiv</a>] [<a
href="https://github.com/uber-research/learning-to-reweight-examples">code</a>]
[<a href="https://vimeo.com/287808016">video</a>] [<a
href="2018/learning-to-reweight-examples-for-robust-deep-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="2018/sbnet-sparse-blocks-network-for-fast-inference">SBNet: Sparse
blocks network for fast inference</a>.</span> Mengye Ren<code>*</code>,
Andrei Pokrovsky<code>*</code>, Bin Yang<code>*</code>, Raquel Urtasun.
<em>CVPR</em>, 2018. [<a href="sbnet/index.html">link</a>] [<a
href="https://arxiv.org/abs/1801.02108">arxiv</a>] [<a
href="https://eng.uber.com/sbnet">blog</a>] [<a
href="https://github.com/uber/sbnet">code</a>] [<a
href="2018/sbnet-sparse-blocks-network-for-fast-inference">html</a>]</p></li>
<li><p><span class="paper-title">Meta-learning for semi-supervised
few-shot classification.</span> Mengye Ren, Eleni
Triantafillou<code>*</code>, Sachin Ravi<code>*</code>, Jake Snell,
Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel.
<em>ICLR</em>, 2018. [<a href="fewshotssl/index.html">link</a>] [<a
href="https://arxiv.org/abs/1803.00676">arxiv</a>] [<a
href="https://github.com/renmengye/few-shot-ssl-public">code</a>]</p></li>
<li><p><span class="paper-title"><a
href="2018/understanding-short-horizon-bias-in-stochastic-meta-optimization">Understanding
short-horizon bias in stochastic meta-optimization</a>.</span> Yuhuai
Wu<code>*</code>, Mengye Ren<code>*</code>, Renjie Liao, Roger B.
Grosse. <em>ICLR</em>, 2018. [<a href="metaoptim/index.html">link</a>]
[<a href="https://arxiv.org/abs/1803.02021">arxiv</a>] [<a
href="https://github.com/renmengye/meta-optim-public">code</a>] [<a
href="2018/understanding-short-horizon-bias-in-stochastic-meta-optimization">html</a>]</p></li>
</ul>
<hr />
<h2 id="section-6">2017</h2>
<ul>
<li><p><span class="paper-title">The reversible residual network:
Backpropagation without storing actications.</span> Aidan N.
Gomez<code>*</code>, Mengye Ren<code>*</code>, Raquel Urtasun, Roger B.
Grosse. <em>NIPS</em>, 2017. [<a href="revnet/index.html">link</a>] [<a
href="https://arxiv.org/abs/1707.04585">arxiv</a>] [<a
href="https://github.com/renmengye/revnet-public">code</a>]</p></li>
<li><p><span class="paper-title">Normalizing the normalizers: Comparing
and extending network normalization schemes.</span> Mengye
Ren<code>*</code>, Renjie Liao<code>*</code>, Raquel Urtasun, Fabian H.
Sinz, Richard S. Zemel. <em>ICLR</em>, 2017. [<a
href="divnorm/index.html">link</a>] [<a
href="https://arxiv.org/abs/1611.04520">arxiv</a>] [<a
href="https://github.com/renmengye/div-norm">code</a>]</p></li>
<li><p><span class="paper-title">End-to-end instance segmentation with
recurrent attention.</span> Mengye Ren, Richard S. Zemel. <em>CVPR</em>,
2017. [<a href="recattend/index.html">link</a>] [<a
href="https://arxiv.org/abs/1605.09410">arxiv</a>] [<a
href="https://github.com/renmengye/rec-attend-public">code</a>] [<a
href="https://www.youtube.com/watch?v=oHgUowLph7E">video</a>]</p></li>
</ul>
<hr />
<h2 id="section-7">2015</h2>
<ul>
<li><span class="paper-title">Exploring models and data for image
question answering.</span> Mengye Ren, Ryan Kiros, Richard S. Zemel.
<em>NIPS</em>, 2015. [<a href="imageqa/index.html">link</a>] [<a
href="https://arxiv.org/abs/1505.02074">arxiv</a>] [<a
href="imageqa/results">results</a>] [<a
href="imageqa/data/cocoqa">dataset</a>] [<a
href="https://github.com/renmengye/imageqa-public">code</a>] [<a
href="https://github.com/renmengye/imageqa-qgen">question
generation</a>]</li>
</ul>
<div class="ribbon">

</div>

</body>
</html>
