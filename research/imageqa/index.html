<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="../../style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<body>
    <title>
Image Question Answering
</title>
<div class="ribbon">

</div>
<h1>
Exploring Models and Data for Image Question Answering
</h1>
<!--<div class="author">
<p>-->
<p>Mengye Ren<sup>1</sup>, Ryan Kiros<sup>1</sup>, Richard S. Zemel<sup>1,2</sup><br /> <br /> <sup>1</sup>Department of Computer Science, University of Toronto, Toronto ON, CANADA<br /> <sup>2</sup>Canadian Institute for Advanced Research, Toronto ON, CANADA<br /> <!--</p>
</div>--> <br/> <img class="paper-fig" src="img/fig1.png" /></p>
<h2 id="abstract">Abstract</h2>
<p>This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.</p>
<hr />
<h2 id="full-paper">Full Paper</h2>
<!-- <img class="paper-snap" src="img/full.png" /> -->
<p>[<a href="papers/imageqa_nips2015.pdf">pdf</a>]</p>
<hr />
<h2 id="supplementary-materials">Supplementary Materials</h2>
<!-- <img class="paper-snap" src="img/supp.png" /> -->
<p>[<a href="papers/imageqa_supplementary_nips2015.pdf">pdf</a>]</p>
<hr />
<h2 id="dataset">Dataset</h2>
<p>[<a href="data/cocoqa">link</a>]</p>
<hr />
<h2 id="full-results">Full results</h2>
<p>[<a href="results">link</a>]</p>
<hr />
<h2 id="code">Code</h2>
<ul>
<li>To reproduce experimental results: [<a href="https://github.com/renmengye/imageqa-public">link</a>]</li>
<li>To generate questions: [<a href="https://github.com/renmengye/imageqa-qgen">link</a>]</li>
</ul>
<hr />
<h2 id="cite">Cite</h2>
<pre>
<code>
@inproceedings{ren2015imageqa,
  title={Exploring Models and Data for Image Question Answering},
  author={Mengye Ren and Ryan Kiros and Richard Zemel},
  booktitle={NIPS},
  year={2015}
}
</code>
</pre>
<div class="ribbon">

</div>

</body>
</html>
