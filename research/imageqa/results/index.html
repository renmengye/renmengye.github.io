<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="../../../style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<body>
    <title>
Image Question Answering
</title>
<div class="ribbon">

</div>
<h1 id="image-question-answering-full-results">Image Question Answering Full Results</h1>
<p>Reference: Mengye Ren, Ryan Kiros, Richard Zemel, &quot;Exploring Models and Data for Image Question Answering&quot;, NIPS 2015.</p>
<hr />
<h2 id="daquar-37">DAQUAR-37</h2>
<h3 id="dataset">Dataset</h3>
<p>References:</p>
<p>QAs: Mateusz Malinowski, Mario Fritz, &quot;Towards a Visual Turing Challenge&quot;, NIPS 2014 Workshop on Learning Semantics. [<a href="http://arxiv.org/abs/1410.8027">ArXiv</a>][<a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-%20computing/research/vision-and-language/visual-turing-challenge/">link</a>]</p>
<p>Images: Nathan Silberman, Pushmeet Kohli, Derek Hoiem, Rob Fergus, &quot;Indoor Segmentation and Support Inference from RGBD Images&quot;, ECCV 2012. [<a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">link</a>]</p>
<p>Notes:</p>
<ol style="list-style-type: decimal">
<li>Here we are only using DAQUAR-37 with one-word answers, a subset of the 37 object classes dataset.</li>
<li>Only test set results are rendered in the links below.</li>
</ol>
<h3 id="individual-models">Individual Models</h3>
<ul>
<li><a href="dq-2-vis-blstm/0.html">2-VIS+BLSTM</a></li>
<li><a href="dq-vis-lstm/0.html">VIS+LSTM</a></li>
<li><a href="dq-img-bow/0.html">IMG+BOW</a></li>
<li><a href="dq-lstm/0.html">LSTM</a></li>
<li><a href="dq-bow/0.html">BOW</a></li>
</ul>
<h3 id="model-comparison">Model Comparison</h3>
<ul>
<li><a href="dq-2-vis-blstm_vs_vis-lstm_vs_lstm">2-VIS+BLSTM vs VIS+LSTM vs LSTM</a></li>
</ul>
<hr />
<h2 id="toronto-coco-qa">Toronto COCO-QA</h2>
<p>QAs: [<a href="../data/cocoqa">link</a>]</p>
<p>Images: Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar and C. Lawrence Zitnick, &quot;Microsoft COCO: Common Objects in Context&quot;, ECCV 2014.</p>
<p>Notes:</p>
<ol style="list-style-type: decimal">
<li>All images are hosted on Flickr, and some links may not be available anymore.</li>
<li>Only test set results are rendered in the links below.</li>
</ol>
<h3 id="individual-models-1">Individual Models</h3>
<ul>
<li><a href="cq-2-vis-blstm/0.html">2-VIS+BLSTM</a></li>
<li><a href="cq-vis-lstm/0.html">VIS+LSTM</a></li>
<li><a href="cq-img-bow/0.html">IMG+BOW</a></li>
<li><a href="cq-img-prior/0.html">IMG+PRIOR</a></li>
<li><a href="cq-img/0.html">IMG</a></li>
<li><a href="cq-lstm/0.html">LSTM</a></li>
<li><a href="cq-bow/0.html">BOW</a></li>
</ul>
<h3 id="model-comparison-1">Model Comparison</h3>
<ul>
<li><a href="cq_img-bow_vs_2-vis-blstm_vs_img%20-prior_vs_bow">IMG+BOW vs 2-VIS+BLSTM vs IMG+PRIOR vs BOW</a></li>
</ul>
<div class="ribbon">

</div>

</body>
</html>
