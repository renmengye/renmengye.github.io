% !TEX root = ../main.tex

\ifarxiv
\vspace{-0.05in}
\fi
Semantic concepts are frequently defined by combinations of underlying
attributes. As mappings from attributes to classes are often simple,
attribute-based representations facilitate novel concept learning with zero or
few examples. A significant limitation of existing attribute-based learning
paradigms, such as zero-shot learning, is that the attributes are assumed to be
known and fixed. In this work we study the rapid learning of attributes that
were not previously labeled. Compared to standard few-shot learning of semantic
classes, in which novel classes may be defined by attributes that were relevant
at training time, learning new attributes imposes a stiffer challenge. We found
that supervised learning with training attributes does not generalize well to
new test attributes, whereas self-supervised pre-training brings significant
improvement. We further experimented with random splits of the attribute space
and found that predictability of test attributes provides an informative
estimate of a model's generalization ability.