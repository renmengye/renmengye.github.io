<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Theoretical bounds on estimation error for meta-learning</title>
<!--Generated on Mon Apr 10 10:29:12 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Theoretical bounds on estimation error for meta-learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi &amp; Richard Zemel
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Machine learning models have traditionally been developed under the assumption that the training and test distributions match exactly. However, recent success in few-shot learning and related problems are encouraging signs that these models can be adapted to more realistic settings where train and test distributions differ. Unfortunately, there is severely limited theoretical support for these algorithms and little is known about the difficulty of these problems. In this work, we provide novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are trained on data from multiple sources and tested on novel data. Our bounds depend intuitively on the information shared between sources of data, and characterize the difficulty of learning in this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian model of meta-learning, computing both upper and lower bounds on parameter estimation via maximum-a-posteriori inference.
</p>
  
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Many practical machine learning applications deal with distributional shift from training to testing. One example is few-shot classification
<cite class="ltx_cite ltx_citemacro_citep">(Ravi and Larochelle, <a href="#bib.bib15" title="Optimization as a model for few-shot learning" class="ltx_ref">2016</a>; Vinyals<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib16" title="Matching networks for one shot learning" class="ltx_ref">2016</a>)</cite>, where new classes need to be learned at test time based on only a few examples for each novel class. Recently, few-shot classification has seen increased success; however,
theoretical properties of this
problem remain poorly understood.
</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In this paper we analyze the <span class="ltx_text ltx_font_italic">meta-learning</span> setting, where
the learner is given access to samples from a set of meta-training distributions, or tasks. At test-time, the learner is exposed to only a small number of samples from some novel task. The meta-learner aims to uncover a useful inductive bias from the original samples, which allows them to learn a new task more efficiently.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
            <span class="ltx_tag ltx_tag_note">1</span>
            
            
            
            
            
            
            
          Note that this definition encompasses few-shot learning.</span></span></span>
While some progress has been made towards understanding the generalization performance of specific meta-learning algorithms <cite class="ltx_cite ltx_citemacro_citep">(Amit and Meir, <a href="#bib.bib18" title="Meta-learning by adjusting priors based on extended PAC-bayes theory" class="ltx_ref">2017</a>; Khodak<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib14" title="Provable guarantees for gradient-based meta-learning" class="ltx_ref">2019</a>; Bullins<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib42" title="Generalize across tasks: efficient algorithms for linear representation learning" class="ltx_ref">2019</a>; Denevi<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib43" title="Learning-to-learn stochastic gradient descent with biased regularization" class="ltx_ref">2019</a>; Cao<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib44" title="A theoretical analysis of the number of shots in few-shot learning" class="ltx_ref">2019</a>)</cite>, little is known about the difficulty of the meta-learning problem in general.
Existing work has studied generalization upper-bounds for novel data distributions <cite class="ltx_cite ltx_citemacro_citep">(Ben-David<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="A theory of learning from different domains" class="ltx_ref">2010</a>; Amit and Meir, <a href="#bib.bib18" title="Meta-learning by adjusting priors based on extended PAC-bayes theory" class="ltx_ref">2017</a>)</cite>, yet to our knowledge, the inherent difficulty of these tasks relative to the <em class="ltx_emph ltx_font_italic">i.i.d </em>case has not been characterized.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this work, we derive novel bounds for meta learners. We first present a general information theoretic lower bound, Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, that we use to derive bounds in particular settings. Using this result, we derive lower bounds in terms of the number of training tasks, data per training task, and data available in a novel target task. Additionally, we provide a specialized analysis for the case where the space of learning tasks is only partially observed, proving that infinite training tasks or data per training task are insufficient to achieve zero minimax risk (Corollary <a href="#Thmcorollary2" title="Corollary 2. ‣ A tighter bound on partially observed environments ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We then derive upper and lower bounds for a particular meta-learning setting. In recent work, <cite class="ltx_cite ltx_citemacro_citet">Grant<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib30" title="Recasting gradient-based meta-learning as hierarchical Bayes" class="ltx_ref">2018</a>)</cite> recast the popular meta-learning algorithm MAML <cite class="ltx_cite ltx_citemacro_citep">(Finn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>)</cite> in terms of inference in a Bayesian hierarchical model. Following this, we provide a theoretical analysis of a hierarchical Bayesian model for meta-linear-regression. We compute sample complexity bounds for posterior inference under Empirical Bayes <cite class="ltx_cite ltx_citemacro_citep">(Robbins, <a href="#bib.bib32" title="An empirical bayes approach to statistics" class="ltx_ref">1956</a>)</cite> in this model and compare them to our predicted lower-bounds in the minimax framework. Furthermore, through asymptotic analysis of the error rate of the MAP estimator, we identify crucial features of the meta-learning environment which are necessary for novel task generalization.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Our primary contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">We introduce novel lower bounds on minimax risk of parameter estimation in meta-learning.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Through these bounds, we compare the relative utility of samples from meta-training tasks and the novel task and emphasize the importance of the relationship between the tasks.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p">We provide novel upper bounds on the error rate for estimation in a hierarchical meta-linear-regression problem, which we verify through an empirical evaluation.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">An early version of this work <cite class="ltx_cite ltx_citemacro_citep">(Lucas<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib50" title="Information-theoretic limitations on novel task generalization" class="ltx_ref">2019</a>)</cite> presented a restricted version of Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The current version includes significantly more content, including more general lower bounds and corresponding upper bounds in a hierarchical Bayesian model of meta-learning (Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Baxter (<a href="#bib.bib4" title="A model of inductive bias learning" class="ltx_ref">2000</a>)</cite> introduced a formulation for inductive bias learning where the learner is embedded in an environment of multiple tasks. The learner must find a hypothesis space which enables good generalization on average tasks within the environment, using finite samples. In our setting, the learner is not explicitly tasked with finding a reduced hypothesis space but instead learns
using a general two-stage approach, which matches the standard meta-learning paradigm <cite class="ltx_cite ltx_citemacro_citep">(Vilalta and Drissi, <a href="#bib.bib7" title="A perspective view and survey of meta-learning" class="ltx_ref">2002</a>)</cite>. In the first stage an inductive bias is extracted from the data, and in the second stage the learner estimates using data from a novel task distribution. Further, we focus on bounding minimax risk of meta learners. Under minimax risk, an optimal learner achieves minimum error on the hardest learning problem in the environment. While average case risk of meta learners is more commonly studied, recent work has turned attention towards the minimax setting <cite class="ltx_cite ltx_citemacro_citep">(Kpotufe and Martinet, <a href="#bib.bib45" title="Marginal singularity, and the benefits of labels in covariate-shift" class="ltx_ref">2018</a>; Hanneke and Kpotufe, <a href="#bib.bib46" title="On the value of target data in transfer learning" class="ltx_ref">2019</a>, <a href="#bib.bib48" title="A no-free-lunch theorem for multitask learning" class="ltx_ref">2020</a>; Mousavi Kalan<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib51" title="Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks" class="ltx_ref">2020</a>; Mehta<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib49" title="Minimax multi-task learning and a generalized loss-compositional paradigm for mtl" class="ltx_ref">2012</a>)</cite>. The worst-case error
in meta-learning is particularly important in safety-critical systems, for example in medical diagnosis.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Mousavi Kalan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib51" title="Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks" class="ltx_ref">2020</a>)</cite> study the minimax risk of transfer learning. In their setting, the learner is provided with a large amount of data from a single source task and is tasked with generalizing to a target task with a limited amount of data. They assume relatedness between tasks by imposing closeness in parameter-space (whereas in our setting, we assume closeness in distribution via KL divergence). They prove only lower bounds, but notably generalize beyond the linear setting towards single layer neural networks.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">There is a large volume of prior work studying upper-bounds on generalization error in multi-task environments <cite class="ltx_cite ltx_citemacro_citep">(Ben-David and Borbely, <a href="#bib.bib28" title="A notion of task relatedness yielding provable multiple-task learning guarantees" class="ltx_ref">2008</a>; Ben-David<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="A theory of learning from different domains" class="ltx_ref">2010</a>; Pentina and Lampert, <a href="#bib.bib6" title="A PAC-bayesian bound for lifelong learning" class="ltx_ref">2014</a>; Amit and Meir, <a href="#bib.bib18" title="Meta-learning by adjusting priors based on extended PAC-bayes theory" class="ltx_ref">2017</a>; Mehta<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib49" title="Minimax multi-task learning and a generalized loss-compositional paradigm for mtl" class="ltx_ref">2012</a>)</cite>. While the approaches in these works vary, one common factor is the need to characterize task-relatedness. Broadly, these approaches either assume a shared distribution for sampling tasks <cite class="ltx_cite ltx_citemacro_citep">(Baxter, <a href="#bib.bib4" title="A model of inductive bias learning" class="ltx_ref">2000</a>; Pentina and Lampert, <a href="#bib.bib6" title="A PAC-bayesian bound for lifelong learning" class="ltx_ref">2014</a>; Amit and Meir, <a href="#bib.bib18" title="Meta-learning by adjusting priors based on extended PAC-bayes theory" class="ltx_ref">2017</a>)</cite>, or a measure of distance between distributions <cite class="ltx_cite ltx_citemacro_citep">(Ben-David and Borbely, <a href="#bib.bib28" title="A notion of task relatedness yielding provable multiple-task learning guarantees" class="ltx_ref">2008</a>; Ben-David<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="A theory of learning from different domains" class="ltx_ref">2010</a>; Mohri and Medina, <a href="#bib.bib41" title="New analysis and algorithm for learning with drifting distributions" class="ltx_ref">2012</a>)</cite>. Our lower-bounds utilize a weak form of task relatedness, assuming that the environment contains a finite set that is suitably separated in parameter space but close in KL divergence—this set of assumptions also arises often when computing <em class="ltx_emph ltx_font_italic">i.i.d </em>minimax lower bounds <cite class="ltx_cite ltx_citemacro_citep">(Loh, <a href="#bib.bib13" title="On lower bounds for statistical learning theory" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">One practical approach to meta-learning
is learning a linear mapping on top of a learned feature space.
Prototypical Networks <cite class="ltx_cite ltx_citemacro_citep">(Snell<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib22" title="Prototypical networks for few-shot learning" class="ltx_ref">2017</a>)</cite> effectively learn a discriminative embedding function and performs linear classification on top using the novel task data.
Analyzing these approaches is challenging due to metric-learning inspired objectives (that require non-<em class="ltx_emph ltx_font_italic">i.i.d </em>sampling) and the simultaneous learning of feature mappings and top-level linear functions. Though some progress has been made <cite class="ltx_cite ltx_citemacro_citep">(Jin<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib10" title="Regularized distance metric learning:theory and algorithm" class="ltx_ref">2009</a>; Saunshi<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib35" title="A theoretical analysis of contrastive unsupervised representation learning" class="ltx_ref">2019</a>; Wang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib8" title="Multitask metric learning: theory and algorithm" class="ltx_ref">2019</a>; Du<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib47" title="Few-shot learning via learning the representation, provably" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Maurer (<a href="#bib.bib34" title="Transfer bounds for linear feature learning" class="ltx_ref">2009</a>)</cite>, for example, explores linear models fitted over a shared linear feature map in a Hilbert space. Our results can be applied in these settings if a suitable packing of the representation space is defined.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">Other approaches to meta-learning aim to parameterize learning algorithms themselves. Traditionally, this has been achieved by hyper-parameter tuning <cite class="ltx_cite ltx_citemacro_citep">(Rasmussen and Nickisch, <a href="#bib.bib39" title="Gaussian processes for machine learning (GPML) toolbox" class="ltx_ref">2010</a>; MacKay<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib38" title="Self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions" class="ltx_ref">2019</a>)</cite> but
recent fully parameterized optimizers also show promising performance in deep neural network optimization <cite class="ltx_cite ltx_citemacro_citep">(Andrychowicz<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib37" title="Learning to learn by gradient descent by gradient descent" class="ltx_ref">2016</a>)</cite>, few-shot learning <cite class="ltx_cite ltx_citemacro_citep">(Ravi and Larochelle, <a href="#bib.bib15" title="Optimization as a model for few-shot learning" class="ltx_ref">2016</a>)</cite>, unsupervised learning <cite class="ltx_cite ltx_citemacro_citep">(Metz<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib40" title="Meta-learning update rules for unsupervised representation learning" class="ltx_ref">2019</a>)</cite>, and reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Duan<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib36" title="RL$^2$: fast reinforcement learning via slow reinforcement learning" class="ltx_ref">2016</a>)</cite>. Yet another approach learns the initialization of task-specific parameters, that are further adapted through regular gradient descent. Model-Agnostic Meta-Learning <cite class="ltx_cite ltx_citemacro_citep">(Finn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>)</cite>, or MAML, augments the global parameters with a meta-initialization of the weight parameters. <cite class="ltx_cite ltx_citemacro_citet">Grant<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib30" title="Recasting gradient-based meta-learning as hierarchical Bayes" class="ltx_ref">2018</a>)</cite> recast MAML in terms of inference in a Bayesian hierarchical model.
In Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we consider learning in a hierarchical environment of linear models and provide both lower and upper bounds on the error of estimating the parameters of a novel linear regression problem.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">Lower bounding estimation error is a critical component of understanding learning problems (and algorithms). Accordingly, there is a large body of literature producing such lower bounds <cite class="ltx_cite ltx_citemacro_citep">(Khas’ minskii, <a href="#bib.bib21" title="A lower bound on the risks of non-parametric estimates of densities in the uniform metric" class="ltx_ref">1979</a>; Yang and Barron, <a href="#bib.bib20" title="Information-theoretic determination of minimax rates of convergence" class="ltx_ref">1999</a>; Loh, <a href="#bib.bib13" title="On lower bounds for statistical learning theory" class="ltx_ref">2017</a>)</cite>. We focus on producing lower-bounds for parameter estimation using local packing sets, but expect that extending these results to density estimation or non-parametric estimation is feasible.

</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Novel task environment risk</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Most existing theoretical work studying out-of-distribution generalization focuses on providing upper-bounds on generalization performance <cite class="ltx_cite ltx_citemacro_citep">(Ben-David<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="A theory of learning from different domains" class="ltx_ref">2010</a>; Pentina and Lampert, <a href="#bib.bib6" title="A PAC-bayesian bound for lifelong learning" class="ltx_ref">2014</a>; Amit and Meir, <a href="#bib.bib18" title="Meta-learning by adjusting priors based on extended PAC-bayes theory" class="ltx_ref">2017</a>)</cite>. We begin by instead exploring the converse: what is the best performance we can hope to achieve on any given task in the environment? After introducing notation and minimax risks, we then show how these ideas can be applied, using meta linear regression as an example.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">A full reference table for notation can be found in Appendix <a href="#A1" title="Appendix A Notation ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> and a short summary is given here.
We consider algorithms that learn in an environment <math id="S3.p2.m1" class="ltx_Math" alttext="(\mathcal{Z},\mathcal{P})" display="inline"><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo stretchy="false">)</mo></mrow></math>, with data domain <math id="S3.p2.m2" class="ltx_Math" alttext="\mathcal{Z}=\mathcal{X}\times\mathcal{Y}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi></mrow></mrow></math> and <math id="S3.p2.m3" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> a space of distributions with support <math id="S3.p2.m4" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒵</mi></math>. In the typical <em class="ltx_emph ltx_font_italic">i.i.d </em>setting, the algorithm is provided training data <math id="S3.p2.m5" class="ltx_Math" alttext="S\in\mathcal{Z}^{k}" display="inline"><mrow><mi>S</mi><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒵</mi><mi>k</mi></msup></mrow></math>, consisting of <math id="S3.p2.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> <em class="ltx_emph ltx_font_italic">i.i.d </em>samples from <math id="S3.p2.m7" class="ltx_Math" alttext="P\in\mathcal{P}" display="inline"><mrow><mi>P</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">In the standard <span class="ltx_text ltx_font_italic">multi-task</span> setting, we sample training data from a set of training tasks <math id="S3.p3.m1" class="ltx_Math" alttext="\{P_{1},\ldots,P_{M+1}\}\subset\mathcal{P}" display="inline"><mrow><mrow><mo stretchy="false">{</mo><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo></mrow><mo>⊂</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math>.
We extend this to a meta-learning, or <span class="ltx_text ltx_font_italic">novel-task</span> setting by first drawing <math id="S3.p3.m2" class="ltx_Math" alttext="S_{1:M}" display="inline"><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math>: <math id="S3.p3.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> training data points from the first <math id="S3.p3.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> distributions, for a total of <math id="S3.p3.m5" class="ltx_Math" alttext="nM" display="inline"><mrow><mi>n</mi><mo>⁢</mo><mi>M</mi></mrow></math> samples. We call this the <span class="ltx_text ltx_font_italic">meta-training set</span>. We then draw a small sample of novel data,
called a <span class="ltx_text ltx_font_italic">support set</span>, <math id="S3.p3.m6" class="ltx_Math" alttext="S_{M+1}\in\mathcal{Z}^{k}" display="inline"><mrow><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒵</mi><mi>k</mi></msup></mrow></math>, from <math id="S3.p3.m7" class="ltx_Math" alttext="P_{M+1}" display="inline"><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>.
</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">Consider a symmetric loss function <math id="S3.p4.m1" class="ltx_Math" alttext="\ell(a,b)=\psi(\rho(a,b))" display="inline"><mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> for non-decreasing <math id="S3.p4.m2" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math> and arbitrary metric <math id="S3.p4.m3" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math>. We seek to estimate the output of <math id="S3.p4.m4" class="ltx_Math" alttext="\theta:\mathcal{P}\rightarrow\Omega" display="inline"><mrow><mi>θ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo stretchy="false">→</mo><mi mathvariant="normal">Ω</mi></mrow></mrow></math>, a functional that maps distributions to a metric space <math id="S3.p4.m5" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Ω</mi></math>. For example, <math id="S3.p4.m6" class="ltx_Math" alttext="\theta(P)" display="inline"><mrow><mi>θ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></math> may describe the coefficient vector of a high-dimensional hyperplane when <math id="S3.p4.m7" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> is a space of linear models, and <math id="S3.p4.m8" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> may be the Euclidean distance.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The <em class="ltx_emph ltx_font_italic">i.i.d </em>minimax risk</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Before studying the meta-learning setting, we first begin with a definition of the <em class="ltx_emph ltx_font_italic">i.i.d </em>minimax risk that measures the worst-case error of the best possible estimator,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1" class="ltx_Math" alttext="R^{*}=\inf_{\hat{\theta}}\sup_{P\in\mathcal{P}}\mathbb{E}_{S\sim P^{k}}\left[%
\ell(\hat{\theta}(S),\theta_{P})\right]." display="block"><mrow><mrow><msup><mi>R</mi><mo>*</mo></msup><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.0835em">inf</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover></munder><mrow><munder><mo lspace="0.0835em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mi>P</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></munder><mrow><msub><mi>𝔼</mi><mrow><mi>S</mi><mo>∼</mo><msup><mi>P</mi><mi>k</mi></msup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><mi>P</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For notational convenience, we denote the output of <math id="S3.SS0.SSS0.Px1.p1.m1" class="ltx_Math" alttext="\theta(P)" display="inline"><mrow><mi>θ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></math> by <math id="S3.SS0.SSS0.Px1.p1.m2" class="ltx_Math" alttext="\theta_{P}" display="inline"><msub><mi>θ</mi><mi>P</mi></msub></math>. The estimator for <math id="S3.SS0.SSS0.Px1.p1.m3" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math> is denoted, <math id="S3.SS0.SSS0.Px1.p1.m4" class="ltx_Math" alttext="\hat{\theta}:\mathcal{Z}^{k}\rightarrow\Omega" display="inline"><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒵</mi><mi>k</mi></msup><mo stretchy="false">→</mo><mi mathvariant="normal">Ω</mi></mrow></mrow></math>, and maps <math id="S3.SS0.SSS0.Px1.p1.m5" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> samples from <math id="S3.SS0.SSS0.Px1.p1.m6" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> to an estimate of <math id="S3.SS0.SSS0.Px1.p1.m7" class="ltx_Math" alttext="\theta_{P}" display="inline"><msub><mi>θ</mi><mi>P</mi></msub></math>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Novel-task minimax risk</h5>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">In the novel-task setting, we wish to estimate <math id="S3.SS0.SSS0.Px2.p1.m1" class="ltx_Math" alttext="\theta_{P_{M+1}}" display="inline"><msub><mi>θ</mi><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></msub></math>, the parameters of the novel task distribution <math id="S3.SS0.SSS0.Px2.p1.m2" class="ltx_Math" alttext="P_{M+1}" display="inline"><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>.
We consider two-stage estimators for <math id="S3.SS0.SSS0.Px2.p1.m3" class="ltx_Math" alttext="\theta_{P_{M+1}}" display="inline"><msub><mi>θ</mi><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></msub></math>. In the first stage, the meta-learner uses a learning algorithm <math id="S3.SS0.SSS0.Px2.p1.m4" class="ltx_Math" alttext="f:S_{1:M}\mapsto\bm{\hat{\theta}}_{S_{1:M}}" display="inline"><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">↦</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></msub></mrow></mrow></math>, that maps the meta-training set to an estimation algorithm, <math id="S3.SS0.SSS0.Px2.p1.m5" class="ltx_Math" alttext="\bm{\hat{\theta}}_{S_{1:M}}:~{}\mathcal{Z}^{k}\rightarrow~{}\Omega" display="inline"><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></msub><mo lspace="0.278em" rspace="0.608em">:</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒵</mi><mi>k</mi></msup><mo rspace="0.608em" stretchy="false">→</mo><mi mathvariant="normal">Ω</mi></mrow></mrow></math>. In the second stage, the learner computes <math id="S3.SS0.SSS0.Px2.p1.m6" class="ltx_Math" alttext="\bm{\hat{\theta}}_{S_{1:M}}(S_{M+1})" display="inline"><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math>, the estimate of <math id="S3.SS0.SSS0.Px2.p1.m7" class="ltx_Math" alttext="\theta_{P_{M+1}}" display="inline"><msub><mi>θ</mi><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></msub></math>.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">The novel-task minimax risk is given by,</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1" class="ltx_Math" alttext="R^{*}_{\mathcal{P}}(\beta)=\inf_{f}\sup_{P_{1},\ldots,P_{M+1}\in\mathcal{Q}^{%
\beta}_{\mathcal{P}}}\mathbb{E}_{\begin{subarray}{c}S_{1:M}\sim P_{1:M}^{n}\\
S_{M+1}\sim P_{M+1}^{k}\end{subarray}}\left[\ell\left((f(S_{1:M}))(S_{M+1}),%
\theta_{P_{M+1}}\right)\right]," display="block"><mrow><mrow><mrow><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.0835em">inf</mo><mi>f</mi></munder><mrow><munder><mo lspace="0.0835em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>∈</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒬</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>β</mi></msubsup></mrow></munder><mrow><msub><mi>𝔼</mi><mtable rowspacing="0pt"><mtr><mtd><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo>∼</mo><msubsup><mi>P</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow><mi>n</mi></msubsup></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><msubsup><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow></mtd></mtr></mtable></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.SS0.SSS0.Px2.p2.m1" class="ltx_Math" alttext="\mathcal{Q}^{\beta}_{\mathcal{P}}=\{(P_{1},\ldots,P_{M+1})\in\mathcal{P}:D_{%
\mathrm{KL}}\left(P_{M+1}\|P_{i}\right)\leq\beta,\textrm{ for }i=1,\ldots,M\}" display="inline"><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒬</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>β</mi></msubsup><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>≤</mo><mi>β</mi></mrow><mo>,</mo><mrow><mrow><mtext> for </mtext><mo>⁢</mo><mi>i</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>M</mi></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></math>. This ensures a degree of relatedness between the novel and meta-training tasks.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">The estimator for <math id="S3.SS0.SSS0.Px2.p3.m1" class="ltx_Math" alttext="\theta_{M+1}" display="inline"><msub><mi>θ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math> now depends additionally on the <math id="S3.SS0.SSS0.Px2.p3.m2" class="ltx_Math" alttext="Mn" display="inline"><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow></math> samples in <math id="S3.SS0.SSS0.Px2.p3.m3" class="ltx_Math" alttext="S_{1:M}" display="inline"><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math>, where only <math id="S3.SS0.SSS0.Px2.p3.m4" class="ltx_Math" alttext="k\ll~{}Mn" display="inline"><mrow><mi>k</mi><mo rspace="0.608em">≪</mo><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow></mrow></math> samples from <math id="S3.SS0.SSS0.Px2.p3.m5" class="ltx_Math" alttext="P_{M+1}" display="inline"><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math> are available to the learner. Thus, <math id="S3.SS0.SSS0.Px2.p3.m6" class="ltx_Math" alttext="R^{*}_{\mathcal{P}}" display="inline"><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup></math> addresses the domain shift expected at test-time in the meta-learning setting and allows the learner to use data from multiple tasks. The goal of <math id="S3.SS0.SSS0.Px2.p3.m7" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> is to learn an inductive bias from <math id="S3.SS0.SSS0.Px2.p3.m8" class="ltx_Math" alttext="S_{1:M}" display="inline"><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math> such that a good estimate is possible with only <math id="S3.SS0.SSS0.Px2.p3.m9" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> data points from <math id="S3.SS0.SSS0.Px2.p3.m10" class="ltx_Math" alttext="P_{M+1}" display="inline"><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>. In this setting, <math id="S3.SS0.SSS0.Px2.p3.m11" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is equivalent to the number of shots in few-shot learning.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">An example with meta-linear regression</h5>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">We present here a short summary based on meta linear regression, which we will analyze
in more detail in Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="830" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold">Meta-learning 1D-regression:</span> The parameters of a 1D regression model are fitted
from a small support set. The training distributions (<math id="S3.F1.m3" class="ltx_Math" alttext="P_{1},P_{2},P_{3}" display="inline"><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><msub><mi>P</mi><mn>2</mn></msub><mo>,</mo><msub><mi>P</mi><mn>3</mn></msub></mrow></math>) give a useful inductive bias
for fitting <math id="S3.F1.m4" class="ltx_Math" alttext="P_{4}" display="inline"><msub><mi>P</mi><mn>4</mn></msub></math> using only 5 points. The MLE solution on the novel task for those 5 points is also
displayed.</figcaption>
</figure>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p">In Figure <a href="#S3.F1" title="Figure 1 ‣ An example with meta-linear regression ‣ 3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show observed data samples from a family of polynomial
regression models. Our aim is to output an algorithm which recovers the parameters of a new polynomial function from limited observations–we choose a MAP estimator which is described fully in
Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In the bottom right, we are given only 5 data points from a
novel task distribution and estimate the parameters of the model with both the MLE and MAP
estimators — the MLE overfits the support set while the MAP estimator is close to the true function.</p>
</div>
<div id="S3.SS0.SSS0.Px3.p3" class="ltx_para">
<p class="ltx_p">In terms of the terminology used above, the set,</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1" class="ltx_Math" alttext="\mathcal{P}=\{p_{\bm{\theta}}(y)=\mathcal{N}(\mathbf{x}^{\top}\bm{\theta},%
\sigma^{2}):\bm{\theta}\in\mathbb{R}^{d},\mathbf{x}=[1,x,\ldots,x^{d-1}]\}," display="block"><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><msub><mi>p</mi><mi>𝜽</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐱</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>𝜽</mi></mrow><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>𝜽</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo>,</mo><mrow><mi>𝐱</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mi>x</mi><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">is the space of polynomial regression models, parameterized by <math id="S3.SS0.SSS0.Px3.p3.m1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math>. For this problem, we take <math id="S3.SS0.SSS0.Px3.p3.m2" class="ltx_Math" alttext="\ell(\hat{\theta},\theta)=\lVert\hat{\theta}-\theta\rVert_{2}^{2}" display="inline"><mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><msubsup><mrow><mo fence="true" lspace="0.1389em" rspace="0em">∥</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>−</mo><mi>θ</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math>. In Figure <a href="#S3.F1" title="Figure 1 ‣ An example with meta-linear regression ‣ 3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, tasks are generated with <math id="S3.SS0.SSS0.Px3.p3.m3" class="ltx_Math" alttext="p(\theta)=\mathcal{N}(\tau,\sigma^{2}_{\theta})" display="inline"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>τ</mi><mo>,</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, for unknown, sparse, <math id="S3.SS0.SSS0.Px3.p3.m4" class="ltx_Math" alttext="\tau\in\mathbb{R}^{d}" display="inline"><mrow><mi>τ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></math>. Thus, each model is a polynomial function with few large coefficients. The algorithm <math id="S3.SS0.SSS0.Px3.p3.m5" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>, first takes samples from <math id="S3.SS0.SSS0.Px3.p3.m6" class="ltx_Math" alttext="P_{1},P_{2},P_{3}" display="inline"><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><msub><mi>P</mi><mn>2</mn></msub><mo>,</mo><msub><mi>P</mi><mn>3</mn></msub></mrow></math> and computes an estimate, <math id="S3.SS0.SSS0.Px3.p3.m7" class="ltx_Math" alttext="\hat{\tau}" display="inline"><mover accent="true"><mi>τ</mi><mo>^</mo></mover></math>. This estimate of <math id="S3.SS0.SSS0.Px3.p3.m8" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> is then used to compute <math id="S3.SS0.SSS0.Px3.p3.m9" class="ltx_Math" alttext="\bm{\hat{\theta}}(S_{M+1};\hat{\tau})=\operatorname*{\textrm{argmax}}_{\bm{%
\theta}_{4}}p(\bm{\theta}_{4}|\hat{\tau},S_{M+1})" display="inline"><mrow><mrow><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mover accent="true"><mi>τ</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mtext>argmax</mtext><msub><mi>𝜽</mi><mn>4</mn></msub></msub><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝜽</mi><mn>4</mn></msub><mo fence="false">|</mo><mrow><mover accent="true"><mi>τ</mi><mo>^</mo></mover><mo>,</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>. Note that this approach is able to learn the correct inductive bias from the data, without requiring a carefully designed regularizer. The lower bounds we derive in Section 4 can be applied to problems of this general type, and the upper and lower bounds in Section 5 apply specifically to meta-learning linear regression.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Information theoretic lower bounds on novel task generalization</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we first present our most general result: Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Using this, we derive Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that gives a lower bound in terms of the sample size in the training and novel tasks. Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> recovers a well-known <em class="ltx_emph ltx_font_italic">i.i.d </em>lower bound (Theorem <a href="#Thmtheorem2" title="Theorem 2 (IID minimax lower-bound). ‣ 4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) when <math id="S4.p1.m1" class="ltx_Math" alttext="Mn=0" display="inline"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mo>=</mo><mn>0</mn></mrow></math>, and, importantly, highlights that the novel task data is significantly more valuable than the training task data. Additionally, we provide a specialized bound that applies when the environment is partially observed — proving that in this setting training task data is insufficient to drive the minimax risk to zero.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">In Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we assume that <math id="S4.p2.m1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> contains <math id="S4.p2.m2" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> distinct <math id="S4.p2.m3" class="ltx_Math" alttext="2\delta" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></math>-separated distributions but only <math id="S4.p2.m4" class="ltx_Math" alttext="M+1\leq J" display="inline"><mrow><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>≤</mo><mi>J</mi></mrow></math> tasks are visible to the learner. Intuitively, the error rate lower-bound shrinks as the amount of information shared between the training tasks and the novel task grows.
All proofs are given in Appendix <a href="#A2.SS1" title="B.1 IID Lower Bound ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. Recall <math id="S4.p2.m5" class="ltx_Math" alttext="\ell(a,b)=\psi(\rho(a,b))" display="inline"><mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> for non-decreasing <math id="S4.p2.m6" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math> and arbitrary metric <math id="S4.p2.m7" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math>.</p>
</div>
<div id="Thmtheorem1" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Minimax novel task risk lower bound)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math id="Thmtheorem1.p1.m1" class="ltx_Math" alttext="\mathcal{J}\subset\mathcal{P}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo mathvariant="normal">⊂</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math> contain <math id="Thmtheorem1.p1.m2" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> distinct distributions such that <math id="Thmtheorem1.p1.m3" class="ltx_Math" alttext="\rho(\theta_{P},\theta_{P^{\prime}})~{}\geq~{}2\delta" display="inline"><mrow><mrow><mi>ρ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mi>θ</mi><mi>P</mi></msub><mo mathvariant="normal">,</mo><msub><mi>θ</mi><msup><mi>P</mi><mo mathvariant="normal">′</mo></msup></msub><mo mathvariant="normal" rspace="0.330em" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal" rspace="0.608em">≥</mo><mrow><mn mathvariant="normal">2</mn><mo mathvariant="italic">⁢</mo><mi>δ</mi></mrow></mrow></math> and <math id="Thmtheorem1.p1.m4" class="ltx_Math" alttext="D_{\mathrm{KL}}\left(P\|P^{\prime}\right)\leq\beta" display="inline"><mrow><mrow><msub><mi>D</mi><mi mathvariant="normal">KL</mi></msub><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal">(</mo><mrow><mi>P</mi><mo mathvariant="normal">∥</mo><msup><mi>P</mi><mo mathvariant="normal">′</mo></msup></mrow><mo mathvariant="normal">)</mo></mrow></mrow><mo mathvariant="normal">≤</mo><mi>β</mi></mrow></math> for all <math id="Thmtheorem1.p1.m5" class="ltx_Math" alttext="P,P^{\prime}\in\mathcal{J}" display="inline"><mrow><mrow><mi>P</mi><mo mathvariant="normal">,</mo><msup><mi>P</mi><mo mathvariant="normal">′</mo></msup></mrow><mo mathvariant="normal">∈</mo><mi class="ltx_font_mathcaligraphic">𝒥</mi></mrow></math>.
Let <math id="Thmtheorem1.p1.m6" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math> be a random ordering of the <math id="Thmtheorem1.p1.m7" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> elements, and <math id="Thmtheorem1.p1.m8" class="ltx_Math" alttext="Z|\pi" display="inline"><mrow><mi>Z</mi><mo fence="false" mathvariant="normal">|</mo><mi>π</mi></mrow></math> be a vector of <math id="Thmtheorem1.p1.m9" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> <em class="ltx_emph ltx_font_upright">i.i.d </em>samples from <math id="Thmtheorem1.p1.m10" class="ltx_Math" alttext="P_{\pi_{M+1}}" display="inline"><msub><mi>P</mi><msub><mi>π</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub></msub></math>.
Further, define <math id="Thmtheorem1.p1.m11" class="ltx_Math" alttext="W|\pi" display="inline"><mrow><mi>W</mi><mo fence="false" mathvariant="normal">|</mo><mi>π</mi></mrow></math> to be an <math id="Thmtheorem1.p1.m12" class="ltx_Math" alttext="n\times M" display="inline"><mrow><mi>n</mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em">×</mo><mi>M</mi></mrow></math> matrix whose <math id="Thmtheorem1.p1.m13" class="ltx_Math" alttext="j^{th}" display="inline"><msup><mi>j</mi><mrow><mi>t</mi><mo mathvariant="italic">⁢</mo><mi>h</mi></mrow></msup></math> column consist of <math id="Thmtheorem1.p1.m14" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> <em class="ltx_emph ltx_font_upright">i.i.d </em>samples from <math id="Thmtheorem1.p1.m15" class="ltx_Math" alttext="P_{\pi_{j}}" display="inline"><msub><mi>P</mi><msub><mi>π</mi><mi>j</mi></msub></msub></math>. Then,</span></p>
<table id="A4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex2.m1" class="ltx_Math" alttext="\displaystyle R^{*}_{\mathcal{P}}(\beta)\geq\psi(\delta)\left(1-\dfrac{I(\pi_{%
M+1};W)+I(\pi_{M+1};Z)+1}{\log_{2}J}\right)." display="inline"><mrow><mrow><mrow><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">Note that <math id="S4.p3.m1" class="ltx_Math" alttext="\delta" display="inline"><mi>δ</mi></math> is a property of the so-called packing set, <math id="S4.p3.m2" class="ltx_Math" alttext="\mathcal{J}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒥</mi></math>, and may depend on the sample size, <math id="S4.p3.m3" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>, and other properties of <math id="S4.p3.m4" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>. For example, practical instances of this bound typically require <math id="S4.p3.m5" class="ltx_Math" alttext="\psi(\delta)=O(1/k)" display="inline"><mrow><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> or similar, as in Theorem <a href="#Thmtheorem3" title="Theorem 3 (Meta linear regression lower bound). ‣ 5.1 Minimax lower bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> below.
To derive this result, we bound the statistical estimation error by the error on a corresponding decoding problem where we must predict the novel task index, given the meta-training set <math id="S4.p3.m6" class="ltx_Math" alttext="S_{1:M}" display="inline"><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math> and <math id="S4.p3.m7" class="ltx_Math" alttext="S_{M+1}" display="inline"><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>. Fano’s inequality provides best-case error rates for this problem.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">Using Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we derive our first bound on the novel-task minimax risk that depends on the number of meta-training tasks (<math id="S4.p4.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>) and datapoints per training task (<math id="S4.p4.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>, <math id="S4.p4.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>), via a local-packing argument. The following corollary implies that if we have <math id="S4.p4.m4" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> meta-training tasks in our <math id="S4.p4.m5" class="ltx_Math" alttext="2\delta" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></math>-packing that are close
(in terms of their pairwise KL distance), then learning a novel task from training samples drawn
from the meta-training tasks requires significantly more examples; in particular, learning the novel task from samples drawn from the
meta training set requires <math id="S4.p4.m6" class="ltx_Math" alttext="\Omega(J)" display="inline"><mrow><mi mathvariant="normal">Ω</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>J</mi><mo stretchy="false">)</mo></mrow></mrow></math> times the sample complexity of the novel task. This matches our intuition that learning the novel task implies the ability to distinguish it from all <math id="S4.p4.m7" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> well-separated meta-training tasks.</p>
</div>
<div id="Thmcorollary1" class="ltx_theorem ltx_theorem_corollary">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmcorollary1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Assume the same setting as in Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Then,</span></p>
<table id="A4.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex3.m1" class="ltx_Math" alttext="\displaystyle R^{*}_{\mathcal{P}}(\beta)\geq\psi(\delta)\left(1-\dfrac{1+\left%
(\frac{Mn}{(J-1)}+k\right)\frac{1}{J^{2}}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}%
\left(P_{i}\|P_{j}\right)}{\log_{2}J}\right)." display="inline"><mrow><mrow><mrow><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>+</mo><mi>k</mi></mrow><mo>)</mo></mrow><mo>⁢</mo><mfrac><mn>1</mn><msup><mi>J</mi><mn>2</mn></msup></mfrac><mo>⁢</mo><mrow><msub><mo>∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></msub><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">A tighter bound on partially observed environments</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We now consider the special case of Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> when <math id="S4.SS0.SSS0.Px1.p1.m1" class="ltx_Math" alttext="M&lt;J-1" display="inline"><mrow><mi>M</mi><mo>&lt;</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mrow></math>, meaning that the meta-training tasks cannot cover the full packing set. In this setting, we prove that no algorithm can generalize perfectly to tasks in unseen regions of the space with small <math id="S4.SS0.SSS0.Px1.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>, regardless of the number of data points <math id="S4.SS0.SSS0.Px1.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> observed in each meta-training task.</p>
</div>
<div id="Thmcorollary2" class="ltx_theorem ltx_theorem_corollary">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmcorollary2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Assume the same setting as in Theorem 1, with <math id="Thmcorollary2.p1.m1" class="ltx_Math" alttext="M+1&lt;J" display="inline"><mrow><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow><mo mathvariant="normal">&lt;</mo><mi>J</mi></mrow></math>. Then,</span></p>
<table id="A4.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex4.m1" class="ltx_Math" alttext="\displaystyle R^{*}_{\mathcal{P}}\geq\psi(\delta)\left(\dfrac{\log_{2}(J-M)-%
\frac{k}{J^{2}}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}\left(P_{i}\|P_{j}\right)-%
1}{\log_{2}J}\right)." display="inline"><mrow><mrow><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mfrac><mi>k</mi><msup><mi>J</mi><mn>2</mn></msup></mfrac><mo>⁢</mo><mrow><msub><mo>∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></msub><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">In this work, we have focused on the setting where <math id="S4.SS0.SSS0.Px1.p2.m1" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> contains an equal number of samples from each of the meta-training tasks — this is the sampling scheme shown in Figure 2. However, it is possible to extend these results to different sampling schemes for <math id="S4.SS0.SSS0.Px1.p2.m2" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>. For example, in the appendix we derive bounds with <math id="S4.SS0.SSS0.Px1.p2.m3" class="ltx_Math" alttext="W|\pi" display="inline"><mrow><mi>W</mi><mo fence="false">|</mo><mi>π</mi></mrow></math> as a mixture distribution. Surprisingly, despite task identity being hidden from the learner, the asymptotic rate for these two sampling schemes match.</p>
</div>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Measuring Task-Relatedness</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">The use of local packing requires the design of an appropriate set of distributions whose corresponding parameters are <math id="S4.SS1.p1.m1" class="ltx_Math" alttext="2\delta" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></math>-separated but maintain small KL divergences. In the multi-task setting such an assumption is intuitively reasonable: challenging tasks should require separated parameters for ideal explanations (<math id="S4.SS1.p1.m2" class="ltx_Math" alttext="2\delta" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></math>-separated) but should satisfy some relatedness measure (small KL). Importantly, these parameters can depend on sample size and other problem-specific variables. As we will see shortly, lower bounds on minimax risk in the <em class="ltx_emph ltx_font_italic">i.i.d </em>setting may also assume the same notion of relatedness for the local-packing in <math id="S4.SS1.p1.m3" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">Task relatedness is a necessary feature for upper-bounds on novel task generalization, but are typically difficult to define (see e.g. <cite class="ltx_cite ltx_citemacro_citet">Ben-David and Borbely (<a href="#bib.bib28" title="A notion of task relatedness yielding provable multiple-task learning guarantees" class="ltx_ref">2008</a>)</cite>). Our lower bounds utilize a relatively weak notion of task-relatedness, and thus may be overly pessimistic compared to the upper bounds computed in existing work. However, task relatedness of this form can be formulated in a representation space shared across tasks and thus can be applied in settings like those explored by e.g. <cite class="ltx_cite ltx_citemacro_citet">Du<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib47" title="Few-shot learning via learning the representation, provably" class="ltx_ref">2020</a>)</cite>. Deriving lower bounds under the different task relatedness assumptions present in the literature would make for exciting future work.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison to risk of <em class="ltx_emph ltx_font_italic">i.i.d </em>learners</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">From the statement of Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> it is not clear how this lower-bound compares to that of the <em class="ltx_emph ltx_font_italic">i.i.d </em>learner which has access only to the <math id="S4.SS2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> samples from <math id="S4.SS2.p1.m2" class="ltx_Math" alttext="S_{M+1}" display="inline"><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>.
To investigate the benefit of additional meta-training tasks, we compare our derived minimax risk lower bounds to those achieved by <em class="ltx_emph ltx_font_italic">i.i.d </em>learners. To do so, we revisit standard minimax lower bounds that can be found in e.g. <cite class="ltx_cite ltx_citemacro_citet">Loh (<a href="#bib.bib13" title="On lower bounds for statistical learning theory" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="Thmtheorem2" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2</span></span><span class="ltx_text ltx_font_bold"> </span>(IID minimax lower-bound)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose <math id="Thmtheorem2.p1.m1" class="ltx_Math" alttext="\{P_{1},\dots,P_{J}\}\subseteq\mathcal{P}" display="inline"><mrow><mrow><mo mathvariant="normal" stretchy="false">{</mo><msub><mi>P</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">,</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">,</mo><msub><mi>P</mi><mi>J</mi></msub><mo mathvariant="normal" stretchy="false">}</mo></mrow><mo mathvariant="normal">⊆</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math> satisfy <math id="Thmtheorem2.p1.m2" class="ltx_Math" alttext="\rho(\theta_{P_{i}},\theta_{P_{j}})\geq 2\delta" display="inline"><mrow><mrow><mi>ρ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo mathvariant="normal">,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>j</mi></msub></msub><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">≥</mo><mrow><mn mathvariant="normal">2</mn><mo mathvariant="italic">⁢</mo><mi>δ</mi></mrow></mrow></math> for all <math id="Thmtheorem2.p1.m3" class="ltx_Math" alttext="i\neq j" display="inline"><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow></math>. Then,</span></p>
<table id="A4.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex5.m1" class="ltx_Math" alttext="\displaystyle R^{*}\geq\psi(\delta)\left(1-\dfrac{\frac{k}{J^{2}}\sum_{1\leq i%
,j\leq J}D_{\mathrm{KL}}\left(P_{i}\|P_{j}\right)+1}{\log_{2}J}\right)." display="inline"><mrow><mrow><msup><mi>R</mi><mo>*</mo></msup><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mfrac><mi>k</mi><msup><mi>J</mi><mn>2</mn></msup></mfrac><mo>⁢</mo><mrow><msub><mo>∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></msub><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">We include a proof of this result in Appendix <a href="#A2.SS1" title="B.1 IID Lower Bound ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>, using local-packing as in our meta-learning bounds. As hoped, Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> recovers Theorem <a href="#Thmtheorem2" title="Theorem 2 (IID minimax lower-bound). ‣ 4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> when there are no training tasks available. Moreover, this <em class="ltx_emph ltx_font_italic">i.i.d </em>bound is strictly larger than the one computed in Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in general. Note that while this <em class="ltx_emph ltx_font_italic">i.i.d </em>minimax risk is asymptotically tight for several learning problems <cite class="ltx_cite ltx_citemacro_citep">(Loh, <a href="#bib.bib13" title="On lower bounds for statistical learning theory" class="ltx_ref">2017</a>; Raskutti<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib31" title="Minimax rates of estimation for high-dimensional linear regression over ⁢ℓ_q-balls" class="ltx_ref">2011</a>)</cite>, there is no immediate guarantee that the same is true for our meta-learning minimax bounds. We investigate the quality of these bounds by providing comparable upper bounds in the next section.

</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis of a hierarchical Bayesian model of meta-learning</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">Our goal is to analyze the sample complexity of meta-learning for linear regression, where samples are drawn from multiple meta-training tasks and we want to generalize to a new task with only a few data points. After introducing the setting, we will compute lower-bounds on the minimax risk using our results from Section <a href="#S4" title="4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, revealing a <math id="S5.p1.m1" class="ltx_Math" alttext="2^{d}" display="inline"><msup><mn>2</mn><mi>d</mi></msup></math> scaling on the meta-training sample complexity. Following the lower bound, we derive an accompanying upper-bound on the risk of a MAP estimator, derived from an empirical Bayes estimate over a hierarchical Bayesian model. Asymptotic analysis of this bound reveals that if the observed samples from the novel task vary considerably more than the task parameters, then observing more meta-training samples may significantly improve convergence in the small <math id="S5.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> regime. This is validated empirically in Section <a href="#S6" title="6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">For <math id="S5.p2.m1" class="ltx_Math" alttext="i=1...M+1" display="inline"><mrow><mi>i</mi><mo>=</mo><mrow><mrow><mn>1</mn><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><mi>M</mi></mrow><mo>+</mo><mn>1</mn></mrow></mrow></math>, where <math id="S5.p2.m2" class="ltx_Math" alttext="M+1" display="inline"><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></math> is the total number of tasks, we define,</p>
<table id="A4.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex6.m1" class="ltx_Math" alttext="\displaystyle\mathbf{y}_{i}" display="inline"><msub><mi>𝐲</mi><mi>i</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex6.m2" class="ltx_Math" alttext="\displaystyle=X_{i}\bm{\theta}_{i}+\bm{\epsilon}_{i}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex6.m3" class="ltx_Math" alttext="\displaystyle X_{i}\in\mathbb{R}^{n_{i}\times d},\mathbf{y}_{i}\in\mathbb{R}^{%
n_{i}},\bm{\epsilon}_{i}\in\mathbb{R}^{n_{i}}" display="inline"><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mi>i</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mrow><msub><mi>𝐲</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>i</mi></msub></msup></mrow><mo>,</mo><mrow><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>i</mi></msub></msup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex7.m1" class="ltx_Math" alttext="\displaystyle\bm{\epsilon}_{i}" display="inline"><msub><mi mathvariant="bold-italic">ϵ</mi><mi>i</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex7.m2" class="ltx_Math" alttext="\displaystyle\sim\mathcal{N}(0,\sigma^{2}_{i}I)," display="inline"><mrow><mrow><mi></mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex7.m3" class="ltx_Math" alttext="\displaystyle\sigma^{2}_{i}\in\mathbb{R}^{+}" display="inline"><mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Each task has some design matrix <math id="S5.p2.m3" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> and unknown parameters <math id="S5.p2.m4" class="ltx_Math" alttext="\bm{\theta}_{i}" display="inline"><msub><mi>𝜽</mi><mi>i</mi></msub></math>. For simplicity, we assume known isotropic noise models and that <math id="S5.p2.m5" class="ltx_Math" alttext="n_{i}={n}" display="inline"><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>=</mo><mi>n</mi></mrow></math> for all <math id="S5.p2.m6" class="ltx_Math" alttext="i\leq M" display="inline"><mrow><mi>i</mi><mo>≤</mo><mi>M</mi></mrow></math>, with <math id="S5.p2.m7" class="ltx_Math" alttext="n_{M+1}={k}" display="inline"><mrow><msub><mi>n</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>k</mi></mrow></math>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">Our meta learner will fit the data using an empirical Bayes estimate in a hierarchical Bayesian model:</p>
<table id="A4.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex8.m1" class="ltx_Math" alttext="\displaystyle\bm{\theta}_{i}=\bm{\tau}+\bm{\xi},\&gt;\&gt;\&gt;\&gt;\bm{\tau}\in\mathbb{R}%
^{d},\&gt;\&gt;\&gt;\&gt;\bm{\xi}\in\mathbb{R}^{d},\&gt;\&gt;\&gt;\&gt;\bm{\xi}\sim\mathcal{N}(0,%
\sigma^{2}_{\theta}I),\&gt;\&gt;\&gt;\&gt;\sigma^{2}_{\theta}\in\mathbb{R}^{+}" display="inline"><mrow><mrow><msub><mi>𝜽</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>𝝉</mi><mo>+</mo><mi>𝝃</mi></mrow></mrow><mo rspace="1.057em">,</mo><mrow><mrow><mi>𝝉</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="1.057em">,</mo><mrow><mrow><mi>𝝃</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="1.057em">,</mo><mrow><mrow><mi>𝝃</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.057em">,</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mo>+</mo></msup></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">We will consider the Maximum a Posterior estimator,</p>
<table id="S5.Ex9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex9.m1" class="ltx_Math" alttext="\bm{\hat{\theta}}_{M+1}=\operatorname*{\textrm{argmax}}_{\bm{\theta}_{M+1}}p(%
\bm{\theta}_{M+1}|\mathbf{y}_{1},\ldots,\mathbf{y}_{M+1})," display="block"><mrow><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><munder><mtext>argmax</mtext><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></munder><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>𝐲</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and will characterize its risk, <math id="S5.p4.m1" class="ltx_Math" alttext="\mathbb{E}[\lVert\bm{\hat{\theta}}_{M+1}~{}-~{}\bm{\theta}_{M+1}\rVert_{2}^{2}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo rspace="0.552em">−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow></math>, where the expectation is with respect to sampled data only. The posterior distribution under the Empirical Bayes estimate for <math id="S5.p4.m2" class="ltx_Math" alttext="\bm{\tau}" display="inline"><mi>𝝉</mi></math> is given in Appendix <a href="#A3.SS2" title="C.2 Posterior estimate ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.2</span></a>. The derivation is standard but dense and we recommend dedicated readers to consult <cite class="ltx_cite ltx_citemacro_citet">Gelman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib33" title="Bayesian data analysis" class="ltx_ref">2013</a>)</cite>, or an equivalent text, for more details.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Minimax lower bounds</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">We now compute lower bounds for parameter estimation with meta-learning over multiple linear regression tasks.
Beginning with a definition of the space of data generating distributions,</p>
<table id="S5.Ex10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex10.m1" class="ltx_math_unparsed" alttext="\mathcal{P}_{LR}=\{p_{\bm{\theta}}(\mathbf{y})=\mathcal{N}(X\bm{\theta},\sigma%
^{2}I):\bm{\theta}\in\mathbb{B}_{2}(1),X\in\mathbb{R}^{n\times d}.\}" display="block"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>p</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mrow><mo stretchy="false">(</mo><mi>X</mi><mi>𝜽</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow><mo rspace="0.278em">:</mo><mi>𝜽</mi><mo>∈</mo><msub><mi>𝔹</mi><mn>2</mn></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>X</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">where <math id="S5.SS1.p2.m1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math> are the parameters to be learned, and <math id="S5.SS1.p2.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> is the design matrix of each linear regression task in the environment.
We write <math id="S5.SS1.p2.m3" class="ltx_Math" alttext="\gamma=\max_{i}\sigma_{\max}(X_{i}/\sqrt{n})" display="inline"><mrow><mi>γ</mi><mo>=</mo><mrow><mrow><msub><mi>max</mi><mi>i</mi></msub><mo lspace="0.167em">⁡</mo><msub><mi>σ</mi><mi>max</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>/</mo><msqrt><mi>n</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, which we assume is bounded for all <math id="S5.SS1.p2.m4" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and <math id="S5.SS1.p2.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> (an assumption that is validated for random Gaussian matrices by <cite class="ltx_cite ltx_citemacro_citet">Raskutti<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib31" title="Minimax rates of estimation for high-dimensional linear regression over ⁢ℓ_q-balls" class="ltx_ref">2011</a>)</cite>).</p>
</div>
<div id="Thmtheorem3" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3</span></span><span class="ltx_text ltx_font_bold"> </span>(Meta linear regression lower bound)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Consider <math id="Thmtheorem3.p1.m1" class="ltx_Math" alttext="\mathcal{P}_{LR}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo mathvariant="italic">⁢</mo><mi>R</mi></mrow></msub></math> defined as above and let <math id="Thmtheorem3.p1.m2" class="ltx_Math" alttext="\ell(a,b)=(\lVert a-b\rVert_{2})^{2}" display="inline"><mrow><mrow><mi mathvariant="normal">ℓ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi>a</mi><mo mathvariant="normal">,</mo><mi>b</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">=</mo><msup><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mrow><mo fence="true" lspace="0em" mathvariant="normal" rspace="0em">∥</mo><mrow><mi>a</mi><mo mathvariant="normal">−</mo><mi>b</mi></mrow><mo fence="true" lspace="0em" mathvariant="normal" rspace="0em">∥</mo></mrow><mn mathvariant="normal">2</mn></msub><mo mathvariant="normal" stretchy="false">)</mo></mrow><mn mathvariant="normal">2</mn></msup></mrow></math>. If <math id="Thmtheorem3.p1.m3" class="ltx_Math" alttext="d\geq 2" display="inline"><mrow><mi>d</mi><mo mathvariant="normal">≥</mo><mn mathvariant="normal">2</mn></mrow></math> and <math id="Thmtheorem3.p1.m4" class="ltx_math_unparsed" alttext="2^{-d}M+kn^{-1}\geq\max\{\frac{d}{4\beta},d\sigma^{2}/(256\gamma^{2}n)" display="inline"><mrow><msup><mn mathvariant="normal">2</mn><mrow><mo mathvariant="normal">−</mo><mi>d</mi></mrow></msup><mi>M</mi><mo mathvariant="normal">+</mo><mi>k</mi><msup><mi>n</mi><mrow><mo mathvariant="normal">−</mo><mn mathvariant="normal">1</mn></mrow></msup><mo mathvariant="normal">≥</mo><mi mathvariant="normal">max</mi><mrow><mo mathvariant="normal" stretchy="false">{</mo><mfrac><mi>d</mi><mrow><mn mathvariant="normal">4</mn><mo mathvariant="italic">⁢</mo><mi>β</mi></mrow></mfrac><mo mathvariant="normal">,</mo><mi>d</mi><msup><mi>σ</mi><mn mathvariant="normal">2</mn></msup><mo mathvariant="normal">/</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mn mathvariant="normal">256</mn><msup><mi>γ</mi><mn mathvariant="normal">2</mn></msup><mi>n</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>, then,</span></p>
<table id="S5.Ex11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex11.m1" class="ltx_Math" alttext="R^{*}_{\mathcal{P}_{LR}}(\beta)\geq O\left(\frac{d\sigma^{2}}{\gamma^{2}(2^{-d%
}{n}M+{k})}\right)" display="block"><mrow><mrow><msubsup><mi>R</mi><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub><mo>*</mo></msubsup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mfrac><mrow><mi>d</mi><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><msup><mi>γ</mi><mn>2</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>M</mi></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">The proof is given in Appendix <a href="#A2.SS5" title="B.5 Proof of Hierarchical linear model lower bound ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.5</span></a>. We see that the size of the meta-training set has an inverse exponential scaling in the dimension, <math id="S5.SS1.p3.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>. This reflects the complexity of the space growing exponentially in dimensions and the need for a matching growth in data size to cover the environment sufficiently.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Minimax upper bounds</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">To compute upper bounds on the estimation error, we require an additional assumption. Namely, we will assume that the design matrices also have bounded minimum singular values, <math id="S5.SS2.p1.m1" class="ltx_Math" alttext="0&lt;s\leq\sigma_{\min}(X/\sqrt{n})" display="inline"><mrow><mn>0</mn><mo>&lt;</mo><mi>s</mi><mo>≤</mo><mrow><msub><mi>σ</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>/</mo><msqrt><mi>n</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> (see <cite class="ltx_cite ltx_citemacro_citet">Raskutti<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib31" title="Minimax rates of estimation for high-dimensional linear regression over ⁢ℓ_q-balls" class="ltx_ref">2011</a>)</cite> for some justification). For the upper-bounds, we allow the bounds on the singular values of the design matrices and the observation noise in the novel task to be different than those in the meta-training tasks. We note that we can still recover the setting assumed in the lower bounds, where all tasks match on these parameters, as a special case.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">The learner observes <math id="S5.SS2.p2.m1" class="ltx_Math" alttext="{n}" display="inline"><mi>n</mi></math> data points from each linear regression model in <math id="S5.SS2.p2.m2" class="ltx_Math" alttext="\{P_{\bm{\theta}_{1}},\ldots,P_{\bm{\theta}_{M}}\}\subset\mathcal{P}" display="inline"><mrow><mrow><mo stretchy="false">{</mo><msub><mi>P</mi><msub><mi>𝜽</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>P</mi><msub><mi>𝜽</mi><mi>M</mi></msub></msub><mo stretchy="false">}</mo></mrow><mo>⊂</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math>. We then bound the error of estimating <math id="S5.SS2.p2.m3" class="ltx_Math" alttext="\bm{\theta}_{M+1}" display="inline"><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>, for which <math id="S5.SS2.p2.m4" class="ltx_Math" alttext="{k}" display="inline"><mi>k</mi></math> samples are available.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">The expected error rate of the MAP estimator can be decomposed as the posterior variance and bias squared. In the appendix we provide a detailed derivation of these results. The bound depends on dimensionality <math id="S5.SS2.p3.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>, the observation noise in each task <math id="S5.SS2.p3.m2" class="ltx_Math" alttext="\sigma^{2}_{i}" display="inline"><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></math>, the number of tasks <math id="S5.SS2.p3.m3" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>, the number of data points in each meta-training task <math id="S5.SS2.p3.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>, and the number of data points in the novel task <math id="S5.SS2.p3.m5" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>.</p>
</div>
<div id="Thmtheorem4" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4</span></span><span class="ltx_text ltx_font_bold"> </span>(Meta Linear Regression Upper Bound)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math id="Thmtheorem4.p1.m1" class="ltx_Math" alttext="\bm{\hat{\theta}}_{M+1}" display="inline"><msub><mover accent="true"><mi>𝛉</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub></math> be the maximum-a-posteriori estimator, <math id="Thmtheorem4.p1.m2" class="ltx_Math" alttext="\mu_{\bm{\theta}_{M+1}|Y_{1:M+1}}" display="inline"><msub><mi>μ</mi><mrow><msub><mi>𝛉</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub><mo fence="false" mathvariant="normal">|</mo><msub><mi>Y</mi><mrow><mn mathvariant="normal">1</mn><mo lspace="0.278em" mathvariant="normal" rspace="0.278em">:</mo><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></mrow></msub></mrow></msub></math>. Then,</span></p>
<table id="S5.Ex12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex12.m1" class="ltx_Math" alttext="R^{*}_{\mathcal{P}_{LR}}\leq\sup_{\bm{\theta}_{1},\ldots,\bm{\theta}_{M+1}\in%
\mathbb{B}_{2}(1)}\mathbb{E}[\|\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1}\|^{2}%
]\leq O\Bigl{(}d\sigma^{2}_{M+1}C(M,{n},{k})^{-2}D(M,{n},{k})\Bigr{)}" display="block"><mrow><msubsup><mi>R</mi><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub><mo>*</mo></msubsup><mo rspace="0.1389em">≤</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mrow><msub><mi>𝜽</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>∈</mo><mrow><msub><mi>𝔹</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mrow><mo stretchy="false">‖</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>d</mi><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><mi>C</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><mi>n</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><mi>n</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where,</span></p>
<table id="S5.Ex13" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex13.m1" class="ltx_Math" alttext="C(M,{n},{k})=\left[{k}+\frac{M{n}}{\frac{{n}(M+\kappa^{2})s_{2}^{2}}{\alpha_{2%
}}+A}\right],\textrm{ and, }D(M,{n},{k})=\left[{k}+\frac{M{n}}{(\frac{{n}}{L_{%
1}}+A_{1})(\frac{M{n}}{L_{2}}+A_{2})}\right]." display="block"><mrow><mrow><mrow><mrow><mi>C</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><mi>n</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>2</mn></msub></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mtext mathvariant="italic"> and, </mtext><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><mi>n</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mfrac><mi>n</mi><msub><mi>L</mi><mn>1</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><msub><mi>L</mi><mn>2</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Expectations are taken over the data conditioned on <math id="Thmtheorem4.p1.m3" class="ltx_Math" alttext="\bm{\theta}_{1},\ldots,\bm{\theta}_{M+1}" display="inline"><mrow><msub><mi>𝛉</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">,</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">,</mo><msub><mi>𝛉</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub></mrow></math>. Additional terms not depending on <math id="Thmtheorem4.p1.m4" class="ltx_Math" alttext="d,\&gt;M,\&gt;{n},\&gt;{k}" display="inline"><mrow><mi>d</mi><mo mathvariant="normal" rspace="0.387em">,</mo><mi>M</mi><mo mathvariant="normal" rspace="0.387em">,</mo><mi>n</mi><mo mathvariant="normal" rspace="0.387em">,</mo><mi>k</mi></mrow></math> are defined in Appendix <a href="#A3.SS2" title="C.2 Posterior estimate ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.2</span></a>.</span></p>
</div>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">While the bounds presented in Theorem <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are relatively complicated, we can probe the asymptotic convergence of the MAP estimator to the true task parameters, <math id="S5.SS2.p4.m1" class="ltx_Math" alttext="\bm{\theta}_{M+1}" display="inline"><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>. In the following section, we will discuss some of the consequences of this result and its implications for our lower bounds.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Asymptotic behavior of the MAP estimator</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">We first notice that when <math id="S5.SS3.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is small, the risk cannot be reduced to zero by adding more meta-training data. Recent work has suggested such a relationship may be inevitable <cite class="ltx_cite ltx_citemacro_citep">(Hanneke and Kpotufe, <a href="#bib.bib48" title="A no-free-lunch theorem for multitask learning" class="ltx_ref">2020</a>)</cite>. Our lower bound presented in Corollary <a href="#Thmcorollary2" title="Corollary 2. ‣ A tighter bound on partially observed environments ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> agrees that more samples from a small number of meta-training tasks will not reduce the error to zero. However,
unlike our lower bounds
based on local packing, the lower bounds presented in this section predict that if the meta-training tasks cover the space sufficiently then an optimal algorithm might hope to reduce the error entirely with enough samples. We hypothesize that this gap is due to limitations in the standard proof techniques we utilize for the lower-bounds when the number of tasks grows, and expect a sharper bound may be possible.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">To emulate the few-shot learning setting where <math id="S5.SS3.p2.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is relatively small, we consider <math id="S5.SS3.p2.m2" class="ltx_Math" alttext="n\rightarrow\infty" display="inline"><mrow><mi>n</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></math>, with <math id="S5.SS3.p2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> and <math id="S5.SS3.p2.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> fixed. In this case, the risk is bounded as,</p>
<table id="S5.Ex14" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex14.m1" class="ltx_Math" alttext="\sup_{\bm{\theta}_{1},\ldots,\bm{\theta}_{M+1}\in\mathbb{B}_{2}(1)}\mathbb{E}[%
\|\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1}\|^{2}]\leq O\left(d\sigma^{2}_{M+1%
}\left[k+\frac{2\alpha_{2}M}{M+\kappa^{2}}\right]^{-1}\right)," display="block"><mrow><mrow><mrow><munder><mo movablelimits="false">sup</mo><mrow><mrow><msub><mi>𝜽</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>∈</mo><mrow><msub><mi>𝔹</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mrow><mo stretchy="false">‖</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>d</mi><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mn>2</mn><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub><mo>⁢</mo><mi>M</mi></mrow><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S5.SS3.p2.m5" class="ltx_Math" alttext="\alpha_{2}={\sigma_{M+1}^{2}}/{\sigma_{\theta}^{2}}" display="inline"><mrow><msub><mi>α</mi><mn>2</mn></msub><mo>=</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>/</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow></mrow></math>, is the ratio of the observation noise to the variance in sampling <math id="S5.SS3.p2.m6" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math>, and <math id="S5.SS3.p2.m7" class="ltx_Math" alttext="\kappa" display="inline"><mi>κ</mi></math> is the condition number of the design matrices. This leads to a key takeaway: if the observed samples from <math id="S5.SS3.p2.m8" class="ltx_Math" alttext="P_{M+1}" display="inline"><msub><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math> vary considerably more than the parameters <math id="S5.SS3.p2.m9" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math>, then observing more samples in <math id="S5.SS3.p2.m10" class="ltx_Math" alttext="S_{1:M}" display="inline"><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math> will significantly improve convergence towards the true parameters in the small <math id="S5.SS3.p2.m11" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> regime. Further, adding more tasks (increasing <math id="S5.SS3.p2.m12" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>) also improves these constant factors by removing the dependence on the condition number, <math id="S5.SS3.p2.m13" class="ltx_Math" alttext="\kappa" display="inline"><mi>κ</mi></math>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Empirical Investigations</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In this section, we provide additional quantitative exploration of the upper bound studied in Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
The aim is to take steps towards relating the bounds to experimental results;
we know of little theoretical work in meta-learning that attempt to relate their results to
practical empirical datasets. Full details of the experiments in this section can be found in Appendix <a href="#A4" title="Appendix D Additional Experiment Details ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Hierarchical Bayes polynomial regression</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">We first focus on the setting of polynomial regression over inputs in the range <math id="S6.SS1.p1.m1" class="ltx_Math" alttext="[-1,1]" display="inline"><mrow><mo stretchy="false">[</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math>. Some examples of these functions and samples are presented in Figure <a href="#S3.F1" title="Figure 1 ‣ An example with meta-linear regression ‣ 3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, alongside the MAP and MLE estimates for the novel task.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p">Figure <a href="#S6.F2" title="Figure 2 ‣ 6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the analytical expected error rate (risk) under various environment settings. We observe that even in this simple hierarchical model, the estimator exhibits complex behavior that is correctly predicted by Theorem <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In Figure <a href="#S6.F2" title="Figure 2 ‣ 6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>A, we varied the novel task difficulty by increasing the novel task observation noise (<math id="S6.SS1.p2.m1" class="ltx_Math" alttext="\sigma^{2}_{M+1}" display="inline"><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></math>). We plot three curves for three different dataset size configurations. When the novel task is much noisier than the source tasks, it is greatly beneficial to add more meta-training data (blue vs. red). And while larger <math id="S6.SS1.p2.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> made little difference when the novel task was relatively difficult (blue vs. green), the expected loss was orders of magnitude lower when the novel task became easier. In Figure <a href="#S6.F2" title="Figure 2 ‣ 6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>B, we fixed the relative task difficulty and instead varied <math id="S6.SS1.p2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> and <math id="S6.SS1.p2.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>. The x-axis now indicates the total data <math id="S6.SS1.p2.m5" class="ltx_Math" alttext="Mn+k" display="inline"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mo>+</mo><mi>k</mi></mrow></math> available to the learner. We observed that adding more tasks has a large effect in the low-data regime but, as predicted, the error has a non-zero asymptotic lower-bound — eventually it is more beneficial to add more novel-task data samples.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p class="ltx_p">These empirical simulations verify that our theoretical analysis is predictive of the behavior of this meta learning algorithm, as each of these observations can be inferred from Theorem <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. While this model is simple, it captures key features of and provides insight into the more general meta-learning problem. We also explored a non-linear sinusoid meta-regression problem <cite class="ltx_cite ltx_citemacro_citep">(Finn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>)</cite>, finding that our theory is largely predictive of the general trends in this setting too.</p>
</div>
<figure id="S6.F2" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_center ltx_align_middle" style="width:216.8pt;">
<span id="S6.F2.p1" class="ltx_para ltx_align_center"><img src="x2.png" id="S6.F2.p1.g1" class="ltx_graphics ltx_img_landscape" width="830" height="415" alt="Refer to caption">
<span class="ltx_p">(A)</span>
</span></span></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_center ltx_align_middle" style="width:216.8pt;">
<span id="S6.F2.p2" class="ltx_para ltx_align_center"><img src="x3.png" id="S6.F2.p2.g1" class="ltx_graphics ltx_img_landscape" width="830" height="415" alt="Refer to caption">
<span class="ltx_p">(B)</span>
</span></span></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The expected error rate of the hierarchical MAP estimator, <math id="S6.F2.m4" class="ltx_Math" alttext="\bm{\hat{\theta}}_{M+1}" display="inline"><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>, over different environment
hyperparameter settings. <span class="ltx_text ltx_font_bold">A)</span> The novel task observation noise is increased, making the novel task harder to learn. <span class="ltx_text ltx_font_bold">B)</span> We increase the size of the dataset, in one case adding new tasks (<math id="S6.F2.m5" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>) and in the other adding new novel task data samples (<math id="S6.F2.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>).</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Sinusoid regression with MAML</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">Following the connections between MAML and hierarchical Bayes explored by <cite class="ltx_cite ltx_citemacro_citet">Grant<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib30" title="Recasting gradient-based meta-learning as hierarchical Bayes" class="ltx_ref">2018</a>)</cite>, we also explored regression on sinusoids using MAML. Our aim was to investigate how predictive our linear theory is for this highly non-linear problem setting. As in <cite class="ltx_cite ltx_citemacro_citet">Finn<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>)</cite>, we sample sinusoid functions by placing a prior over the amplitude and phase. In other works <cite class="ltx_cite ltx_citemacro_citep">(Finn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>; Grant<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib30" title="Recasting gradient-based meta-learning as hierarchical Bayes" class="ltx_ref">2018</a>)</cite> the same prior is used for the training and testing stages. However, to better measure generalization to novel tasks we use different prior distributions when training versus evaluating the model.</p>
</div>
<figure id="S6.F3" class="ltx_figure ltx_align_floatright"><img src="x4.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="399" height="200" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average risk for regressing sinusoid functions with MAML.</figcaption>
</figure>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p">We display the risk averaged over 30 trials in Figure <a href="#S6.F3" title="Figure 3 ‣ 6.2 Sinusoid regression with MAML ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We varied the novel task difficulty by increasing the observation noise in the novel task.
We plot separate curves for different dataset size configurations, and observe that the empirical results align fairly well with the results derived by sampling the hierarchical model (Figure <a href="#S6.F2" title="Figure 2 ‣ 6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>A). Adding more meta-training data (increasing <math id="S6.SS2.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>) is beneficial (green vs. yellow)
and adding more test data-points (higher <math id="S6.SS2.p2.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>) is also beneficial (red vs. green). Here however, these relationships did not interact with the task difficulty, as the wins for increased meta-training and meta-testing data were consistent, until task noise prevents any setting of the model from performing the task.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">Meta-learning algorithms identify the inductive bias from source tasks and make models more adaptive towards unseen novel distribution.
In this paper, we take initial steps towards characterizing the difficulty of meta-learning and understanding how these limitations present in practice. We have derived both lower bounds and upper bounds on the error of meta-learners, which are particularly relevant in the few-shot learning setting where <math id="S7.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is small. Our bounds capture key features of the meta-learning problem, such as the effect of increasing the number of shots or training tasks.
We have also identified a gap between our lower and upper bounds when there are a large number of training tasks, which we hypothesize is a limitation of the proof technique that we applied to derive the lower bounds — suggesting an exciting direction for future research.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">This work benefited greatly from the input of many other researchers. In particular, we extend our thanks to Shai Ben-David, Karolina Dziugaite, Samory Kpotufe, and Daniel Roy for discussions and feedback on the results presented in this work. We thank Ahmad Beirami and anonymous reviewers for their valuable feedback that led to significant improvements to this paper. We also thank Elliot Creager, Will Grathwohl, Mufan Li, and many of our other colleagues at the Vector Institute for feedback that greatly improved the presentation of this work. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute (<a href="www.vectorinstitute.ai/partners" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.vectorinstitute.ai/partners</a>).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib18" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Amit and R. Meir (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Meta-learning by adjusting priors based on extended PAC-bayes theory</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1711.01244</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p1" title="3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to learn by gradient descent by gradient descent</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 29</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3981–3989</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Baxter (2000)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A model of inductive bias learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of artificial intelligence research</span> <span class="ltx_text ltx_bib_volume">12</span>, <span class="ltx_text ltx_bib_pages"> pp. 149–198</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A theory of learning from different domains</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine learning</span> <span class="ltx_text ltx_bib_volume">79</span> (<span class="ltx_text ltx_bib_number">1-2</span>), <span class="ltx_text ltx_bib_pages"> pp. 151–175</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p1" title="3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Ben-David and R. S. Borbely (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A notion of task relatedness yielding provable multiple-task learning guarantees</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine learning</span> <span class="ltx_text ltx_bib_volume">73</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 273–287</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS1.p2" title="4.1 Measuring Task-Relatedness ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Bullins, E. Hazan, A. Kalai, and R. Livni (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generalize across tasks: efficient algorithms for linear representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 30th International Conference on Algorithmic Learning Theory</span>,  <span class="ltx_text ltx_bib_editor">A. Garivier and S. Kale (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">98</span>, <span class="ltx_text ltx_bib_place">Chicago, Illinois</span>, <span class="ltx_text ltx_bib_pages"> pp. 235–246</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://proceedings.mlr.press/v98/bullins19a.html" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Cao, M. Law, and S. Fidler (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A theoretical analysis of the number of shots in few-shot learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1909.11722</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. M. Cover and J. A. Thomas (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Elements of information theory</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">John Wiley &amp; Sons</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#Thmlemma1.p1" title="Lemma 1. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Lemma 1</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Denevi, C. Ciliberto, R. Grazzi, and M. Pontil (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning-to-learn stochastic gradient descent with biased regularization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 36th International Conference on Machine Learning</span>,  <span class="ltx_text ltx_bib_editor">K. Chaudhuri and R. Salakhutdinov (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">97</span>, <span class="ltx_text ltx_bib_place">Long Beach, California, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1566–1575</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Few-shot learning via learning the representation, provably</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2002.09434</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS1.p2" title="4.1 Measuring Task-Relatedness ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RL$^2$: fast reinforcement learning via slow reinforcement learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1611.02779</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1611.02779</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Fano (1961)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Transmission of information</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">A Statistical Theory of Communication</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#Thmlemma1.p1" title="Lemma 1. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Lemma 1</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Finn, P. Abbeel, and S. Levine (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Model-agnostic meta-learning for fast adaptation of deep networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 34th International Conference on Machine Learning-Volume 70</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1126–1135</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#A4.SS2.p1" title="D.2 Sinusoid Regression with MAML ‣ Appendix D Additional Experiment Details ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§D.2</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S6.SS1.p3" title="6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.1</span></a>,
<a href="#S6.SS2.p1" title="6.2 Sinusoid regression with MAML ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bayesian data analysis</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Chapman and Hall/CRC</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p4" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Recasting gradient-based meta-learning as hierarchical Bayes</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1801.08930</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S6.SS2.p1" title="6.2 Sinusoid regression with MAML ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.2</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Hanneke and S. Kpotufe (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">On the value of target data in transfer learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 32</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 9871–9881</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Hanneke and S. Kpotufe (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A no-free-lunch theorem for multitask learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2006.15785</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S5.SS3.p1" title="5.3 Asymptotic behavior of the MAP estimator ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.3</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Jin, S. Wang, and Y. Zhou (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Regularized distance metric learning:theory and algorithm</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 22</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 862–870</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Z. Khas’ minskii (1979)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A lower bound on the risks of non-parametric estimates of densities in the uniform metric</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Theory of Probability &amp; Its Applications</span> <span class="ltx_text ltx_bib_volume">23</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 794–798</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p7" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#Thmlemma2.p1" title="Lemma 2. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Lemma 2</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Khodak, M. Balcan, and A. Talwalkar (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Provable guarantees for gradient-based meta-learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1902.10644</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Kpotufe and G. Martinet (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Marginal singularity, and the benefits of labels in covariate-shift</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 31st Conference On Learning Theory</span>,  <span class="ltx_text ltx_bib_editor">S. Bubeck, V. Perchet, and P. Rigollet (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">75</span>, <span class="ltx_text ltx_bib_place"></span>, <span class="ltx_text ltx_bib_pages"> pp. 1882–1886</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Loh (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">On lower bounds for statistical learning theory</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Entropy</span> <span class="ltx_text ltx_bib_volume">19</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp. 617</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.p7" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS2.p1" title="4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#S4.SS2.p2" title="4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#Thmlemma3.p1" title="Lemma 3. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Lemma 3</span></a>.
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Lucas, M. Ren, and R. Zemel (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Information-theoretic limitations on novel task generalization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Neurips 2019 Workshop on Machine Learning with Guarantees</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. MacKay, P. Vicol, J. Lorraine, D. Duvenaud, and R. B. Grosse (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Self-tuning networks: bilevel optimization of hyperparameters using structured best-response functions</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">7th International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Maurer (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Transfer bounds for linear feature learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine learning</span> <span class="ltx_text ltx_bib_volume">75</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 327–350</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Mehta, D. Lee, and A. Gray (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minimax multi-task learning and a generalized loss-compositional paradigm for mtl</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">25</span>, <span class="ltx_text ltx_bib_pages"> pp. 2150–2158</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Metz, N. Maheswaranathan, B. Cheung, and J. Sohl-Dickstein (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Meta-learning update rules for unsupervised representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">7th International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Mohri and A. M. Medina (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">New analysis and algorithm for learning with drifting distributions</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Algorithmic Learning Theory</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 124–138</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Mousavi Kalan, Z. Fabian, S. Avestimehr, and M. Soltanolkotabi (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">33</span>, <span class="ltx_text ltx_bib_pages"> pp. 1959–1969</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://proceedings.neurips.cc/paper/2020/file/151d21647527d1079781ba6ae6571ffd-Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S2.p3" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Pentina and C. Lampert (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A PAC-bayesian bound for lifelong learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 991–999</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.p1" title="3 Novel task environment risk ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Raskutti, M. J. Wainwright, and B. Yu (2011)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minimax rates of estimation for high-dimensional linear regression over <math id="bib.bib31.m1a" class="ltx_Math" alttext="\ell\_q" display="inline"><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi>q</mi></mrow></math>-balls</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE transactions on information theory</span> <span class="ltx_text ltx_bib_volume">57</span> (<span class="ltx_text ltx_bib_number">10</span>), <span class="ltx_text ltx_bib_pages"> pp. 6976–6994</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#S5.SS1.p2" title="5.1 Minimax lower bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.SS2.p1" title="5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. E. Rasmussen and H. Nickisch (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gaussian processes for machine learning (GPML) toolbox</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">J. Mach. Learn. Res.</span> <span class="ltx_text ltx_bib_volume">11</span>, <span class="ltx_text ltx_bib_pages"> pp. 3011–3015</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Ravi and H. Larochelle (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Optimization as a model for few-shot learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Conference on Learning Representations</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p6" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Robbins (1956)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An empirical bayes approach to statistics</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Berkeley, Calif.</span>, <span class="ltx_text ltx_bib_pages"> pp. 157–163</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, and H. Khandeparkar (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A theoretical analysis of contrastive unsupervised representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 5628–5637</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Snell, K. Swersky, and R. Zemel (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Prototypical networks for few-shot learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 4077–4087</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Vilalta and Y. Drissi (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A perspective view and survey of meta-learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Artificial intelligence review</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 77–95</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, <span class="ltx_text ltx_bib_etal">et al.</span> (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Matching networks for one shot learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in neural information processing systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3630–3638</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Von Neumann (1937)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Some matrix-inequalities and metrization of matric space</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#Thmlemma8.p1" title="Lemma 8. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Lemma 8</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Wang, H. Zhang, P. Liu, Z. Shen, and J. Pineau (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multitask metric learning: theory and algorithm</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of Machine Learning Research</span>,  <span class="ltx_text ltx_bib_editor">K. Chaudhuri and M. Sugiyama (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">89</span>, <span class="ltx_text ltx_bib_place"></span>, <span class="ltx_text ltx_bib_pages"> pp. 3362–3371</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Yang and A. Barron (1999)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Information-theoretic determination of minimax rates of convergence</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Annals of Statistics</span>, <span class="ltx_text ltx_bib_pages"> pp. 1564–1599</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p7" title="2 Related work ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Notation</h2>

<figure id="A1.T1" class="ltx_table">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold ltx_align_top">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m1" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒳</mi></math></th>
<td class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The domain of the data, e.g. <math id="A1.T1.m2" class="ltx_Math" alttext="\mathbb{R}^{d}" display="inline"><msup><mi>ℝ</mi><mi>d</mi></msup></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m3" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒴</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The range of the data, e.g. <math id="A1.T1.m4" class="ltx_Math" alttext="\mathbb{R}" display="inline"><mi>ℝ</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m5" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒵</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The product space <math id="A1.T1.m6" class="ltx_Math" alttext="\mathcal{X}\times\mathcal{Y}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m7" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">A collection of distributions over <math id="A1.T1.m8" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒵</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m9" class="ltx_Math" alttext="\mathcal{J}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒥</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">A (finite) subset of distributions in <math id="A1.T1.m10" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m11" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">An element of <math id="A1.T1.m12" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m13" class="ltx_Math" alttext="P^{k}" display="inline"><msup><mi>P</mi><mi>k</mi></msup></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The product distribution, whose samples correspond to <math id="A1.T1.m14" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> independent draws from <math id="A1.T1.m15" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m16" class="ltx_Math" alttext="P_{1:M}" display="inline"><msub><mi>P</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The product distribution, <math id="A1.T1.m17" class="ltx_Math" alttext="\Pi_{i=1}^{M}P_{i}" display="inline"><mrow><msubsup><mi mathvariant="normal">Π</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mo>⁢</mo><msub><mi>P</mi><mi>i</mi></msub></mrow></math>, for <math id="A1.T1.m18" class="ltx_Math" alttext="P_{i}\in\mathcal{P}" display="inline"><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m19" class="ltx_Math" alttext="\bar{P}_{1:M}" display="inline"><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The mixture distribution, <math id="A1.T1.m20" class="ltx_Math" alttext="\frac{1}{M}\sum_{i=1}^{M}P_{i}" display="inline"><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></math>, for <math id="A1.T1.m21" class="ltx_Math" alttext="P_{i}\in\mathcal{P}" display="inline"><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m22" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Ω</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">A metric space, containing parameters for each distribution</p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m23" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">A functional, mapping distributions in <math id="A1.T1.m24" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> to parameters in <math id="A1.T1.m25" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Ω</mi></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m26" class="ltx_Math" alttext="\hat{\theta}" display="inline"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">An estimator <math id="A1.T1.m27" class="ltx_Math" alttext="\hat{\theta}:\mathcal{Z}^{n}\rightarrow\mathcal{F}" display="inline"><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒵</mi><mi>n</mi></msup><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">ℱ</mi></mrow></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m28" class="ltx_Math" alttext="I(X;Y)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The mutual information between random variables <math id="A1.T1.m29" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and <math id="A1.T1.m30" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math>.</p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m31" class="ltx_Math" alttext="S,S_{M+1}" display="inline"><mrow><mi>S</mi><mo>,</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">Denotes training datasets drawn <em class="ltx_emph ltx_font_italic">i.i.d </em>from some <math id="A1.T1.m32" class="ltx_Math" alttext="P\in\mathcal{P}" display="inline"><mrow><mi>P</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math>. Typically <math id="A1.T1.m33" class="ltx_Math" alttext="S=\{z_{1},\ldots,z_{n}\}" display="inline"><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>z</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow></mrow></math>, <math id="A1.T1.m34" class="ltx_Math" alttext="S_{M+1}=\{z^{\prime}_{1},\ldots,z^{\prime}_{k}\}" display="inline"><mrow><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>z</mi><mn>1</mn><mo>′</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>z</mi><mi>k</mi><mo>′</mo></msubsup><mo stretchy="false">}</mo></mrow></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m35" class="ltx_Math" alttext="S_{\mathcal{P}}" display="inline"><msub><mi>S</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi></msub></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">Denotes a meta-training set drawn <em class="ltx_emph ltx_font_italic">i.i.d </em>from <math id="A1.T1.m36" class="ltx_Math" alttext="P_{1:M}" display="inline"><msub><mi>P</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<math id="A1.T1.m37" class="ltx_Math" alttext="[N]" display="inline"><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></math>, for <math id="A1.T1.m38" class="ltx_Math" alttext="N\in\mathbb{N}" display="inline"><mrow><mi>N</mi><mo>∈</mo><mi>ℕ</mi></mrow></math>
</th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">Indicates the set <math id="A1.T1.m39" class="ltx_Math" alttext="\{1,\ldots,N\}" display="inline"><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>N</mi><mo stretchy="false">}</mo></mrow></math></p>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T1.m40" class="ltx_Math" alttext="\mathbb{B}_{p}(r)" display="inline"><mrow><msub><mi>𝔹</mi><mi>p</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mrow></math></th>
<td class="ltx_td ltx_align_justify" style="padding-left:4.0pt;padding-right:4.0pt;">
<p class="ltx_p ltx_align_left ltx_align_top">The <math id="A1.T1.m41" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>-norm ball of radius <math id="A1.T1.m42" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math>, centered at <math id="A1.T1.m43" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>.</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of notation used in this manuscript</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Lower Bound Proofs</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">We will make use of several standard results below, which we present here.</p>
</div>
<div id="Thmlemma1" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Fano’s Inequality <cite class="ltx_cite ltx_citemacro_citep">(Fano, <a href="#bib.bib11" title="Transmission of information" class="ltx_ref">1961</a>; Cover and Thomas, <a href="#bib.bib12" title="Elements of information theory" class="ltx_ref">2012</a>)</cite> <span class="ltx_text ltx_font_medium">
For any estimator <math id="Thmlemma1.p1.m1" class="ltx_Math" alttext="\hat{Y}" display="inline"><mover accent="true"><mi mathvariant="normal">Y</mi><mo mathvariant="normal">^</mo></mover></math> of a random variable <math id="Thmlemma1.p1.m2" class="ltx_Math" alttext="Y" display="inline"><mi mathvariant="normal">Y</mi></math> such that <math id="Thmlemma1.p1.m3" class="ltx_Math" alttext="Y\rightarrow Z\rightarrow\hat{Y}" display="inline"><mrow><mi mathvariant="normal">Y</mi><mo mathvariant="normal" stretchy="false">→</mo><mi mathvariant="normal">Z</mi><mo mathvariant="normal" stretchy="false">→</mo><mover accent="true"><mi mathvariant="normal">Y</mi><mo mathvariant="normal">^</mo></mover></mrow></math> forms a Markov chain, it holds that,</span></span></p>
<table id="A2.Ex15" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex15.m1" class="ltx_Math" alttext="\mathbb{P}(\hat{Y}\neq Y)\geq\dfrac{H(Y|Z)-1}{\log_{2}|Y|}=\dfrac{H(Y)-I(Y;Z)-%
1}{\log_{2}|Y|}." display="block"><mrow><mrow><mrow><mi>ℙ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>≠</mo><mi>Y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mfrac><mrow><mrow><mi>H</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Y</mi><mo fence="false">|</mo><mi>Z</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mrow><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mi>H</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mrow><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="Thmlemma2" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Mutual information equality <cite class="ltx_cite ltx_citemacro_citep">(Khas’ minskii, <a href="#bib.bib21" title="A lower bound on the risks of non-parametric estimates of densities in the uniform metric" class="ltx_ref">1979</a>)</cite><span class="ltx_text ltx_font_medium">
Consider random variables <math id="Thmlemma2.p1.m1" class="ltx_Math" alttext="Z_{1},Z_{2},Y" display="inline"><mrow><msub><mi mathvariant="normal">Z</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">,</mo><msub><mi mathvariant="normal">Z</mi><mn mathvariant="normal">2</mn></msub><mo mathvariant="normal">,</mo><mi mathvariant="normal">Y</mi></mrow></math>, then,</span></span></p>
<table id="A2.Ex16" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex16.m1" class="ltx_Math" alttext="I(Y;(Z_{1},Z_{2}))+I(Z_{1};Z_{2})=I(Z_{1};(Z_{2},Y))+I(Z_{2};Y)" display="block"><mrow><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo>;</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>;</mo><msub><mi>Z</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>;</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>;</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="Thmlemma3" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Local packing lemma <cite class="ltx_cite ltx_citemacro_citep">(Loh, <a href="#bib.bib13" title="On lower bounds for statistical learning theory" class="ltx_ref">2017</a>)</cite><span class="ltx_text ltx_font_medium">
Consider distributions <math id="Thmlemma3.p1.m1" class="ltx_Math" alttext="P_{1},\ldots,P_{J}\in\mathcal{P}" display="inline"><mrow><mrow><msub><mi mathvariant="normal">P</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">,</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">,</mo><msub><mi mathvariant="normal">P</mi><mi mathvariant="normal">J</mi></msub></mrow><mo mathvariant="normal">∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math>. Let <math id="Thmlemma3.p1.m2" class="ltx_Math" alttext="Y" display="inline"><mi mathvariant="normal">Y</mi></math> be a random variable distributed uniformly on <math id="Thmlemma3.p1.m3" class="ltx_Math" alttext="[J]" display="inline"><mrow><mo mathvariant="normal" stretchy="false">[</mo><mi mathvariant="normal">J</mi><mo mathvariant="normal" stretchy="false">]</mo></mrow></math> and let <math id="Thmlemma3.p1.m4" class="ltx_Math" alttext="Z|\{Y=j\}" display="inline"><mrow><mi mathvariant="normal">Z</mi><mo fence="false" mathvariant="normal">|</mo><mrow><mo mathvariant="normal" stretchy="false">{</mo><mrow><mi mathvariant="normal">Y</mi><mo mathvariant="normal">=</mo><mi mathvariant="normal">j</mi></mrow><mo mathvariant="normal" stretchy="false">}</mo></mrow></mrow></math> be a vector of <math id="Thmlemma3.p1.m5" class="ltx_Math" alttext="k" display="inline"><mi mathvariant="normal">k</mi></math> <em class="ltx_emph ltx_font_upright">i.i.d </em>samples from <math id="Thmlemma3.p1.m6" class="ltx_Math" alttext="P_{j}" display="inline"><msub><mi mathvariant="normal">P</mi><mi mathvariant="normal">j</mi></msub></math>. Then,</span></span></p>
<table id="A2.Ex17" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex17.m1" class="ltx_Math" alttext="I(Y;Z)\leq\frac{k}{J^{2}}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}\left(P_{i}\|P_{%
j}\right)." display="block"><mrow><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mfrac><mi>k</mi><msup><mi>J</mi><mn>2</mn></msup></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></munder><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="A2.p2" class="ltx_para">
<p class="ltx_p">We will require a novel local packing bound for the novel-task risk, which we present in Lemma <a href="#Thmlemma5" title="Lemma 5 (Leave-one-task-out mixture local packing). ‣ B.4 Bounds using mixture distributions ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>IID Lower Bound</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p">We first prove the <em class="ltx_emph ltx_font_italic">i.i.d </em>result, which will serve as a guide for our novel lower bounds.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p class="ltx_p">See <a href="#Thmtheorem2" title="Theorem 2 (IID minimax lower-bound). ‣ 4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS1.p3" class="ltx_para">
<p class="ltx_p">First, notice that,</p>
<table id="A2.Ex18" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex18.m1" class="ltx_Math" alttext="\sup_{P\in\mathcal{P}}\mathbb{E}_{S\sim P^{k}}\left[\ell(\hat{\theta}(S),%
\theta_{P})\right]\geq\dfrac{1}{J}\sum_{i=1}^{J}\mathbb{E}_{S\sim P_{i}^{k}}%
\left[\ell(\hat{\theta}(S),\theta_{P_{i}})\right]." display="block"><mrow><mrow><mrow><munder><mo movablelimits="false">sup</mo><mrow><mi>P</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></munder><mrow><msub><mi>𝔼</mi><mrow><mi>S</mi><mo>∼</mo><msup><mi>P</mi><mi>k</mi></msup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><mi>P</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>≥</mo><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mrow><msub><mi>𝔼</mi><mrow><mi>S</mi><mo>∼</mo><msubsup><mi>P</mi><mi>i</mi><mi>k</mi></msubsup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS1.p4" class="ltx_para">
<p class="ltx_p">Now define the decision rule,</p>
<table id="A2.Ex19" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex19.m1" class="ltx_math_unparsed" alttext="f(S)=\mathop{\textrm{argmin}}_{1\leq j\leq J}\rho(\hat{\theta}(S),\theta_{P_{j%
}})]," display="block"><mrow><mi>f</mi><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><munder><mtext>argmin</mtext><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>j</mi></msub></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">with ties broken arbitrarily. We proceed by bounding the expected loss. First, using Markov’s inequality,</p>
<table id="A4.EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex20.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}_{S\sim P_{i}^{k}}\left[\ell(\hat{\theta}(S),\theta_{P_%
{i}})\right]" display="inline"><mrow><msub><mi>𝔼</mi><mrow><mi>S</mi><mo>∼</mo><msubsup><mi>P</mi><mi>i</mi><mi>k</mi></msubsup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex20.m2" class="ltx_Math" alttext="\displaystyle\geq\psi(\delta)\mathbb{P}_{S\sim P^{k}_{i}}\left[\psi(\rho(\hat{%
\theta}(S),\theta_{P_{i}}))\geq\psi(\delta)\right]," display="inline"><mrow><mrow><mi></mi><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>ℙ</mi><mrow><mi>S</mi><mo>∼</mo><msubsup><mi>P</mi><mi>i</mi><mi>k</mi></msubsup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex21.m1" class="ltx_Math" alttext="\displaystyle=\psi(\delta)\mathbb{P}_{S\sim P^{k}_{i}}\left[\rho(\hat{\theta}(%
S),\theta_{P_{i}})\geq\delta\right]." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>ℙ</mi><mrow><mi>S</mi><mo>∼</mo><msubsup><mi>P</mi><mi>i</mi><mi>k</mi></msubsup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mi>δ</mi></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS1.p5" class="ltx_para">
<p class="ltx_p">Next, consider the case <math id="A2.SS1.p5.m1" class="ltx_math_unparsed" alttext="\rho(\hat{\theta}(S),\theta_{i}))&lt;\delta" display="inline"><mrow><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo><mo>&lt;</mo><mi>δ</mi></mrow></math>. Through the triangle inequality,</p>
<table id="A4.EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex22.m1" class="ltx_Math" alttext="\displaystyle\rho(\hat{\theta}(S),\theta_{P_{j}})" display="inline"><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>j</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex22.m2" class="ltx_Math" alttext="\displaystyle\geq\rho(\theta_{P_{i}},\theta_{P_{j}})-\rho(\hat{\theta}(S),%
\theta_{P_{i}})" display="inline"><mrow><mi></mi><mo>≥</mo><mrow><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>j</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex23.m1" class="ltx_Math" alttext="\displaystyle\geq 2\delta-\delta&gt;\rho(\hat{\theta}(S),\theta_{P_{i}})" display="inline"><mrow><mi></mi><mo>≥</mo><mrow><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow><mo>−</mo><mi>δ</mi></mrow><mo>&gt;</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, the probability that the distance is less than <math id="A2.SS1.p5.m2" class="ltx_Math" alttext="\delta" display="inline"><mi>δ</mi></math> is at least as large as the probability that the estimator is correct.</p>
<table id="A2.Ex24" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex24.m1" class="ltx_Math" alttext="\psi(\delta)\mathbb{P}_{S\sim P^{k}_{i}}\left[\ell(\hat{\theta}(S),\theta_{P_{%
i}})\geq\psi(\delta)\right]\geq\psi(\delta)\mathbb{P}(f(S)\neq i)." display="block"><mrow><mrow><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>ℙ</mi><mrow><mi>S</mi><mo>∼</mo><msubsup><mi>P</mi><mi>i</mi><mi>k</mi></msubsup></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>≥</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>ℙ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≠</mo><mi>i</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, using Fano’s inequality with <math id="A2.SS1.p5.m3" class="ltx_Math" alttext="Y=\pi_{M+1}" display="inline"><mrow><mi>Y</mi><mo>=</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>, and <math id="A2.SS1.p5.m4" class="ltx_Math" alttext="\hat{Y}=f(S)" display="inline"><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math> (and the corresponding Markov chain
<br class="ltx_break"><math id="A2.SS1.p5.m5" class="ltx_Math" alttext="\pi_{M+1}\rightarrow S\rightarrow f(S)" display="inline"><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">→</mo><mi>S</mi><mo stretchy="false">→</mo><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>), we have,</p>
<table id="A2.Ex25" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex25.m1" class="ltx_Math" alttext="\dfrac{1}{J}\sum_{i=1}^{J}\mathbb{P}(f(S)\neq i)\geq\dfrac{\log_{2}J-I(\pi_{M+%
1};Z)-1}{\log_{2}J}." display="block"><mrow><mrow><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mrow><mi>ℙ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≠</mo><mi>i</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>≥</mo><mfrac><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow><mo>−</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Combining the above inequalities with the Local Packing Lemma gives the final result.
∎</p>
</div>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Proof of Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p">See <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS2.p2" class="ltx_para">
<p class="ltx_p">As in the <em class="ltx_emph ltx_font_italic">i.i.d </em>case, we first bound the supremum from below with an average,</p>
<table id="A2.Ex26" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex26.m1" class="ltx_Math" alttext="\sup_{P^{\prime}_{1},\ldots,P^{\prime}_{M+1}\in\mathcal{P}}\mathbb{E}_{\begin{%
subarray}{c}S_{1:M}\sim(P^{\prime}_{1:M})^{n}\\
S_{M+1}\sim(P^{\prime}_{i})^{k}\end{subarray}}\left[\ell(\hat{\theta}_{S_{1:M}%
}(S_{M+1}),\theta_{(P^{\prime}_{i})})\right]\geq\frac{1}{J}\sum^{J}_{i=1}\frac%
{1}{{J-1\choose M}}\sum_{\pi|\{\pi_{M+1}=i\}}\mathbb{E}_{\begin{subarray}{c}w%
\sim W|\pi\\
z\sim Z|\pi\end{subarray}}\left[\ell(\hat{\theta}_{w}(z),\theta_{i})\right]," display="block"><mrow><mrow><mrow><munder><mo movablelimits="false">sup</mo><mrow><mrow><msubsup><mi>P</mi><mn>1</mn><mo>′</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>P</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>′</mo></msubsup></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></munder><mrow><msub><mi>𝔼</mi><mtable rowspacing="0pt"><mtr><mtd><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo>∼</mo><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow><mo>′</mo></msubsup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>i</mi><mo>′</mo></msubsup><mo stretchy="false">)</mo></mrow><mi>k</mi></msup></mrow></mtd></mtr></mtable></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mi>S</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><mrow><mo stretchy="false">(</mo><msubsup><mi>P</mi><mi>i</mi><mo>′</mo></msubsup><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>≥</mo><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mo>(</mo><mfrac linethickness="0pt"><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>M</mi></mfrac><mo>)</mo></mrow></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>π</mi><mo fence="false">|</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></munder><mrow><msub><mi>𝔼</mi><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>w</mi><mo>∼</mo><mrow><mi>W</mi><mo fence="false">|</mo><mi>π</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>z</mi><mo>∼</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>π</mi></mrow></mrow></mtd></mtr></mtable></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>w</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the inner sum is over all length <math id="A2.SS2.p2.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> orderings, <math id="A2.SS2.p2.m2" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math> with <math id="A2.SS2.p2.m3" class="ltx_Math" alttext="\pi_{M+1}=i" display="inline"><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi></mrow></math>.
</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p class="ltx_p">As before, we consider the following estimator,</p>
<table id="A2.Ex27" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex27.m1" class="ltx_Math" alttext="f(W,Z)=\mathop{\textrm{argmin}}_{1\leq j\leq J}\rho(\hat{\theta}_{W}(Z),\theta%
(P_{j}))" display="block"><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mtext>argmin</mtext><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>W</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>θ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS2.p4" class="ltx_para">
<p class="ltx_p">Using Markov’s inequality, and then following the proof of Theorem <a href="#Thmtheorem2" title="Theorem 2 (IID minimax lower-bound). ‣ 4.2 Comparison to risk of i.i.d learners ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we have,</p>
<table id="A4.EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex30.m1" class="ltx_Math" alttext="\displaystyle\frac{1}{J}\sum^{J}_{i=1}\frac{1}{{J-1\choose M}}\sum_{\pi|\{\pi_%
{M+1}=i\}}\mathbb{E}_{\begin{subarray}{c}w\sim W|\pi\\
z\sim Z|\pi\end{subarray}}\left[\ell(\hat{\theta}_{w}(z),\theta_{i})\right]" display="inline"><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo>(</mo><mfrac linethickness="0pt"><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>M</mi></mfrac><mo>)</mo></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>π</mi><mo fence="false">|</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></munder></mstyle><mrow><msub><mi>𝔼</mi><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>w</mi><mo>∼</mo><mrow><mi>W</mi><mo fence="false">|</mo><mi>π</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>z</mi><mo>∼</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>π</mi></mrow></mrow></mtd></mtr></mtable></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>w</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>θ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex30.m2" class="ltx_Math" alttext="\displaystyle\geq\frac{1}{J}\sum^{J}_{i=1}\frac{1}{{J-1\choose M}}\sum_{\pi|\{%
\pi_{M+1}=i\}}\psi(\delta)\mathbb{P}[f(W,Z)\neq i|\pi]" display="inline"><mrow><mi></mi><mo>≥</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo>(</mo><mfrac linethickness="0pt"><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>M</mi></mfrac><mo>)</mo></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>π</mi><mo fence="false">|</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi></mrow><mo stretchy="false">}</mo></mrow></mrow></munder></mstyle><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>ℙ</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≠</mo><mrow><mi>i</mi><mo fence="false">|</mo><mi>π</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex31.m1" class="ltx_Math" alttext="\displaystyle=\psi(\delta)\mathbb{P}[f(W,Z)\neq\pi_{M+1}]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>ℙ</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≠</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS2.p5" class="ltx_para">
<p class="ltx_p">with the use of Fano’s inequality, we arrive at,</p>
<table id="A4.EGx10" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex32.m1" class="ltx_Math" alttext="\displaystyle\psi(\delta)\left(1-\dfrac{I(\pi_{M+1};(W,Z))+1}{\log_{2}J}\right)" display="inline"><mrow><mi>ψ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>J</mi></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Conditioned on <math id="A2.SS2.p5.m1" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math>, each element of <math id="A2.SS2.p5.m2" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> and <math id="A2.SS2.p5.m3" class="ltx_Math" alttext="Z" display="inline"><mi>Z</mi></math> are independent but they are not identically distributed. Thus, with the application of Lemma <a href="#Thmlemma2" title="Lemma 2. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,</p>
<table id="A2.Ex33" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex33.m1" class="ltx_Math" alttext="I(\pi_{M+1};(W,Z))\leq I(\pi_{M+1};Z)+I(\pi_{M+1};W)" display="block"><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The result follows by combining these inequalities.
∎</p>
</div>
</div>
<section id="A2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Remark</h5>

<div id="A2.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">In the above proof of Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we did not need to make use of the form of the distribution of <math id="A2.SS2.SSS0.Px1.p1.m1" class="ltx_Math" alttext="W|{Y=i}" display="inline"><mrow><mrow><mi>W</mi><mo fence="false">|</mo><mi>Y</mi></mrow><mo>=</mo><mi>i</mi></mrow></math>, only that the correct graph structure was observed. This grants us some flexibility, which we utilize later in Section <a href="#A2.SS4" title="B.4 Bounds using mixture distributions ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a> to prove lower bounds for mixture distributions.</p>
</div>
<div id="A2.SS2.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">We now proceed with proofs of the corollaries of Section 4.
</p>
</div>
</section>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Local packing results</h3>

<div id="Thmlemma4" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 4</span></span><span class="ltx_text ltx_font_bold"> </span>(Meta-learning local packing)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Consider the same setting as in Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, then</span></p>
<table id="A2.Ex34" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex34.m1" class="ltx_Math" alttext="I(\pi_{M+1};W)\leq\frac{Mn}{J^{2}(J-1)}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}%
\left(P_{i}\|P_{j}\right)" display="block"><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><msup><mi>J</mi><mn>2</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></munder><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS3.p1" class="ltx_para">
<p class="ltx_p">There are <math id="A2.SS3.p1.m1" class="ltx_Math" alttext="(J-1)!/(J-M-1)!" display="inline"><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow><mo>/</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mrow></math> orderings on the first <math id="A2.SS3.p1.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> indices, given the <math id="A2.SS3.p1.m3" class="ltx_Math" alttext="(M+1)^{th}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math>. We introduce the following notation,</p>
<table id="A2.Ex35" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex35.m1" class="ltx_Math" alttext="\bar{P}_{-i}:=\frac{(J-M-1)!}{(J-1)!}\sum_{\pi|\pi_{M+1}=i}p(W|\pi)\hskip 56.9%
055pt\bar{P}:=\frac{1}{J}\sum_{i=1}^{J}\bar{P}_{-i}" display="block"><mrow><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo lspace="0.278em" rspace="0.278em">:=</mo><mrow><mfrac><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mrow><mi>π</mi><mo fence="false">|</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mi>i</mi></mrow></munder><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>W</mi><mo fence="false">|</mo><mi>π</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mspace width="5.69em"></mspace><mrow><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mo lspace="0.278em" rspace="0.278em">:=</mo><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">As in previous proofs, we notice that we can write,</p>
<table id="A2.Ex36" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex36.m1" class="ltx_Math" alttext="\bar{P}=\frac{1}{J}\bar{P}_{-i}+\frac{J-1}{J}\frac{1}{J-1}\sum_{j\neq i}\bar{P%
}_{-j}" display="block"><mrow><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><mrow><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac><mo>⁢</mo><mfrac><mn>1</mn><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">First, note that we can upper bound <math id="A2.SS3.p1.m4" class="ltx_Math" alttext="I(\pi_{M+1};W)\leq nI(\pi_{M+1};w)" display="inline"><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>n</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, where <math id="A2.SS3.p1.m5" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> denotes a single row in <math id="A2.SS3.p1.m6" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>. Further,</p>
<table id="A4.EGx11" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex37"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex37.m1" class="ltx_Math" alttext="\displaystyle I(\pi_{M+1};w)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex37.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{J}\sum_{i=1}^{J}\mathbb{E}\log\frac{\bar{P}_{-i}}{\bar{%
P}}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mi>𝔼</mi><mo lspace="0.167em">⁢</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover></mfrac></mstyle></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex38.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{J}\sum_{i=1}^{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}\|%
\frac{1}{J}\bar{P}_{-i}+\frac{J-1}{J}\frac{1}{J-1}\sum_{j\neq i}\bar{P}_{-j}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder></mstyle><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex39.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{J}\sum_{i=1}^{J}\frac{1}{J}D_{\mathrm{KL}}\left(\bar%
{P}_{-i}\|\bar{P}_{-i}\right)+\frac{J-1}{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}\|%
\frac{1}{J-1}\sum_{j\neq i}\bar{P}_{-j}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder></mstyle><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex40"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex40.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{J(J-1)}\sum_{1\leq i\neq j\leq J}D_{\mathrm{KL}}%
\left(\bar{P}_{-i}\|\bar{P}_{-j}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We will use convexity of the KL divergence to upper bound this quantity. Each distribution <math id="A2.SS3.p1.m7" class="ltx_Math" alttext="\bar{P}_{-i}" display="inline"><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></math> is an average over a random selection of index orderings.</p>
</div>
<div id="A2.SS3.p2" class="ltx_para">
<p class="ltx_p">When applying convexity, all pairs of selections that exactly match will lead to a KL divergence of zero. There are the same number of these in each component of <math id="A2.SS3.p2.m1" class="ltx_Math" alttext="\bar{P}_{-i}" display="inline"><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></math>. Thus we care only about selections that contain either <math id="A2.SS3.p2.m2" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math> or <math id="A2.SS3.p2.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> such that matching pairs of distributions exactly is not possible. Further, we need only consider pairs of product distributions who differ only in a single, identical position.</p>
</div>
<div id="A2.SS3.p3" class="ltx_para">
<p class="ltx_p">Each of the above described pairs of distributions has KL divergence equal to <math id="A2.SS3.p3.m1" class="ltx_Math" alttext="D_{\mathrm{KL}}\left(P_{j}\|P_{i}\right)" display="inline"><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math>. We conclude by counting the total number of orderings producing such pairs. First, there are <math id="A2.SS3.p3.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> choices for the index of <math id="A2.SS3.p3.m3" class="ltx_Math" alttext="P_{j}" display="inline"><msub><mi>P</mi><mi>j</mi></msub></math> and <math id="A2.SS3.p3.m4" class="ltx_Math" alttext="P_{i}" display="inline"><msub><mi>P</mi><mi>i</mi></msub></math>. Then, there are <math id="A2.SS3.p3.m5" class="ltx_Math" alttext="(J-2)!/(J-M-1)!" display="inline"><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow><mo>/</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mrow></math> total orderings of the remaining <math id="A2.SS3.p3.m6" class="ltx_Math" alttext="M-1" display="inline"><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow></math> elements. Thus, we have,</p>
</div>
<div id="A2.SS3.p4" class="ltx_para">
<table id="A4.EGx12" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex41.m1" class="ltx_Math" alttext="\displaystyle I(\pi_{M+1};w)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex41.m2" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{J(J-1)}\sum_{1\leq i\neq j\leq J}D_{\mathrm{KL}}%
\left(\bar{P}_{-i}\|\bar{P}_{-j}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex42"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex42.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{J(J-1)}\frac{(J-M-1)!}{(J-1)!}\frac{M(J-2)!}{(J-M-1)%
!}\sum_{1\leq i\neq j\leq J}D_{\mathrm{KL}}\left(P_{j}\|P_{i}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo>⁢</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex43"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex43.m1" class="ltx_Math" alttext="\displaystyle=\frac{M}{J(J-1)^{2}}\sum_{1\leq i\neq j\leq J}D_{\mathrm{KL}}%
\left(P_{j}\|P_{i}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mi>M</mi><mrow><mi>J</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="A2.SS3.p5" class="ltx_para">
<p class="ltx_p">These results together provide an immediate proof of Corollary 1.</p>
</div>
<div id="A2.SS3.p6" class="ltx_para">
<p class="ltx_p">See <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS3.p7" class="ltx_para">
<p class="ltx_p">Putting together the results of Theorem 1, Lemma 3, and Lemma 4, and using the fact that <math id="A2.SS3.p7.m1" class="ltx_Math" alttext="D_{\mathrm{KL}}\left(P_{i}\|P_{j}\right)\leq\alpha" display="inline"><mrow><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>≤</mo><mi>α</mi></mrow></math>, the result follows immediately.
∎</p>
</div>
</div>
<div id="A2.SS3.p8" class="ltx_para">
<p class="ltx_p">See <a href="#Thmcorollary2" title="Corollary 2. ‣ A tighter bound on partially observed environments ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
</p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS3.p9" class="ltx_para">
<p class="ltx_p">This result follows as an application of the data processing inequality. Notice that <math id="A2.SS3.p9.m1" class="ltx_Math" alttext="\pi_{M+1}\rightarrow\pi_{1:M}\rightarrow W" display="inline"><mrow><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">→</mo><msub><mi>π</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">→</mo><mi>W</mi></mrow></math> forms a Markov chain. Thus,</p>
<table id="A2.Ex45" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex45.m1" class="ltx_Math" alttext="I(\pi_{M+1};W)\leq I(\pi_{M+1};\pi_{1:M})," display="block"><mrow><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msub><mi>π</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">by the data processing inequality. We can compute <math id="A2.SS3.p9.m2" class="ltx_Math" alttext="I(\pi_{M+1};\pi_{1:M})" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msub><mi>π</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math> in closed form:</p>
<table id="A2.Ex46" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex46.m1" class="ltx_Math" alttext="I(\pi_{M+1};\pi_{1:M})=\log\frac{J}{J-M}." display="block"><mrow><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msub><mi>π</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mi>J</mi><mrow><mi>J</mi><mo>−</mo><mi>M</mi></mrow></mfrac></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The proof is completed by plugging in the <em class="ltx_emph ltx_font_italic">i.i.d </em>local packing bound alongside the above.
∎</p>
</div>
</div>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Bounds using mixture distributions</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p class="ltx_p">In this section we introduce tools to lower bound the minimax risk when the meta-training set is sampled from a mixture over the meta-training tasks, <math id="A2.SS4.p1.m1" class="ltx_Math" alttext="\bar{P}_{1:M}=\frac{1}{M}\sum_{i=1}^{M}P_{i}" display="inline"><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></mrow></math>. We note first that Theorem <a href="#Thmtheorem1" title="Theorem 1 (Minimax novel task risk lower bound). ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> can be reproduced exactly when <math id="A2.SS4.p1.m2" class="ltx_Math" alttext="W\sim\bar{P}_{1:M}" display="inline"><mrow><mi>W</mi><mo>∼</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></math>. Thus, we need only provide a local packing bound for the mixture distribution. In Lemma <a href="#Thmlemma5" title="Lemma 5 (Leave-one-task-out mixture local packing). ‣ B.4 Bounds using mixture distributions ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we provide such a lower bound for the special case where <math id="A2.SS4.p1.m3" class="ltx_Math" alttext="M=J-1" display="inline"><mrow><mi>M</mi><mo>=</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mrow></math>, so that data is sampled from a mixture over the entire environment.</p>
</div>
<div id="Thmlemma5" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 5</span></span><span class="ltx_text ltx_font_bold"> </span>(Leave-one-task-out mixture local packing)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math id="Thmlemma5.p1.m1" class="ltx_Math" alttext="\mathcal{J}\subset\mathcal{P}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo mathvariant="normal">⊂</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></math> contain <math id="Thmlemma5.p1.m2" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> distinct distributions such that <math id="Thmlemma5.p1.m3" class="ltx_Math" alttext="\rho(\theta_{P},\theta_{P^{\prime}})~{}\geq~{}2\delta" display="inline"><mrow><mrow><mi>ρ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mi>θ</mi><mi>P</mi></msub><mo mathvariant="normal">,</mo><msub><mi>θ</mi><msup><mi>P</mi><mo mathvariant="normal">′</mo></msup></msub><mo mathvariant="normal" rspace="0.330em" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal" rspace="0.608em">≥</mo><mrow><mn mathvariant="normal">2</mn><mo mathvariant="italic">⁢</mo><mi>δ</mi></mrow></mrow></math> for all <math id="Thmlemma5.p1.m4" class="ltx_Math" alttext="P,P^{\prime}\in\mathcal{J}" display="inline"><mrow><mrow><mi>P</mi><mo mathvariant="normal">,</mo><msup><mi>P</mi><mo mathvariant="normal">′</mo></msup></mrow><mo mathvariant="normal">∈</mo><mi class="ltx_font_mathcaligraphic">𝒥</mi></mrow></math> and let <math id="Thmlemma5.p1.m5" class="ltx_Math" alttext="\bar{P}_{-i}=\frac{1}{J-1}\sum_{j\neq i}P_{j}" display="inline"><mrow><msub><mover accent="true"><mi>P</mi><mo mathvariant="normal">¯</mo></mover><mrow><mo mathvariant="normal">−</mo><mi>i</mi></mrow></msub><mo mathvariant="normal">=</mo><mrow><mfrac><mn mathvariant="normal">1</mn><mrow><mi>J</mi><mo mathvariant="normal">−</mo><mn mathvariant="normal">1</mn></mrow></mfrac><mo mathvariant="italic">⁢</mo><mrow><msub><mo mathvariant="normal">∑</mo><mrow><mi>j</mi><mo mathvariant="normal">≠</mo><mi>i</mi></mrow></msub><msub><mi>P</mi><mi>j</mi></msub></mrow></mrow></mrow></math>. Let <math id="Thmlemma5.p1.m6" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math> be a random ordering of the <math id="Thmlemma5.p1.m7" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> elements, and define <math id="Thmlemma5.p1.m8" class="ltx_Math" alttext="W|\pi" display="inline"><mrow><mi>W</mi><mo fence="false" mathvariant="normal">|</mo><mi>π</mi></mrow></math> to be a vector of <math id="Thmlemma5.p1.m9" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> <em class="ltx_emph ltx_font_upright">i.i.d </em>samples from <math id="Thmlemma5.p1.m10" class="ltx_Math" alttext="\bar{P}_{-\pi_{M+1}}" display="inline"><msub><mover accent="true"><mi>P</mi><mo mathvariant="normal">¯</mo></mover><mrow><mo mathvariant="normal">−</mo><msub><mi>π</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub></mrow></msub></math>. Then,</span></p>
<table id="A2.Ex47" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex47.m1" class="ltx_Math" alttext="I(\pi_{M+1};W)\leq\frac{1}{(J-1)J^{2}}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}%
\left(P_{i}\|P_{j}\right)." display="block"><mrow><mrow><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mi>J</mi><mn>2</mn></msup></mrow></mfrac><mo>⁢</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></munder><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS4.p2" class="ltx_para">
<p class="ltx_p">From Lemma <a href="#Thmlemma3" title="Lemma 3. ‣ Appendix B Lower Bound Proofs ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (and some simple arithmetic) we have,</p>
<table id="A4.EGx13" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex48"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex48.m1" class="ltx_Math" alttext="\displaystyle I(\pi_{M+1};W)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex48.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{J}\sum_{i=1}^{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}\|\bar%
{P}_{1:J}\right)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>J</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that by the definition of the mixture distribution,</p>
<table id="A2.Ex49" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex49.m1" class="ltx_Math" alttext="\bar{P}_{1:J}=\frac{J-1}{J}\bar{P}_{-i}+\frac{1}{J}P_{i}." display="block"><mrow><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>J</mi></mrow></msub><mo>=</mo><mrow><mrow><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac><mo>⁢</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>J</mi></mfrac><mo>⁢</mo><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Using the convexity of the KL divergence,</p>
<table id="A4.EGx14" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex50"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex50.m1" class="ltx_Math" alttext="\displaystyle I(\pi_{M+1};W)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex50.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{J}\sum_{i=1}^{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}\Big{%
\|}\frac{J-1}{J}\bar{P}_{-i}+\frac{1}{J}P_{i}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo mathsize="160%">∥</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex51"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex51.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{J}\sum_{i=1}^{J}\frac{J-1}{J}D_{\mathrm{KL}}\left(%
\bar{P}_{-i}\|\bar{P}_{-i}\right)+\frac{1}{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}%
\|P_{i}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>J</mi></mfrac></mstyle><mo>⁢</mo><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex52"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex52.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{J^{2}}\sum_{i=1}^{J}D_{\mathrm{KL}}\left(\bar{P}_{-i}\|%
P_{i}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>J</mi><mn>2</mn></msup></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mover accent="true"><mi>P</mi><mo>¯</mo></mover><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex53"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex53.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{J^{2}}\sum_{i=1}^{J}D_{\mathrm{KL}}\left(\frac{1}{J-1}%
\sum_{1\leq j\leq J,j\neq i}P_{j}\Big{\|}P_{i}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>J</mi><mn>2</mn></msup></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></mrow></munder></mstyle><msub><mi>P</mi><mi>j</mi></msub></mrow></mrow><mo mathsize="160%">∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex54"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex54.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{(J-1)J^{2}}\sum_{i=1}^{J}\sum_{1\leq j\leq J,j\neq i%
}D_{\mathrm{KL}}\left(P_{j}\|P_{i}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mi>J</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>J</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex55"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex55.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{(J-1)J^{2}}\sum_{1\leq i,j\leq J}D_{\mathrm{KL}}\left(P%
_{i}\|P_{j}\right)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>J</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mi>J</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>≤</mo><mi>J</mi></mrow></mrow></munder></mstyle><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Noting for the last step that the KL is zero if and only if the distributions are the same almost everywhere.
∎</p>
</div>
</div>
</section>
<section id="A2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Proof of Hierarchical linear model lower bound</h3>

<div id="A2.SS5.p1" class="ltx_para">
<p class="ltx_p">Recall that the space of distributions we consider is given by,</p>
<table id="A2.Ex56" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex56.m1" class="ltx_math_unparsed" alttext="\mathcal{P}_{LR}=\{p_{\bm{\theta}}(\mathbf{y})=\mathcal{N}(X\bm{\theta},\sigma%
^{2}I):\bm{\theta}\in\mathbb{B}_{2}(1),X\in\mathbb{R}^{n\times d}.\}" display="block"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>p</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mrow><mo stretchy="false">(</mo><mi>X</mi><mi>𝜽</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow><mo rspace="0.278em">:</mo><mi>𝜽</mi><mo>∈</mo><msub><mi>𝔹</mi><mn>2</mn></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>X</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS5.p2" class="ltx_para">
<p class="ltx_p">See <a href="#Thmtheorem3" title="Theorem 3 (Meta linear regression lower bound). ‣ 5.1 Minimax lower bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS5.p3" class="ltx_para">
<p class="ltx_p">The proof consists of two steps, we first construct a <math id="A2.SS5.p3.m1" class="ltx_Math" alttext="2\delta" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></math>-packing of <math id="A2.SS5.p3.m2" class="ltx_Math" alttext="\mathcal{P}_{LR}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub></math>. Then, we upper bound the KL divergence between two distributions in this packing and use Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to give the desired bound.</p>
</div>
<div id="A2.SS5.p4" class="ltx_para">
<p class="ltx_p">The maximal packing number <math id="A2.SS5.p4.m1" class="ltx_Math" alttext="J" display="inline"><mi>J</mi></math> for the unit 2-norm ball can be bounded by the following,</p>
<table id="A2.Ex57" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex57.m1" class="ltx_Math" alttext="\left(\frac{1}{\delta}\right)^{d}\leq J\leq\left(1+\frac{2}{\delta}\right)^{d}." display="block"><mrow><mrow><msup><mrow><mo>(</mo><mfrac><mn>1</mn><mi>δ</mi></mfrac><mo>)</mo></mrow><mi>d</mi></msup><mo>≤</mo><mi>J</mi><mo>≤</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>2</mn><mi>δ</mi></mfrac></mrow><mo>)</mo></mrow><mi>d</mi></msup></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We use a common scaling trick. First, through this bound, we can build a packing set, <math id="A2.SS5.p4.m2" class="ltx_Math" alttext="\mathcal{V}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒱</mi></math>, with packing radius <math id="A2.SS5.p4.m3" class="ltx_Math" alttext="1/2" display="inline"><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></math>, giving <math id="A2.SS5.p4.m4" class="ltx_Math" alttext="2^{d}\leq J\leq 5^{d}" display="inline"><mrow><msup><mn>2</mn><mi>d</mi></msup><mo>≤</mo><mi>J</mi><mo>≤</mo><msup><mn>5</mn><mi>d</mi></msup></mrow></math>. We define a new packing set of the same cardinality by taking <math id="A2.SS5.p4.m5" class="ltx_Math" alttext="\theta_{i}=4\delta v_{i}" display="inline"><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>4</mn><mo>⁢</mo><mi>δ</mi><mo>⁢</mo><msub><mi>v</mi><mi>i</mi></msub></mrow></mrow></math> for all <math id="A2.SS5.p4.m6" class="ltx_Math" alttext="v_{i}\in\mathcal{V}" display="inline"><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒱</mi></mrow></math> (requiring <math id="A2.SS5.p4.m7" class="ltx_Math" alttext="\delta\leq 1/2" display="inline"><mrow><mi>δ</mi><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></math>). Giving for all <math id="A2.SS5.p4.m8" class="ltx_Math" alttext="i\neq j" display="inline"><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></math>,</p>
<table id="A2.Ex58" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex58.m1" class="ltx_Math" alttext="\|\theta_{i}-\theta_{j}\|=4\delta\|v_{i}-v_{j}\|\geq 2\delta" display="block"><mrow><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo>=</mo><mrow><mn>4</mn><mo>⁢</mo><mi>δ</mi><mo>⁢</mo><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><msub><mi>v</mi><mi>j</mi></msub></mrow><mo stretchy="false">‖</mo></mrow></mrow><mo>≥</mo><mrow><mn>2</mn><mo>⁢</mo><mi>δ</mi></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">similarly, <math id="A2.SS5.p4.m9" class="ltx_Math" alttext="\|\theta_{i}-\theta_{j}\|\leq 4\delta" display="inline"><mrow><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo>≤</mo><mrow><mn>4</mn><mo>⁢</mo><mi>δ</mi></mrow></mrow></math>.</p>
</div>
<div id="A2.SS5.p5" class="ltx_para">
<p class="ltx_p">We now proceed with bounding the KL divergences.</p>
<table id="A4.EGx15" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex59"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex59.m1" class="ltx_Math" alttext="\displaystyle D_{\mathrm{KL}}\left(P_{i}\|P_{j}\right)" display="inline"><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex59.m2" class="ltx_Math" alttext="\displaystyle=\frac{1}{2\sigma^{2}}\|X_{i}\bm{\theta}_{i}-X_{j}\bm{\theta}_{j}%
\|_{2}^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>i</mi></msub></mrow><mo>−</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>j</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex60"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex60.m1" class="ltx_Math" alttext="\displaystyle=\frac{1}{2\sigma^{2}}\left(\bm{\theta}_{i}^{\top}X_{i}^{\top}X_{%
i}\bm{\theta}_{i}+\bm{\theta}_{j}^{\top}X_{j}^{\top}X_{j}\bm{\theta}_{j}-2\bm{%
\theta}_{i}^{\top}X_{i}^{\top}X_{j}\bm{\theta}_{j}\right)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mrow><msubsup><mi>𝜽</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>𝜽</mi><mi>j</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mi>j</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>j</mi></msub></mrow></mrow><mo>−</mo><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>𝜽</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex61"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex61.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{2\sigma^{2}}\left(n_{i}\gamma_{i}^{2}\|\bm{\theta}_{%
i}\|^{2}+n_{j}\gamma_{j}^{2}\|\bm{\theta}_{j}\|^{2}-2\bm{\theta}_{i}^{\top}X_{%
i}^{\top}X_{j}\bm{\theta}_{j}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>⁢</mo><msubsup><mi>γ</mi><mi>i</mi><mn>2</mn></msubsup><mo>⁢</mo><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><msub><mi>n</mi><mi>j</mi></msub><mo>⁢</mo><msubsup><mi>γ</mi><mi>j</mi><mn>2</mn></msubsup><mo>⁢</mo><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>−</mo><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>𝜽</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="A2.SS5.p5.m1" class="ltx_Math" alttext="\gamma_{i}^{2}=\sup_{\bm{\theta}}\frac{\|X_{i}\bm{\theta}\|}{\sqrt{n_{i}}\|\bm%
{\theta}\|}" display="inline"><mrow><msubsup><mi>γ</mi><mi>i</mi><mn>2</mn></msubsup><mo rspace="0.1389em">=</mo><mrow><msub><mo lspace="0.1389em" rspace="0.167em">sup</mo><mi>𝜽</mi></msub><mfrac><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><mi>𝜽</mi></mrow><mo stretchy="false">‖</mo></mrow><mrow><msqrt><msub><mi>n</mi><mi>i</mi></msub></msqrt><mo>⁢</mo><mrow><mo stretchy="false">‖</mo><mi>𝜽</mi><mo stretchy="false">‖</mo></mrow></mrow></mfrac></mrow></mrow></math>. We write <math id="A2.SS5.p5.m2" class="ltx_Math" alttext="n=\max_{k}{n_{k}}" display="inline"><mrow><mi>n</mi><mo>=</mo><mrow><msub><mi>max</mi><mi>k</mi></msub><mo lspace="0.167em">⁡</mo><msub><mi>n</mi><mi>k</mi></msub></mrow></mrow></math> and <math id="A2.SS5.p5.m3" class="ltx_Math" alttext="\gamma=\max_{k}{\gamma_{k}}" display="inline"><mrow><mi>γ</mi><mo>=</mo><mrow><msub><mi>max</mi><mi>k</mi></msub><mo lspace="0.167em">⁡</mo><msub><mi>γ</mi><mi>k</mi></msub></mrow></mrow></math>, then,</p>
<table id="A4.EGx16" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex62"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.Ex62.m1" class="ltx_Math" alttext="\displaystyle D_{\mathrm{KL}}\left(P_{i}\|P_{j}\right)" display="inline"><mrow><msub><mi>D</mi><mi>KL</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex62.m2" class="ltx_Math" alttext="\displaystyle\leq\frac{n\gamma^{2}}{2\sigma^{2}}\left(\|\bm{\theta}_{i}\|^{2}+%
\|\bm{\theta}_{j}\|^{2}-\frac{2}{n\gamma^{2}}\bm{\theta}_{i}^{\top}X_{i}^{\top%
}X_{j}\bm{\theta}_{j}\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mrow><mi>n</mi><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><msubsup><mi>𝜽</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>𝜽</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex63"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex63.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{n\gamma^{2}}{2\sigma^{2}}\left(\|\bm{\theta}_{i}\|^{2}+%
\|\bm{\theta}_{j}\|^{2}+2\|\bm{\theta}_{i}\|\|\bm{\theta}_{j}\|\right)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex64"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A2.Ex64.m1" class="ltx_Math" alttext="\displaystyle=\frac{n\gamma^{2}}{2\sigma^{2}}\left(\|\bm{\theta}_{i}\|+\|\bm{%
\theta}_{j}\|\right)^{2}\leq\frac{32n\gamma^{2}\delta^{2}}{\sigma^{2}}\leq\beta" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>⁢</mo><msup><mrow><mo>(</mo><mrow><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mo>+</mo><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>≤</mo><mstyle displaystyle="true"><mfrac><mrow><mn>32</mn><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup><mo>⁢</mo><msup><mi>δ</mi><mn>2</mn></msup></mrow><msup><mi>σ</mi><mn>2</mn></msup></mfrac></mstyle><mo>≤</mo><mi>β</mi></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The second line is derived using the Cauchy-Schwarz inequality, and the final inequality uses <math id="A2.SS5.p5.m4" class="ltx_Math" alttext="\|\bm{\theta}_{i}\|=\|4\delta v_{i}\|\leq 4\delta" display="inline"><mrow><mrow><mo stretchy="false">‖</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mo>=</mo><mrow><mo stretchy="false">‖</mo><mrow><mn>4</mn><mo>⁢</mo><mi>δ</mi><mo>⁢</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo>≤</mo><mrow><mn>4</mn><mo>⁢</mo><mi>δ</mi></mrow></mrow></math>. We will not proceed by invoking Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> on this packing set. This will require choosing <math id="A2.SS5.p5.m5" class="ltx_Math" alttext="\delta" display="inline"><mi>δ</mi></math> to achieve our desired rate and will in turn impose constraints on the problem dimensions to ensure the packing is valid.</p>
</div>
<div id="A2.SS5.p6" class="ltx_para">
<p class="ltx_p">Now, using Corollary <a href="#Thmcorollary1" title="Corollary 1. ‣ 4 Information theoretic lower bounds on novel task generalization ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,</p>
<table id="A2.Ex65" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex65.m1" class="ltx_Math" alttext="R^{*}_{\mathcal{P}_{LR}}\geq\delta^{2}\left(1-\frac{(nM2^{-d}+k)32\gamma^{2}%
\delta^{2}/\sigma^{2}+1}{d}\right)," display="block"><mrow><mrow><msubsup><mi>R</mi><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi></mrow></msub><mo>*</mo></msubsup><mo>≥</mo><mrow><msup><mi>δ</mi><mn>2</mn></msup><mo>⁢</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>n</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mn>32</mn><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup><mo>⁢</mo><msup><mi>δ</mi><mn>2</mn></msup></mrow><mo>/</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo>+</mo><mn>1</mn></mrow><mi>d</mi></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Choosing <math id="A2.SS5.p6.m1" class="ltx_Math" alttext="\delta^{2}=d\sigma^{2}/\left[128\gamma^{2}(2^{-d}Mn+k)\right]" display="inline"><mrow><msup><mi>δ</mi><mn>2</mn></msup><mo>=</mo><mrow><mrow><mi>d</mi><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo>/</mo><mrow><mo>[</mo><mrow><mn>128</mn><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math> gives,</p>
<table id="A2.Ex66" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex66.m1" class="ltx_Math" alttext="1-\frac{(nM2^{-d}+k)32\delta^{2}/\sigma^{2}+1}{d}=1-\frac{d/4+1}{d}\geq 1/4," display="block"><mrow><mrow><mrow><mn>1</mn><mo>−</mo><mfrac><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>n</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mn>32</mn><mo>⁢</mo><msup><mi>δ</mi><mn>2</mn></msup></mrow><mo>/</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo>+</mo><mn>1</mn></mrow><mi>d</mi></mfrac></mrow><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mfrac><mrow><mrow><mi>d</mi><mo>/</mo><mn>4</mn></mrow><mo>+</mo><mn>1</mn></mrow><mi>d</mi></mfrac></mrow><mo>≥</mo><mrow><mn>1</mn><mo>/</mo><mn>4</mn></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">for <math id="A2.SS5.p6.m2" class="ltx_Math" alttext="d\geq 2" display="inline"><mrow><mi>d</mi><mo>≥</mo><mn>2</mn></mrow></math>. To enforce <math id="A2.SS5.p6.m3" class="ltx_Math" alttext="\delta\leq 1/2" display="inline"><mrow><mi>δ</mi><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></math>, we further require that,</p>
<table id="A2.Ex67" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex67.m1" class="ltx_Math" alttext="2^{-d}Mn+k\geq\frac{d\sigma^{2}}{256\gamma^{2}}" display="block"><mrow><mrow><mrow><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mo>+</mo><mi>k</mi></mrow><mo>≥</mo><mfrac><mrow><mi>d</mi><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><mn>256</mn><mo>⁢</mo><msup><mi>γ</mi><mn>2</mn></msup></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Additionally, we may only consider packing sets with KL divergence no more than <math id="A2.SS5.p6.m4" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>, hence we also require that,</p>
<table id="A2.Ex68" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex68.m1" class="ltx_Math" alttext="2^{-d}M+kn^{-1}\geq\frac{d}{4\beta}" display="block"><mrow><mrow><mrow><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup><mo>⁢</mo><mi>M</mi></mrow><mo>+</mo><mrow><mi>k</mi><mo>⁢</mo><msup><mi>n</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>≥</mo><mfrac><mi>d</mi><mrow><mn>4</mn><mo>⁢</mo><mi>β</mi></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus,</p>
<table id="A2.Ex69" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.Ex69.m1" class="ltx_Math" alttext="R^{*}_{\mathcal{P}}\geq O\left(\frac{d\sigma^{2}}{\gamma^{2}(2^{-d}nM+k)}\right)" display="block"><mrow><msubsup><mi>R</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>*</mo></msubsup><mo>≥</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mfrac><mrow><mi>d</mi><mo>⁢</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><msup><mi>γ</mi><mn>2</mn></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mn>2</mn><mrow><mo>−</mo><mi>d</mi></mrow></msup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>M</mi></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Hierarchical Bayesian Linear Regression Upper Bounds</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Some useful linear algebra results</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p class="ltx_p">Let <math id="A3.SS1.p1.m1" class="ltx_Math" alttext="{s_{\max}}(A)" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow></math> denotes the maximum singular value of <math id="A3.SS1.p1.m2" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>; <math id="A3.SS1.p1.m3" class="ltx_Math" alttext="{s_{\min}}(A)" display="inline"><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow></math> denotes the minimum singular value of <math id="A3.SS1.p1.m4" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>.</p>
</div>
<div id="Thmlemma6" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 6</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma6.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Singular value of sum of two matrices<span class="ltx_text ltx_font_medium">
Let <math id="Thmlemma6.p1.m1" class="ltx_Math" alttext="A,B\in\mathbb{R}^{m\times n}" display="inline"><mrow><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">,</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal">∈</mo><msup><mi>ℝ</mi><mrow><mi mathvariant="normal">m</mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em">×</mo><mi mathvariant="normal">n</mi></mrow></msup></mrow></math>, then
<math id="Thmlemma6.p1.m2" class="ltx_Math" alttext="{s_{\max}}(A)+{s_{\max}}(B)\geq{s_{\max}}(A+B)" display="inline"><mrow><mrow><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">A</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">+</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">B</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow><mo mathvariant="normal">≥</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">+</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>. Furthermore, if <math id="Thmlemma6.p1.m3" class="ltx_Math" alttext="A,B" display="inline"><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">,</mo><mi mathvariant="normal">B</mi></mrow></math> are positive definite,
<math id="Thmlemma6.p1.m4" class="ltx_Math" alttext="{s_{\min}}(A)+{s_{\min}}(B)\leq{s_{\min}}(A+B)" display="inline"><mrow><mrow><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">A</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">+</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">B</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow><mo mathvariant="normal">≤</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">+</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>.</span></span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS1.p2" class="ltx_para">
<p class="ltx_p">The first result follows immediately from the triangle inequality of the matrix norm <math id="A3.SS1.p2.m1" class="ltx_Math" alttext="\lVert\cdot\rVert_{2}" display="inline"><msub><mrow><mo fence="true" rspace="0em">∥</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msub></math>.</p>
</div>
<div id="A3.SS1.p3" class="ltx_para">
<p class="ltx_p">For the second result, suppose that <math id="A3.SS1.p3.m1" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> and <math id="A3.SS1.p3.m2" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math> are positive definite.</p>
<table id="A4.EGx17" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E3.m1" class="ltx_Math" alttext="\displaystyle{s_{\min}}(A+B)" display="inline"><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E3.m2" class="ltx_Math" alttext="\displaystyle=\inf_{\lVert u\rVert=1}\lVert(A+B)u\rVert" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0em">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="A3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E4.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{\inf_{\lVert u\rVert=1}\lVert(A+B)u\rVert^{2}}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="A3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E5.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{\inf_{\lVert u\rVert=1}\lVert Au\rVert^{2}+\lVert Bu\rVert%
^{2}+2\langle Au,Bu\rangle}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mrow><mo stretchy="false">⟨</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo>,</mo><mrow><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow><mo stretchy="false">⟩</mo></mrow></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="A3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E6.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{\inf_{\lVert u\rVert=1}\lVert Au\rVert^{2}+\lVert Bu\rVert%
^{2}+2u^{\top}A^{\top}Bu}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><msup><mi>u</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, notice that <math id="A3.SS1.p3.m3" class="ltx_Math" alttext="A^{\top}B" display="inline"><mrow><msup><mi>A</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>B</mi></mrow></math> is similar to the matrix <math id="A3.SS1.p3.m4" class="ltx_Math" alttext="A^{1/2}BA^{1/2}" display="inline"><mrow><msup><mi>A</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>⁢</mo><mi>B</mi><mo>⁢</mo><msup><mi>A</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></math>, which exists as <math id="A3.SS1.p3.m5" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> is positive definite. This matrix is itself positive definite, and thus has non-negative eigenvalues, meaning <math id="A3.SS1.p3.m6" class="ltx_Math" alttext="A^{\top}B" display="inline"><mrow><msup><mi>A</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>B</mi></mrow></math> also has all positive eigenvalues. Thus, <math id="A3.SS1.p3.m7" class="ltx_Math" alttext="u^{\top}A^{\top}Bu\geq 0" display="inline"><mrow><mrow><msup><mi>u</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow><mo>≥</mo><mn>0</mn></mrow></math>, for all <math id="A3.SS1.p3.m8" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math>, and,</p>
<table id="A4.EGx18" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E8.m1" class="ltx_Math" alttext="\displaystyle{s_{\min}}(A+B)" display="inline"><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E8.m2" class="ltx_Math" alttext="\displaystyle\geq\sqrt{\inf_{\lVert u\rVert=1}\lVert Au\rVert^{2}+\lVert Bu%
\rVert^{2}}" display="inline"><mrow><mi></mi><mo>≥</mo><msqrt><mrow><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="A3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E9.m1" class="ltx_Math" alttext="\displaystyle\geq\sqrt{\inf_{\lVert u\rVert=1}\lVert Au\rVert^{2}+\inf_{\lVert
v%
\rVert=1}\lVert Bv\rVert^{2}}" display="inline"><mrow><mi></mi><mo>≥</mo><msqrt><mrow><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><munder><mo lspace="0em" movablelimits="false" rspace="0em">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="A3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E10.m1" class="ltx_Math" alttext="\displaystyle\geq\sqrt{{s_{\min}}^{2}(A)+{s_{\min}}^{2}(B)}" display="inline"><mrow><mi></mi><mo>≥</mo><msqrt><mrow><mrow><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
<tbody id="A3.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E11.m1" class="ltx_Math" alttext="\displaystyle\geq{s_{\min}}(A)+{s_{\min}}(B)" display="inline"><mrow><mi></mi><mo>≥</mo><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E11.m2" class="ltx_Math" alttext="\displaystyle(\text{Concavity of}\ \ \sqrt{\cdot})" display="inline"><mrow><mo stretchy="false">(</mo><mtext>Concavity of</mtext><mspace width="1em"></mspace><msqrt><mo>⋅</mo></msqrt><mo stretchy="false">)</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="Thmlemma7" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 7</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma7.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Singular value of product of two matrices<span class="ltx_text ltx_font_medium">
Let <math id="Thmlemma7.p1.m1" class="ltx_Math" alttext="A,B\in\mathbb{C}^{n\times n}" display="inline"><mrow><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">,</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal">∈</mo><msup><mi>ℂ</mi><mrow><mi mathvariant="normal">n</mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em">×</mo><mi mathvariant="normal">n</mi></mrow></msup></mrow></math>, then
<math id="Thmlemma7.p1.m2" class="ltx_Math" alttext="{s_{\max}}(A){s_{\max}}(B)\geq{s_{\max}}(AB)" display="inline"><mrow><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">A</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow><mo mathvariant="normal">⁢</mo><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">B</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">≥</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">max</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">⁢</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>, and, <math id="Thmlemma7.p1.m3" class="ltx_Math" alttext="{s_{\min}}(A){s_{\min}}(B)\leq{s_{\min}}(AB)" display="inline"><mrow><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">A</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow><mo mathvariant="normal">⁢</mo><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mi mathvariant="normal">B</mi><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow><mo mathvariant="normal">≤</mo><mrow><msub><mi mathvariant="normal">s</mi><mi mathvariant="normal">min</mi></msub><mo mathvariant="normal">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mrow><mi mathvariant="normal">A</mi><mo mathvariant="normal">⁢</mo><mi mathvariant="normal">B</mi></mrow><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>.</span></span></p>
</div>
</div>
<div id="A3.SS1.p4" class="ltx_para">
<p class="ltx_p">First we prove the maximum singular value.</p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS1.p5" class="ltx_para">
<table id="A4.EGx19" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E12.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}(AB)" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E12.m2" class="ltx_Math" alttext="\displaystyle=\sup_{\lVert v\rVert=1}\sqrt{v^{*}B^{*}A^{*}ABv}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msqrt><mrow><msup><mi>v</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>B</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow></msqrt></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
<tbody id="A3.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E13.m1" class="ltx_Math" alttext="\displaystyle=\sup_{\lVert v\rVert=1}\sqrt{\lVert Bv\rVert^{2}u^{*}A^{*}Au}\ %
\ \text{for }u=\frac{Bv}{\lVert Bv\rVert}," display="inline"><mrow><mrow><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msqrt><mrow><msup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msup><mi>u</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow></msqrt></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><mtext>for </mtext><mo>⁢</mo><mi>u</mi></mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
<tbody id="A3.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E14.m1" class="ltx_Math" alttext="\displaystyle\leq\sup_{\lVert v\rVert=1,\lVert u\rVert=1}\sqrt{\lVert Bv\rVert%
^{2}u^{*}A^{*}Au}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">≤</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">sup</mo><mrow><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></mrow></munder><msqrt><mrow><msup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msup><mi>u</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow></msqrt></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
<tbody id="A3.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E15.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{\sup_{\lVert v\rVert=1}\lVert Bv\rVert^{2}\sup_{\lVert u%
\rVert=1}\lVert Au\rVert^{2}}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><munder><mo movablelimits="false">sup</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><mrow><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo lspace="0.167em">⁢</mo><mrow><munder><mo movablelimits="false" rspace="0em">sup</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
<tbody id="A3.E16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E16.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{{s_{\max}}^{2}(B){s_{\max}}^{2}(A)}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
<tbody id="A3.E17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E17.m1" class="ltx_Math" alttext="\displaystyle={s_{\max}}(A){s_{\max}}(B)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The minimum singular value follows a similar structure. Suppose <math id="A3.SS1.p5.m1" class="ltx_Math" alttext="AB" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow></math> is full rank,</p>
<table id="A4.EGx20" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E18.m1" class="ltx_Math" alttext="\displaystyle{s_{\min}}(AB)" display="inline"><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E18.m2" class="ltx_Math" alttext="\displaystyle=\inf_{\lVert v\rVert=1}\sqrt{v^{*}B^{*}A^{*}ABv}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msqrt><mrow><msup><mi>v</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>B</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow></msqrt></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
<tbody id="A3.E19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E19.m1" class="ltx_Math" alttext="\displaystyle=\inf_{\lVert v\rVert=1}\sqrt{\lVert Bv\rVert^{2}u^{*}A^{*}Au}\ %
\ \text{for }u=\frac{Bv}{\lVert Bv\rVert}," display="inline"><mrow><mrow><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msqrt><mrow><msup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msup><mi>u</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow></msqrt></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><mtext>for </mtext><mo>⁢</mo><mi>u</mi></mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(19)</span></td>
</tr></tbody>
<tbody id="A3.E20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E20.m1" class="ltx_Math" alttext="\displaystyle\geq\inf_{\lVert v\rVert=1,\lVert u\rVert=1}\sqrt{\lVert Bv\rVert%
^{2}u^{*}A^{*}Au}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">≥</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">inf</mo><mrow><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></mrow></munder><msqrt><mrow><msup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msup><mi>u</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi>A</mi><mo>*</mo></msup><mo>⁢</mo><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow></msqrt></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(20)</span></td>
</tr></tbody>
<tbody id="A3.E21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E21.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{\inf_{\lVert v\rVert=1}\lVert Bv\rVert^{2}\inf_{\lVert u%
\rVert=1}\lVert Au\rVert^{2}}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><munder><mo movablelimits="false">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>v</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><mrow><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>B</mi><mo>⁢</mo><mi>v</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup><mo lspace="0.167em">⁢</mo><mrow><munder><mo movablelimits="false" rspace="0em">inf</mo><mrow><mrow><mo fence="true" rspace="0em">∥</mo><mi>u</mi><mo fence="true" lspace="0em" rspace="0.1389em">∥</mo></mrow><mo lspace="0.1389em">=</mo><mn>1</mn></mrow></munder><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><mi>A</mi><mo>⁢</mo><mi>u</mi></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(21)</span></td>
</tr></tbody>
<tbody id="A3.E22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E22.m1" class="ltx_Math" alttext="\displaystyle=\sqrt{{s_{\min}}^{2}(B){s_{\min}}^{2}(A)}" display="inline"><mrow><mi></mi><mo>=</mo><msqrt><mrow><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow></msqrt></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(22)</span></td>
</tr></tbody>
<tbody id="A3.E23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E23.m1" class="ltx_Math" alttext="\displaystyle={s_{\min}}(A){s_{\min}}(B)." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If <math id="A3.SS1.p5.m2" class="ltx_Math" alttext="AB" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow></math> is not full rank, then <math id="A3.SS1.p5.m3" class="ltx_Math" alttext="{s_{\min}}(AB)={s_{\min}}(A){s_{\min}}(B)=0" display="inline"><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>.
∎</p>
</div>
</div>
<div id="Thmlemma8" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 8</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma8.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Von Neumann’s Trace Inequality</span> <cite class="ltx_cite ltx_citemacro_citep">(Von Neumann, <a href="#bib.bib9" title="Some matrix-inequalities and metrization of matric space" class="ltx_ref">1937</a>)</cite>)
Given two <math id="Thmlemma8.p1.m1" class="ltx_Math" alttext="n\times n" display="inline"><mrow><mi>n</mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em">×</mo><mi>n</mi></mrow></math> complex matrices <math id="Thmlemma8.p1.m2" class="ltx_Math" alttext="A,B" display="inline"><mrow><mi>A</mi><mo mathvariant="normal">,</mo><mi>B</mi></mrow></math>, with singular vales <math id="Thmlemma8.p1.m3" class="ltx_Math" alttext="a_{1}\geq\ldots\geq a_{n}" display="inline"><mrow><msub><mi>a</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">≥</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">≥</mo><msub><mi>a</mi><mi>n</mi></msub></mrow></math> and <math id="Thmlemma8.p1.m4" class="ltx_Math" alttext="b_{1}\geq\ldots\geq b_{n}" display="inline"><mrow><msub><mi>b</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">≥</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">≥</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></math> respectively. We have,</span></p>
<table id="A3.Ex70" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex70.m1" class="ltx_Math" alttext="\left|\mathrm{Tr}(AB)\right|\leq\sum_{i=1}^{n}a_{i}b_{i}" display="block"><mrow><mrow><mo>|</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>|</mo></mrow><mo rspace="0.111em">≤</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="A3.SS1.p6" class="ltx_para">
<p class="ltx_p">This is a classic result whose proof we exclude.</p>
</div>
<div id="A3.SS1.p7" class="ltx_para">
<p class="ltx_p">As a direct consequence of Lemma <a href="#Thmlemma8" title="Lemma 8. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <math id="A3.SS1.p7.m1" class="ltx_Math" alttext="\left|\mathrm{Tr}(AB)\right|\leq na_{1}b_{1}" display="inline"><mrow><mrow><mo>|</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>⁢</mo><mi>B</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mi>n</mi><mo>⁢</mo><msub><mi>a</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>b</mi><mn>1</mn></msub></mrow></mrow></math>.

</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Posterior estimate</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p class="ltx_p">For reference, we reproduce the posterior estimate for the true parameters <math id="A3.SS2.p1.m1" class="ltx_Math" alttext="\bm{\theta}_{M+1}" display="inline"><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></math>. As a shorthand, we write <math id="A3.SS2.p1.m2" class="ltx_Math" alttext="Y_{1:M+1}=(\mathbf{y}_{1},\ldots,\mathbf{y}_{M+1})" display="inline"><mrow><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math>.</p>
<table id="A4.EGx21" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E24.m1" class="ltx_Math" alttext="\displaystyle p(\bm{\tau}|Y_{1:M})" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝉</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E24.m2" class="ltx_Math" alttext="\displaystyle=\mathcal{N}(\mu_{\bm{\tau}|Y_{1:M}},\Sigma_{\bm{\tau}|Y_{1:M}})," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mrow><mi>𝝉</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>𝝉</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(24)</span></td>
</tr></tbody>
<tbody id="A3.E25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E25.m1" class="ltx_Math" alttext="\displaystyle\Sigma_{\tau|Y_{1:M}}^{-1}" display="inline"><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E25.m2" class="ltx_Math" alttext="\displaystyle=\sum^{M}_{i=1}X_{i}^{\top}(\sigma_{\theta}^{2}X_{i}X_{i}^{\top}+%
\sigma_{1}^{2}I)^{-1}X_{i}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(25)</span></td>
</tr></tbody>
<tbody id="A3.E26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E26.m1" class="ltx_Math" alttext="\displaystyle\mu_{\tau|Y_{1:M}}" display="inline"><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E26.m2" class="ltx_Math" alttext="\displaystyle=\Sigma_{\tau|Y_{1:M}}\sum^{M}_{i=1}X_{i}^{\top}(\sigma_{\theta}^%
{2}X_{i}X_{i}^{\top}+\sigma_{1}^{2}I)^{-1}\mathbf{y}_{i}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><msub><mi>𝐲</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(26)</span></td>
</tr></tbody>
</table>
<table id="A4.EGx22" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E27.m1" class="ltx_Math" alttext="\displaystyle p(\bm{\theta}_{M+1}|Y_{1:M},\mathbf{y}_{M+1})" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub><mo>,</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E27.m2" class="ltx_Math" alttext="\displaystyle=\mathcal{N}(\mu_{\bm{\theta}_{M+1}|Y_{1:M+1}},\Sigma_{\bm{\theta%
}_{M+1}|Y_{1:M+1}})," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(27)</span></td>
</tr></tbody>
<tbody id="A3.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E28.m1" class="ltx_Math" alttext="\displaystyle\Sigma_{\theta+\tau|Y_{1:M}}" display="inline"><msub><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E28.m2" class="ltx_Math" alttext="\displaystyle={\sigma_{\theta}^{2}I}+\Sigma_{\tau|Y_{1:M}}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo>+</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(28)</span></td>
</tr></tbody>
<tbody id="A3.E29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E29.m1" class="ltx_Math" alttext="\displaystyle\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}}^{-1}" display="inline"><msubsup><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E29.m2" class="ltx_Math" alttext="\displaystyle=\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}+\Sigma_{\theta+\tau|Y_{1:%
M}}^{-1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(29)</span></td>
</tr></tbody>
<tbody id="A3.E30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E30.m1" class="ltx_Math" alttext="\displaystyle\mu_{\bm{\theta}_{M+1}|Y_{1:M+1}}" display="inline"><msub><mi>μ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E30.m2" class="ltx_Math" alttext="\displaystyle=\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}}(\sigma_{M+1}^{-2}X_{M+1}^{%
\top}\mathbf{y}_{M+1}+\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\mu_{\tau|Y_{1:M}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(30)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Upper bound for meta linear regression</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p class="ltx_p">In this section we prove the main upper bound result of our paper, Theorem <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
See <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></p>
</div>
<div id="A3.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Before proceeding with the proof, we introduce some additional notation and technical results.
</p>
</div>
<section id="A3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Additional notation</h5>

<div id="A3.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">To alleviate (only a little of) the notational clutter, we will define the following quantities,</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i1.p1.m1" class="ltx_Math" alttext="{\Sigma^{\prime}}=\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}}" display="inline"><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>=</mo><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub></mrow></math></p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i2.p1.m1" class="ltx_Math" alttext="{\Sigma^{\prime}_{0}}=\Sigma_{\theta+\tau|Y_{1:M}}" display="inline"><mrow><msubsup><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo></msubsup><mo>=</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow></msub></mrow></math>.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i3.p1.m1" class="ltx_Math" alttext="{s_{\min}}(X/\sqrt{{n}})=s_{1}" display="inline"><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>/</mo><msqrt><mi>n</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>s</mi><mn>1</mn></msub></mrow></math></p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i4.p1.m1" class="ltx_Math" alttext="{s_{\min}}(X_{M+1}/\sqrt{{k}})=s_{2}" display="inline"><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>/</mo><msqrt><mi>k</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></math></p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i5.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i5.p1.m1" class="ltx_Math" alttext="{s_{\max}}(X/\sqrt{{n}})=\gamma_{1}=\kappa s_{1}" display="inline"><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>/</mo><msqrt><mi>n</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>γ</mi><mn>1</mn></msub><mo>=</mo><mrow><mi>κ</mi><mo>⁢</mo><msub><mi>s</mi><mn>1</mn></msub></mrow></mrow></math></p>
</div>
</li>
<li id="A3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i6.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i6.p1.m1" class="ltx_Math" alttext="{s_{\max}}(X_{M+1}/\sqrt{{k}})=\gamma_{2}=\kappa_{M+1}s_{2}" display="inline"><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>/</mo><msqrt><mi>k</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>γ</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></mrow></math></p>
</div>
</li>
<li id="A3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i7.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i7.p1.m1" class="ltx_Math" alttext="\alpha_{1}={\sigma_{1}^{2}}/{\sigma_{\theta}^{2}}" display="inline"><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>/</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow></mrow></math></p>
</div>
</li>
<li id="A3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i8.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i8.p1.m1" class="ltx_Math" alttext="\alpha_{2}={\sigma_{M+1}^{2}}/{\sigma_{\theta}^{2}}" display="inline"><mrow><msub><mi>α</mi><mn>2</mn></msub><mo>=</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>/</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow></mrow></math></p>
</div>
</li>
<li id="A3.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i9.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i9.p1.m1" class="ltx_Math" alttext="L=\frac{\alpha_{2}}{(M+\kappa^{2})s_{2}^{2}}" display="inline"><mrow><mi>L</mi><mo>=</mo><mfrac><msub><mi>α</mi><mn>2</mn></msub><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow></mfrac></mrow></math></p>
</div>
</li>
<li id="A3.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i10.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i10.p1.m1" class="ltx_Math" alttext="L_{1}=\frac{\alpha_{1}}{s_{1}^{2}s_{2}^{2}\kappa_{M+1}^{2}}" display="inline"><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>=</mo><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></math></p>
</div>
</li>
<li id="A3.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i11.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i11.p1.m1" class="ltx_Math" alttext="L_{2}=\frac{\tilde{\kappa}\kappa_{\tau}\alpha_{2}}{2s_{2}^{2}\kappa_{M+1}^{2}}" display="inline"><mrow><msub><mi>L</mi><mn>2</mn></msub><mo>=</mo><mfrac><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></math></p>
</div>
</li>
<li id="A3.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i12.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i12.p1.m1" class="ltx_Math" alttext="A=\frac{s_{2}^{2}\alpha_{1}}{s_{1}^{2}\alpha_{2}}" display="inline"><mrow><mi>A</mi><mo>=</mo><mfrac><mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>1</mn></msub></mrow><mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></math></p>
</div>
</li>
<li id="A3.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i13.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i13.p1.m1" class="ltx_Math" alttext="A_{1}=s_{2}^{2}\kappa_{M+1}^{2}" display="inline"><mrow><msub><mi>A</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mrow></math></p>
</div>
</li>
<li id="A3.I1.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i14.p1" class="ltx_para">
<p class="ltx_p"><math id="A3.I1.i14.p1.m1" class="ltx_Math" alttext="A_{2}=\frac{\alpha_{1}s_{2}^{2}\kappa_{M+1}^{2}}{\kappa_{\tau}^{2}s_{1}^{2}%
\alpha_{2}}" display="inline"><mrow><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></math></p>
</div>
</li>
</ul>
</div>
<div id="A3.SS3.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">As we have uniform bounds on the singular values of all design matrices, we introduced an auxillary matrix <math id="A3.SS3.SSS0.Px1.p2.m1" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> whose largest and smallest singular values are given by <math id="A3.SS3.SSS0.Px1.p2.m2" class="ltx_Math" alttext="\sqrt{n}\gamma_{1}" display="inline"><mrow><msqrt><mi>n</mi></msqrt><mo>⁢</mo><msub><mi>γ</mi><mn>1</mn></msub></mrow></math> and <math id="A3.SS3.SSS0.Px1.p2.m3" class="ltx_Math" alttext="\sqrt{n}s_{1}" display="inline"><mrow><msqrt><mi>n</mi></msqrt><mo>⁢</mo><msub><mi>s</mi><mn>1</mn></msub></mrow></math> respectively.</p>
</div>
<div id="A3.SS3.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">We will also write <math id="A3.SS3.SSS0.Px1.p3.m1" class="ltx_Math" alttext="S(A)=\mathop{\mathrm{Cov}}[A,A]" display="inline"><mrow><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" rspace="0em">Cov</mo><mrow><mo stretchy="false">[</mo><mi>A</mi><mo>,</mo><mi>A</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></math>, and <math id="A3.SS3.SSS0.Px1.p3.m2" class="ltx_Math" alttext="\kappa(A)={s_{\max}}(A)/{s_{\min}}(a)" display="inline"><mrow><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msub><mi>s</mi><mi>min</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math> throughout.</p>
</div>
</section>
<section id="A3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Bias-Variance Decomposition</h5>

<div id="A3.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">As is standard, we can decompose the risk into the bias and variance of the estimator:</p>
<table id="A4.EGx23" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E31.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}[\lVert\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1}\rVert^%
{2}]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><msup><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E31.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}[\mathrm{Tr}((\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1%
})(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})^{\top})]" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(31)</span></td>
</tr></tbody>
<tbody id="A3.E32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E32.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}(\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1%
})(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})^{\top}])" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(32)</span></td>
</tr></tbody>
<tbody id="A3.E33"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E33.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}(\mathop{\mathrm{Cov}}[\bm{\hat{\theta}}_{M+1},\bm{%
\hat{\theta}}_{M+1}])+\mathrm{Tr}(\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{%
\theta}_{M+1})]\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]^{\top})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mo lspace="0em" movablelimits="false" rspace="0em">Cov</mo><mrow><mo stretchy="false">[</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(33)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.SS3.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">In the next two sections, we will derive upper bounds on the bias and variance terms above.</p>
</div>
</section>
<section id="A3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.1 </span>Variance technical lemmas</h4>

<div id="A3.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">We first decompose the variance into contributions from two sources: the variance from data in the novel task and the variance from data in the source tasks.</p>
</div>
<div id="Thmlemma9" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 9</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma9.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Variance decomposition</span>)
Let <math id="Thmlemma9.p1.m1" class="ltx_Math" alttext="\bm{\hat{\theta}}_{M+1}=\mu_{\bm{\theta}_{M+1}|Y_{1:M+1}}" display="inline"><mrow><msub><mover accent="true"><mi>𝛉</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub><mo mathvariant="normal">=</mo><msub><mi>μ</mi><mrow><msub><mi>𝛉</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub><mo fence="false" mathvariant="normal">|</mo><msub><mi>Y</mi><mrow><mn mathvariant="normal">1</mn><mo lspace="0.278em" mathvariant="normal" rspace="0.278em">:</mo><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></mrow></msub></mrow></msub></mrow></math> as defined above. Then the variance of the estimator can be written as</span></p>
<table id="A3.Ex71" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex71.m1" class="ltx_Math" alttext="\mathrm{Tr}(S(\bm{\hat{\theta}}_{M+1}))=\mathrm{Tr}({\Sigma^{\prime}}\sigma_{M%
+1}^{-2}X_{M+1}^{\top}X_{M+1}{\Sigma^{\prime}})+\mathrm{Tr}(S({\Sigma^{\prime}%
}{{\Sigma^{\prime}_{0}}}^{-1}\mu_{\tau|Y_{1:M}}))" display="block"><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS3.SSS1.p2" class="ltx_para">
<table id="A4.EGx24" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E34"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E34.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Tr}(\mathop{\mathrm{Cov}}[\bm{\hat{\theta}}_{M+1},\bm{%
\hat{\theta}}_{M+1}])" display="inline"><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mo lspace="0em" movablelimits="false" rspace="0em">Cov</mo><mrow><mo stretchy="false">[</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E34.m2" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}(S({\Sigma^{\prime}}(\sigma_{M+1}^{-2}X_{M+1}^{\top}%
\mathbf{y}_{M+1}+{{\Sigma^{\prime}_{0}}}^{-1}\mu_{\tau|Y_{1:M}})))" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(34)</span></td>
</tr></tbody>
<tbody id="A3.E35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E35.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}(S({\Sigma^{\prime}}\sigma_{M+1}^{-2}X_{M+1}^{\top}%
\mathbf{y}_{M+1})+S({\Sigma^{\prime}}{{\Sigma^{\prime}_{0}}}^{-1}\mu_{\tau|Y_{%
1:M}}))" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(35)</span></td>
</tr></tbody>
<tbody id="A3.E36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E36.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}({\Sigma^{\prime}}\sigma_{M+1}^{-2}X_{M+1}^{\top}S(%
\mathbf{y}_{M+1})\sigma_{M+1}^{-2}X_{M+1}{\Sigma^{\prime}}+S({\Sigma^{\prime}}%
{{\Sigma^{\prime}_{0}}}^{-1}\mu_{\tau|Y_{1:M}}))" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo>+</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(36)</span></td>
</tr></tbody>
<tbody id="A3.E37"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E37.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}({\Sigma^{\prime}}\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M%
+1}{\Sigma^{\prime}})+\mathrm{Tr}(S({\Sigma^{\prime}}{{\Sigma^{\prime}_{0}}}^{%
-1}\mu_{\tau|Y_{1:M}}))" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(37)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="A3.SS3.SSS1.p3" class="ltx_para">
<p class="ltx_p">We will now work towards a bound for each of the two variance terms in Lemma <a href="#Thmlemma9" title="Lemma 9. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> separately. To do so, we will need to produce bounds on the singular values of terms appearing in Lemma <a href="#Thmlemma9" title="Lemma 9. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="A3.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">We begin with the covariance term <math id="A3.SS3.SSS1.p4.m1" class="ltx_Math" alttext="{\Sigma^{\prime}}" display="inline"><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></math>.</p>
</div>
<div id="Thmlemma10" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 10</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma10.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Novel task covariance singular value bound</span>)
Let <math id="Thmlemma10.p1.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>, <math id="Thmlemma10.p1.m2" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> and <math id="Thmlemma10.p1.m3" class="ltx_Math" alttext="s_{2}" display="inline"><msub><mi>s</mi><mn mathvariant="normal">2</mn></msub></math> be as defined above. Then,</span></p>
<table id="A3.Ex72" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex72.m1" class="ltx_Math" alttext="{s_{\max}}({\Sigma^{\prime}})\leq\frac{{\sigma_{M+1}^{2}}}{s_{2}^{2}}\left[{k}%
+\frac{{n}}{\frac{M{n}}{L}+A}\right]^{-1}." display="block"><mrow><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mfrac><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mfrac><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mi>n</mi><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mi>L</mi></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS3.SSS1.p5" class="ltx_para">
<p class="ltx_p">Using Lemma <a href="#Thmlemma6" title="Lemma 6. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we can bound <math id="A3.SS3.SSS1.p5.m1" class="ltx_Math" alttext="{s_{\max}}({\Sigma^{\prime}})" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math> as follows,</p>
<table id="A4.EGx25" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E38.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}({\Sigma^{\prime}})" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E38.m2" class="ltx_Math" alttext="\displaystyle={s_{\max}}(\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(38)</span></td>
</tr></tbody>
<tbody id="A3.E39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E39.m1" class="ltx_Math" alttext="\displaystyle={s_{\max}}((\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}+\Sigma_{%
\theta+\tau|Y_{1:M}}^{-1})^{-1})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(39)</span></td>
</tr></tbody>
<tbody id="A3.E40"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E40.m1" class="ltx_Math" alttext="\displaystyle=1/{s_{\min}}(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}+\Sigma_{%
\theta+\tau|Y_{1:M}}^{-1})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>s</mi><mi>min</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(40)</span></td>
</tr></tbody>
<tbody id="A3.E41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E41.m1" class="ltx_Math" alttext="\displaystyle\leq 1/({s_{\min}}(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1})+{s_{%
\min}}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}))" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(41)</span></td>
</tr></tbody>
<tbody id="A3.E42"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E42.m1" class="ltx_Math" alttext="\displaystyle=1/({s_{\min}}(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1})+1/{s_{\max%
}}(\Sigma_{\theta+\tau|Y_{1:M}}))" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>s</mi><mi>max</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(42)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, using the auxillary matrix <math id="A3.SS3.SSS1.p5.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>,</p>
<table id="A4.EGx26" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E43"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E43.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}({\Sigma^{\prime}})" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E43.m2" class="ltx_Math" alttext="\displaystyle\leq\left[{s_{\min}}(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1})+%
\frac{1}{{\sigma_{\theta}^{2}}+\frac{1}{M}{s_{\max}}((X^{\top}\tilde{C}^{-1}X)%
^{-1})}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(43)</span></td>
</tr></tbody>
<tbody id="A3.E44"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E44.m1" class="ltx_Math" alttext="\displaystyle=\left[\frac{{s_{\min}}(X_{M+1}^{\top}X_{M+1})}{{\sigma_{M+1}^{2}%
}}+\frac{1}{{\sigma_{\theta}^{2}}+\frac{1}{M}\frac{1}{{s_{\min}}(X^{\top}%
\tilde{C}^{-1}X)}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>=</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mn>1</mn><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(44)</span></td>
</tr></tbody>
<tbody id="A3.E45"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E45.m1" class="ltx_Math" alttext="\displaystyle\leq\left[\frac{{k}{s_{\min}}^{2}(X_{M+1}/\sqrt{{k}})}{{\sigma_{M%
+1}^{2}}}+\frac{1}{{\sigma_{\theta}^{2}}+\frac{1}{M}\frac{1}{{s_{\min}}(X^{%
\top}\tilde{C}^{-1}X)}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>/</mo><msqrt><mi>k</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mn>1</mn><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(45)</span></td>
</tr></tbody>
<tbody id="A3.E46"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E46.m1" class="ltx_Math" alttext="\displaystyle\leq\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{%
\sigma_{\theta}^{2}}+\frac{1}{M}\frac{1}{{s_{\min}}(X^{\top}(X{\sigma_{\theta}%
^{2}I}X^{\top}+\sigma^{2}I)^{-1}X)}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mn>1</mn><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>X</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(46)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.SS3.SSS1.p6" class="ltx_para">
<p class="ltx_p">Above we have used Lemma <a href="#Thmlemma6" title="Lemma 6. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> repeatedly, alongside the standard identity, <math id="A3.SS3.SSS1.p6.m1" class="ltx_Math" alttext="{s_{\max}}(A^{-1})={s_{\min}}(A)^{-1}" display="inline"><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow></math>. We continue now, additionally using Lemma <a href="#Thmlemma7" title="Lemma 7. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,</p>
<table id="A4.EGx27" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E47"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E47.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}({\Sigma^{\prime}})" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E47.m2" class="ltx_Math" alttext="\displaystyle\leq\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{%
\sigma_{\theta}^{2}}+\frac{1}{M}\frac{1}{{s_{\min}}(X^{\top}X){s_{\min}}((X{%
\sigma_{\theta}^{2}I}X^{\top}+\sigma_{1}^{2}I)^{-1})}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mn>1</mn><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>X</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(47)</span></td>
</tr></tbody>
<tbody id="A3.E48"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E48.m1" class="ltx_Math" alttext="\displaystyle=\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{\sigma_{%
\theta}^{2}}+\frac{{s_{\max}}(X{\sigma_{\theta}^{2}I}X^{\top}+\sigma_{1}^{2}I)%
}{{s_{\min}}(X^{\top}X)}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>=</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>X</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(48)</span></td>
</tr></tbody>
<tbody id="A3.E49"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E49.m1" class="ltx_Math" alttext="\displaystyle\leq\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{%
\sigma_{\theta}^{2}}+\frac{1}{M}\frac{{s_{\max}}(X{\sigma_{\theta}^{2}I}X^{%
\top})+{\sigma_{1}^{2}}}{{n}s_{1}^{2}}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(49)</span></td>
</tr></tbody>
<tbody id="A3.E50"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E50.m1" class="ltx_Math" alttext="\displaystyle\leq\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{%
\sigma_{\theta}^{2}}+\frac{{\sigma_{\theta}^{2}}{s_{\max}}(XX^{\top})+{\sigma_%
{1}^{2}}}{{n}s_{1}^{2}}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mfrac><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(50)</span></td>
</tr></tbody>
<tbody id="A3.E51"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E51.m1" class="ltx_Math" alttext="\displaystyle=\left[\frac{{k}s_{2}^{2}}{{\sigma_{M+1}^{2}}}+\frac{1}{{\sigma_{%
\theta}^{2}}+\frac{1}{M}\frac{{\sigma_{\theta}^{2}}{n}s_{1}^{2}\kappa^{2}+{%
\sigma_{1}^{2}}}{{n}s_{1}^{2}}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>=</mo><msup><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>⁢</mo><mfrac><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo>+</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(51)</span></td>
</tr></tbody>
<tbody id="A3.E52"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E52.m1" class="ltx_Math" alttext="\displaystyle={\sigma_{M+1}^{2}}\left[{k}s_{2}^{2}+\frac{M{n}}{\frac{{n}(M+%
\kappa^{2})}{\alpha_{2}}+\frac{\alpha_{1}}{s_{1}^{2}\alpha_{2}}}\right]^{-1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mrow><mi>k</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><msub><mi>α</mi><mn>2</mn></msub></mfrac><mo>+</mo><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(52)</span></td>
</tr></tbody>
<tbody id="A3.E53"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E53.m1" class="ltx_Math" alttext="\displaystyle=\frac{{\sigma_{M+1}^{2}}}{s_{2}^{2}}\left[{k}+\frac{{n}}{\frac{M%
{n}}{L}+A}\right]^{-1}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mstyle displaystyle="true"><mfrac><mi>n</mi><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mi>L</mi></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(53)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="A3.SS3.SSS1.p7" class="ltx_para">
<p class="ltx_p">Next, we deal with terms appearing corresponding to the data from the source tasks.</p>
</div>
<div id="Thmlemma11" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 11</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma11.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Source tasks covariance singular value bound</span>)
Let <math id="Thmlemma11.p1.m1" class="ltx_Math" alttext="\tilde{C}_{1}=\sigma_{\theta}^{2}XX^{\top}+\sigma_{1}^{2}I" display="inline"><mrow><msub><mover accent="true"><mi>C</mi><mo mathvariant="normal">~</mo></mover><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">=</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn mathvariant="normal">2</mn></msubsup><mo mathvariant="italic">⁢</mo><mi>X</mi><mo mathvariant="italic">⁢</mo><msup><mi>X</mi><mo mathvariant="normal">⊤</mo></msup></mrow><mo mathvariant="normal">+</mo><mrow><msubsup><mi>σ</mi><mn mathvariant="normal">1</mn><mn mathvariant="normal">2</mn></msubsup><mo mathvariant="italic">⁢</mo><mi>I</mi></mrow></mrow></mrow></math>, and write <math id="Thmlemma11.p1.m2" class="ltx_Math" alttext="\tilde{\kappa}=\kappa(\tilde{C}_{1})" display="inline"><mrow><mover accent="true"><mi>κ</mi><mo mathvariant="normal">~</mo></mover><mo mathvariant="normal">=</mo><mrow><mi>κ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo mathvariant="normal">~</mo></mover><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math> and <math id="Thmlemma11.p1.m3" class="ltx_Math" alttext="\kappa_{\tau}=\kappa(\Sigma_{\tau|Y_{1:M}})" display="inline"><mrow><msub><mi>κ</mi><mi>τ</mi></msub><mo mathvariant="normal">=</mo><mrow><mi>κ</mi><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false" mathvariant="normal">|</mo><msub><mi>Y</mi><mrow><mn mathvariant="normal">1</mn><mo lspace="0.278em" mathvariant="normal" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>. Then,</span></p>
<table id="A3.Ex73" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex73.m1" class="ltx_math_unparsed" alttext="{s_{\max}}^{2}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\Sigma_{\tau|Y_{1:M}})\leq%
\frac{1}{\frac{2M{\sigma_{\theta}^{2}}{n}s_{1}^{2}}{{s_{\max}}(\tilde{C}_{1})%
\kappa_{\tau}}+\frac{1}{\kappa_{\tau}^{2}}}=:D_{1}" display="block"><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mrow><mo stretchy="false">(</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>≤</mo><mfrac><mn>1</mn><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac><mo rspace="0em">=</mo><mo rspace="0.278em">:</mo><msub><mi>D</mi><mn>1</mn></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">and,</span></p>
<table id="A3.Ex74" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex74.m1" class="ltx_math_unparsed" alttext="{s_{\max}}(\sigma_{1}^{2}\tilde{C}_{1}^{-1})\leq\frac{1}{\frac{{n}s_{1}^{2}}{%
\alpha_{1}}+1}=:D_{2}" display="block"><mrow><msub><mi>s</mi><mi>max</mi></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>≤</mo><mfrac><mn>1</mn><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mn>1</mn></mrow></mfrac><mo rspace="0em">=</mo><mo rspace="0.278em">:</mo><msub><mi>D</mi><mn>2</mn></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS3.SSS1.p8" class="ltx_para">
<p class="ltx_p">Using Lemma <a href="#Thmlemma6" title="Lemma 6. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Lemma <a href="#Thmlemma7" title="Lemma 7. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we have,</p>
</div>
<div id="A3.SS3.SSS1.p9" class="ltx_para">
<table id="A4.EGx28" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E54"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E54.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}^{2}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\Sigma_{\tau|Y_{1%
:M}})" display="inline"><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E54.m2" class="ltx_Math" alttext="\displaystyle={s_{\max}}^{2}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\Sigma_{\tau|Y_{%
1:M}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(54)</span></td>
</tr></tbody>
<tbody id="A3.E55"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E55.m1" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}^{2}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}){s_{\max}}^{%
2}(\Sigma_{\tau|Y_{1:M}})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(55)</span></td>
</tr></tbody>
<tbody id="A3.E56"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E56.m1" class="ltx_Math" alttext="\displaystyle={s_{\min}}^{-2}(\Sigma_{\theta+\tau|Y_{1:M}}){s_{\max}}^{2}(%
\Sigma_{\tau|Y_{1:M}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mrow><mo>−</mo><mn>2</mn></mrow></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(56)</span></td>
</tr></tbody>
<tbody id="A3.E57"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E57.m1" class="ltx_Math" alttext="\displaystyle={s_{\min}}^{-2}({\sigma_{\theta}^{2}I}+\Sigma_{\tau|Y_{1:M}}){s_%
{\max}}^{2}(\Sigma_{\tau|Y_{1:M}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mmultiscripts><mi>s</mi><mi>min</mi><mrow></mrow><mrow></mrow><mrow><mo>−</mo><mn>2</mn></mrow></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo>+</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(57)</span></td>
</tr></tbody>
<tbody id="A3.E58"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E58.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{{s_{\max}}(\Sigma_{\tau|Y_{1:M}})^{2}}{({\sigma_{\theta%
}^{2}}+{s_{\min}}(\Sigma_{\tau|Y_{1:M}}))^{2}}" display="inline"><mrow><mi></mi><mo>≤</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(58)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, using <math id="A3.SS3.SSS1.p9.m1" class="ltx_Math" alttext="{\sigma_{\theta}^{2}}&gt;0" display="inline"><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>&gt;</mo><mn>0</mn></mrow></math>,</p>
<table id="A4.EGx29" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E59"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E59.m1" class="ltx_Math" alttext="\displaystyle\frac{{s_{\max}}(\Sigma_{\tau|Y_{1:M}})^{2}}{({\sigma_{\theta}^{2%
}}+{s_{\min}}(\Sigma_{\tau|Y_{1:M}}))^{2}}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E59.m2" class="ltx_Math" alttext="\displaystyle\leq\frac{{s_{\max}}(\Sigma_{\tau|Y_{1:M}})^{2}}{2{\sigma_{\theta%
}^{2}}{s_{\min}}(\Sigma_{\tau|Y_{1:M}})+{s_{\min}}(\Sigma_{\tau|Y_{1:M}})^{2}}" display="inline"><mrow><mi></mi><mo>≤</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(59)</span></td>
</tr></tbody>
<tbody id="A3.E60"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E60.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{1}{\frac{2{\sigma_{\theta}^{2}}}{{s_{\max}}(\Sigma_{%
\tau|Y_{1:M}})\kappa_{\tau}}+\frac{1}{\kappa_{\tau}^{2}}}" display="inline"><mrow><mi></mi><mo>≤</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(60)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Introducing the auxillary matrix <math id="A3.SS3.SSS1.p9.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and using Lemma <a href="#Thmlemma6" title="Lemma 6. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Lemma <a href="#Thmlemma7" title="Lemma 7. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> on <math id="A3.SS3.SSS1.p9.m3" class="ltx_Math" alttext="\Sigma_{\tau|Y_{1:M}}" display="inline"><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></math>, we have</p>
<table id="A4.EGx30" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E61"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E61.m1" class="ltx_Math" alttext="\displaystyle\frac{1}{\frac{2{\sigma_{\theta}^{2}}}{{s_{\max}}(\Sigma_{\tau|Y_%
{1:M}})\kappa_{\tau}}+\frac{1}{\kappa_{\tau}^{2}}}\leq\frac{1}{\frac{2M{\sigma%
_{\theta}^{2}}{s_{\min}}(X^{\top}\tilde{C}^{-1}X)}{\kappa_{\tau}}+\frac{1}{%
\kappa_{\tau}^{2}}}," display="inline"><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle><mo>≤</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><msub><mi>κ</mi><mi>τ</mi></msub></mfrac><mo>+</mo><mfrac><mn>1</mn><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(61)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where,</p>
<table id="A4.EGx31" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E62"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E62.m1" class="ltx_Math" alttext="\displaystyle{s_{\min}}(X^{\top}\tilde{C}_{1}^{-1}X)" display="inline"><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E62.m2" class="ltx_Math" alttext="\displaystyle\geq\frac{{s_{\min}}(X^{\top}X)}{{s_{\max}}(\tilde{C}_{1})}" display="inline"><mrow><mi></mi><mo>≥</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(62)</span></td>
</tr></tbody>
<tbody id="A3.E63"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E63.m1" class="ltx_Math" alttext="\displaystyle=\frac{{n}s_{1}^{2}}{{s_{\max}}(\tilde{C}_{1})}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(63)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This gives the first stated inequality,</p>
<table id="A4.EGx32" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E64"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E64.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}^{2}(\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\Sigma_{\tau|Y_{1%
:M}})" display="inline"><mrow><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E64.m2" class="ltx_math_unparsed" alttext="\displaystyle\leq\frac{1}{\frac{2M{\sigma_{\theta}^{2}}{n}s_{1}^{2}}{{s_{\max}%
}(\tilde{C}_{1})\kappa_{\tau}}+\frac{1}{\kappa_{\tau}^{2}}}=:D_{1}" display="inline"><mrow><mo>≤</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle><mo rspace="0em">=</mo><mo rspace="0.278em">:</mo><msub><mi>D</mi><mn>1</mn></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(64)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The second follows as,</p>
<table id="A4.EGx33" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E65"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E65.m1" class="ltx_Math" alttext="\displaystyle{s_{\max}}(\sigma_{1}^{2}\tilde{C}_{1}^{-1})" display="inline"><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E65.m2" class="ltx_Math" alttext="\displaystyle=\frac{{\sigma_{1}^{2}}}{{s_{\min}}(\sigma_{\theta}^{2}XX^{\top}+%
\sigma_{1}^{2}I)}" display="inline"><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(65)</span></td>
</tr></tbody>
<tbody id="A3.E66"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E66.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{{\sigma_{1}^{2}}}{{s_{\min}}(\sigma_{\theta}^{2}XX^{%
\top})+{s_{\min}}(\sigma_{1}^{2}I)}" display="inline"><mrow><mi></mi><mo>≤</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(66)</span></td>
</tr></tbody>
<tbody id="A3.E67"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E67.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{{\sigma_{1}^{2}}}{{n}s_{1}^{2}{\sigma_{\theta}^{2}}+{%
\sigma_{1}^{2}}}" display="inline"><mrow><mi></mi><mo>≤</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(67)</span></td>
</tr></tbody>
<tbody id="A3.E68"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E68.m1" class="ltx_math_unparsed" alttext="\displaystyle=\frac{1}{\frac{{n}s_{1}^{2}}{\alpha_{1}}+1}=:D_{2}" display="inline"><mrow><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle><mo rspace="0em">=</mo><mo rspace="0.278em">:</mo><msub><mi>D</mi><mn>2</mn></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(68)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="A3.SS3.SSS1.p10" class="ltx_para ltx_noindent">
<p class="ltx_p">In Lemma <a href="#Thmlemma11" title="Lemma 11. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, we introduced additional condition numbers, which we can bound as follows,</p>
<table id="A4.EGx34" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E69"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E69.m1" class="ltx_Math" alttext="\displaystyle\tilde{\kappa}" display="inline"><mover accent="true"><mi>κ</mi><mo>~</mo></mover></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E69.m2" class="ltx_Math" alttext="\displaystyle=\kappa(\tilde{C}_{1})=\kappa(\sigma_{\theta}^{2}XX^{\top}+\sigma%
_{1}^{2}I)\leq\kappa(\sigma_{\theta}^{2}XX^{\top})\leq\kappa(\sigma_{\theta}^{%
2}XX^{\top})\kappa({\sigma_{\theta}^{2}I})=\kappa^{2}," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(69)</span></td>
</tr></tbody>
<tbody id="A3.E70"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E70.m1" class="ltx_Math" alttext="\displaystyle\kappa_{\tau}" display="inline"><msub><mi>κ</mi><mi>τ</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E70.m2" class="ltx_Math" alttext="\displaystyle=\kappa(\Sigma_{\tau|Y_{1:M}})\leq\kappa(X^{\top}X)\kappa(\tilde{%
C}_{1})=\kappa^{2}\tilde{\kappa}\leq\kappa^{4}." display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>κ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>κ</mi><mn>2</mn></msup><mo>⁢</mo><mover accent="true"><mi>κ</mi><mo>~</mo></mover></mrow><mo>≤</mo><msup><mi>κ</mi><mn>4</mn></msup></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(70)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.2 </span>Variance upper bound</h4>

<div id="A3.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">We are now ready to put the above technical results together to achieve a bound on the variance of the estimator.</p>
</div>
<div id="Thmlemma12" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 12</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma12.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Variance bound</span>)</span></p>
<table id="A3.Ex75" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex75.m1" class="ltx_Math" alttext="\mathrm{Tr}(S(\bm{\hat{\theta}}_{M+1}))\leq\frac{\kappa_{M+1}^{2}\sigma_{M+1}^%
{2}}{s_{2}^{2}}d\left[{k}+\frac{{n}}{\frac{{n}}{L}+A}\right]^{-2}\left[{k}+%
\frac{M{n}}{(\frac{{n}}{L_{1}}+A_{1})(\frac{M{n}}{L_{2}}+A_{2})}\right]" display="block"><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mfrac><mrow><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mfrac><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mi>n</mi><mrow><mfrac><mi>n</mi><mi>L</mi></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo>⁢</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mfrac><mi>n</mi><msub><mi>L</mi><mn>1</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><msub><mi>L</mi><mn>2</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p">First, by Lemma <a href="#Thmlemma9" title="Lemma 9. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we can decompose the overall variance into two terms:</p>
<table id="A3.Ex76" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex76.m1" class="ltx_Math" alttext="\mathrm{Tr}(S(\bm{\hat{\theta}}_{M+1}))=\mathrm{Tr}({\Sigma^{\prime}}\sigma_{M%
+1}^{-2}X_{M+1}^{\top}X_{M+1}{\Sigma^{\prime}})+\mathrm{Tr}(S({\Sigma^{\prime}%
}{{\Sigma^{\prime}_{0}}}^{-1}\mu_{\tau|Y_{1:M}}))" display="block"><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We deal with the left term first.</p>
</div>
<div id="A3.SS3.SSS2.p3" class="ltx_para">
<p class="ltx_p">Using trace permutation invariance and the von Neumann trace inequality (Lemma <a href="#Thmlemma8" title="Lemma 8. ‣ C.1 Some useful linear algebra results ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). We can upper bound the left variance term as follows,</p>
<table id="A4.EGx35" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E71"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E71.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Tr}({\Sigma^{\prime}}\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+%
1}{\Sigma^{\prime}})" display="inline"><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E71.m2" class="ltx_Math" alttext="\displaystyle=\sigma_{M+1}^{-2}\mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}}X%
_{M+1}^{\top}X_{M+1})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(71)</span></td>
</tr></tbody>
<tbody id="A3.E72"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E72.m1" class="ltx_Math" alttext="\displaystyle\leq d{k}\sigma_{M+1}^{-2}{s_{\max}}({\Sigma^{\prime}})^{2}{s_{%
\max}}^{2}(X_{M+1}/\sqrt{{k}})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mi>d</mi><mo>⁢</mo><mi>k</mi><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>/</mo><msqrt><mi>k</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(72)</span></td>
</tr></tbody>
<tbody id="A3.E73"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E73.m1" class="ltx_Math" alttext="\displaystyle=d{k}\sigma_{M+1}^{-2}{s_{\max}}({\Sigma^{\prime}})^{2}s_{2}^{2}%
\kappa_{M+1}^{2}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>d</mi><mo>⁢</mo><mi>k</mi><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(73)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For the second variance term, we observe that,</p>
<table id="A4.EGx36" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E74"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E74.m1" class="ltx_Math" alttext="\displaystyle\ \ \ \ \ \mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}_{0}}^{-1}%
S(\mu_{\tau|Y_{1:M}}){\Sigma^{\prime}_{0}}^{-1}{\Sigma^{\prime}})" display="inline"><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(74)</span></td>
</tr></tbody>
<tbody id="A3.E75"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E75.m1" class="ltx_Math" alttext="\displaystyle=\mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}_{0}}^{-1}S(\Sigma_%
{\tau|Y_{1:M}}\sum^{M}_{i=1}X_{i}^{\top}(\sigma_{\theta}^{2}X_{i}X_{i}^{\top}+%
\sigma_{1}^{2}I)^{-1}\mathbf{y}_{i}){\Sigma^{\prime}_{0}}^{-1}{\Sigma^{\prime}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><msub><mi>𝐲</mi><mi>i</mi></msub></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(75)</span></td>
</tr></tbody>
<tbody id="A3.E76"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E76.m1" class="ltx_Math" alttext="\displaystyle\leq M\mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}_{0}}^{-1}%
\Sigma_{\tau|Y_{1:M}}X^{\top}\tilde{C}^{-1}S(y_{1})\tilde{C}^{-1}X\Sigma_{\tau%
|Y_{1:M}}{\Sigma^{\prime}_{0}}^{-1}{\Sigma^{\prime}})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mi>M</mi><mo>⁢</mo><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(76)</span></td>
</tr></tbody>
<tbody id="A3.E77"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E77.m1" class="ltx_Math" alttext="\displaystyle=M\mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}_{0}}^{-1}\Sigma_{%
\tau|Y_{1:M}}X^{\top}\tilde{C}_{1}^{-1}\sigma_{1}^{2}I\tilde{C}_{1}^{-1}X%
\Sigma_{\tau|Y_{1:M}}{\Sigma^{\prime}_{0}}^{-1}{\Sigma^{\prime}})" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mi>M</mi><mo>⁢</mo><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>I</mi><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi>X</mi><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(77)</span></td>
</tr></tbody>
<tbody id="A3.E78"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E78.m1" class="ltx_Math" alttext="\displaystyle\leq M{s_{\max}}({\Sigma^{\prime}})^{2}{s_{\max}}^{2}(\Sigma_{%
\theta+\tau|Y_{1:M}}^{-1}\Sigma_{\tau|Y_{1:M}}){\sigma_{1}^{2}}\mathrm{Tr}(X^{%
\top}\tilde{C}^{-1}\tilde{C}^{-1}X)" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mi>M</mi><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mmultiscripts><mi>s</mi><mi>max</mi><mrow></mrow><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(78)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Using Lemma <a href="#Thmlemma11" title="Lemma 11. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, we have,</p>
<table id="A4.EGx37" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E79"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E79.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Tr}({\Sigma^{\prime}}{\Sigma^{\prime}_{0}}^{-1}S(\mu_{%
\tau|Y_{1:M}}){\Sigma^{\prime}_{0}}^{-1}{\Sigma^{\prime}})" display="inline"><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mmultiscripts><mi mathvariant="normal">Σ</mi><mn>0</mn><mo>′</mo><mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></mmultiscripts><mo>⁢</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E79.m2" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}({\Sigma^{\prime}})^{2}MD_{1}D_{2}\mathrm{Tr}(X^{%
\top}X){s_{\max}}(\tilde{C}_{1}^{-1})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msub><mi>D</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>D</mi><mn>2</mn></msub><mo>⁢</mo><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(79)</span></td>
</tr></tbody>
<tbody id="A3.E80"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E80.m1" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}({\Sigma^{\prime}})^{2}MD_{1}D_{2}\min({n},d){n}{s_%
{\max}}(\tilde{C}_{1}^{-1})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msub><mi>D</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>D</mi><mn>2</mn></msub><mo lspace="0.167em">⁢</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(80)</span></td>
</tr></tbody>
<tbody id="A3.E81"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E81.m1" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}({\Sigma^{\prime}})^{2}D_{2}\frac{M\min({n},d){n}}{%
\frac{2M{\sigma_{\theta}^{2}}{n}s_{1}^{2}{s_{\min}}(\tilde{C}_{1})}{{s_{\max}}%
(\tilde{C}_{1})\kappa_{\tau}}+\frac{{s_{\min}}(\tilde{C}_{1})}{\kappa_{\tau}^{%
2}}}\ " display="inline"><mrow><mi></mi><mo>≤</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msub><mi>D</mi><mn>2</mn></msub><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo lspace="0.167em">⁢</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(81)</span></td>
</tr></tbody>
<tbody id="A3.E82"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E82.m1" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}({\Sigma^{\prime}})^{2}D_{2}\frac{M\min({n},d){n}}{%
\frac{2M{\sigma_{\theta}^{2}}{n}s_{1}^{2}{s_{\min}}(\tilde{C}_{1})}{{s_{\max}}%
(\tilde{C}_{1})\kappa_{\tau}}+\frac{{\sigma_{1}^{2}}}{\kappa_{\tau}^{2}}}\ " display="inline"><mrow><mi></mi><mo>≤</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msub><mi>D</mi><mn>2</mn></msub><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo lspace="0.167em">⁢</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>s</mi><mi>min</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub></mrow></mfrac><mo>+</mo><mfrac><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup></mfrac></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(82)</span></td>
</tr></tbody>
<tbody id="A3.E83"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E83.m1" class="ltx_Math" alttext="\displaystyle\leq{s_{\max}}({\Sigma^{\prime}})^{2}D_{2}\frac{\min({n},d){n}}{%
\sigma_{M+1}^{2}}\frac{M}{\frac{2M{n}}{\tilde{\kappa}\kappa_{\tau}\alpha_{2}}+%
\frac{\alpha_{1}}{\kappa_{\tau}^{2}{\sigma_{1}^{2}}^{2}\alpha_{2}}}\ " display="inline"><mrow><mi></mi><mo>≤</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msub><mi>D</mi><mn>2</mn></msub><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>n</mi></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mi>M</mi><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><mmultiscripts><mi>σ</mi><mn>1</mn><mn>2</mn><mrow></mrow><mn>2</mn></mmultiscripts><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(83)</span></td>
</tr></tbody>
<tbody id="A3.E84"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E84.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{{s_{\max}}({\Sigma^{\prime}})^{2}}{\sigma_{M+1}^{2}}%
\frac{{n}d}{\frac{{n}s_{1}^{2}}{\alpha_{1}}+1}\frac{M}{\frac{2M{n}}{\tilde{%
\kappa}\kappa_{\tau}\alpha_{2}}+\frac{\alpha_{1}}{\kappa_{\tau}^{2}s_{1}^{2}%
\alpha_{2}}}" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mi>M</mi><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(84)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, rearranging and using Lemma <a href="#Thmlemma10" title="Lemma 10. ‣ C.3.1 Variance technical lemmas ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we can bound the sum of the two terms in the variance as follows,</p>
<table id="A4.EGx38" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E85"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E85.m1" class="ltx_Math" alttext="\displaystyle\mathrm{Tr}(S(\bm{\hat{\theta}}_{M+1}))" display="inline"><mrow><mi>Tr</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E85.m2" class="ltx_Math" alttext="\displaystyle\leq\frac{{s_{\max}}({\Sigma^{\prime}})^{2}}{\sigma_{M+1}^{2}}({k%
}ds_{2}^{2}\kappa_{M+1}^{2}+\frac{{n}d}{\frac{{n}s_{1}^{2}}{\alpha_{1}}+1}%
\frac{M}{\frac{2M{n}}{\tilde{\kappa}\kappa_{\tau}\alpha_{2}}+\frac{\alpha_{1}}%
{\kappa_{\tau}^{2}s_{1}^{2}\alpha_{2}}})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>k</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mi>M</mi><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></mfrac></mstyle></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(85)</span></td>
</tr></tbody>
<tbody id="A3.E86"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E86.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{{s_{\max}}({\Sigma^{\prime}})^{2}s_{2}^{2}\kappa_{M+1}^%
{2}}{\sigma_{M+1}^{2}}({k}d+\frac{{n}d}{\frac{{n}s_{1}^{2}s_{2}^{2}\kappa_{M+1%
}^{2}}{\alpha_{1}}+s_{2}^{2}\kappa_{M+1}^{2}}\frac{M}{\frac{2M{n}s_{2}^{2}%
\kappa_{M+1}^{2}}{\tilde{\kappa}\kappa_{\tau}\alpha_{2}}+\frac{\alpha_{1}s_{2}%
^{2}\kappa_{M+1}^{2}}{\kappa_{\tau}^{2}s_{1}^{2}\alpha_{2}}})" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>k</mi><mo>⁢</mo><mi>d</mi></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><mi>M</mi><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow></mfrac></mstyle></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(86)</span></td>
</tr></tbody>
<tbody id="A3.E87"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E87.m1" class="ltx_math_unparsed" alttext="\displaystyle\leq\frac{\kappa_{M+1}^{2}\sigma_{M+1}^{2}}{s_{2}^{2}}\left[{k}+%
\frac{{n}}{\frac{{n}}{L}+A}\right]^{-2}\cdot" display="inline"><mrow><mo>≤</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mstyle><msup><mrow><mo>[</mo><mi>k</mi><mo>+</mo><mstyle displaystyle="true"><mfrac><mi>n</mi><mrow><mfrac><mi>n</mi><mi>L</mi></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mstyle><mo rspace="0.055em">]</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo>⋅</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(87)</span></td>
</tr></tbody>
<tbody id="A3.E88"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E88.m1" class="ltx_Math" alttext="\displaystyle\hskip 56.9055pt\left[{k}d+\frac{M{n}d}{(\frac{{n}s_{1}^{2}s_{2}^%
{2}\kappa_{M+1}^{2}}{\alpha_{1}}+s_{2}^{2}\kappa_{M+1}^{2})(\frac{2M{n}s_{2}^{%
2}\kappa_{M+1}^{2}}{\tilde{\kappa}\kappa_{\tau}\alpha_{2}}+\frac{\alpha_{1}s_{%
2}^{2}\kappa_{M+1}^{2}}{\kappa_{\tau}^{2}s_{1}^{2}\alpha_{2}})}\right]" display="inline"><mrow><mo>[</mo><mrow><mrow><mi>k</mi><mo>⁢</mo><mi>d</mi></mrow><mo>+</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>1</mn></msub></mfrac><mo>+</mo><mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mrow><mover accent="true"><mi>κ</mi><mo>~</mo></mover><mo>⁢</mo><msub><mi>κ</mi><mi>τ</mi></msub><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>κ</mi><mi>τ</mi><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>⁢</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mfrac></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(88)</span></td>
</tr></tbody>
<tbody id="A3.E89"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E89.m1" class="ltx_Math" alttext="\displaystyle\leq\frac{\kappa_{M+1}^{2}\sigma_{M+1}^{2}}{s_{2}^{2}}d\left[{k}+%
\frac{{n}}{\frac{{n}}{L}+A}\right]^{-2}\left[{k}+\frac{M{n}}{(\frac{{n}}{L_{1}%
}+A_{1})(\frac{M{n}}{L_{2}}+A_{2})}\right]" display="inline"><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>κ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mstyle><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mstyle displaystyle="true"><mfrac><mi>n</mi><mrow><mfrac><mi>n</mi><mi>L</mi></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo>⁢</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mstyle displaystyle="true"><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mfrac><mi>n</mi><msub><mi>L</mi><mn>1</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><msub><mi>L</mi><mn>2</mn></msub></mfrac><mo>+</mo><msub><mi>A</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(89)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
</section>
<section id="A3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.3 </span>Bounding the Bias</h4>

<div id="Thmlemma13" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 13</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma13.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">(<span class="ltx_text ltx_font_bold">Bias upper bound</span>)
Given <math id="Thmlemma13.p1.m1" class="ltx_Math" alttext="\theta_{1},\ldots,\bm{\theta}_{M+1}\in\mathbb{B}_{2}(1)" display="inline"><mrow><mrow><msub><mi>θ</mi><mn mathvariant="normal">1</mn></msub><mo mathvariant="normal">,</mo><mi mathvariant="normal">…</mi><mo mathvariant="normal">,</mo><msub><mi>𝛉</mi><mrow><mi>M</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow></msub></mrow><mo mathvariant="normal">∈</mo><mrow><msub><mi>𝔹</mi><mn mathvariant="normal">2</mn></msub><mo mathvariant="italic">⁢</mo><mrow><mo mathvariant="normal" stretchy="false">(</mo><mn mathvariant="normal">1</mn><mo mathvariant="normal" stretchy="false">)</mo></mrow></mrow></mrow></math>, we have,</span></p>
<table id="A3.Ex77" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex77.m1" class="ltx_Math" alttext="\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]\leq O\left(d\left[{k}+%
\frac{M{n}}{\frac{{n}(M+\kappa^{2})s_{2}^{2}}{\alpha_{2}}+A}\right]^{-2}\right)" display="block"><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>d</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>2</mn></msub></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A3.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p">The bias can be computed as follows,</p>
<table id="A4.EGx39" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.E90"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.E90.m1" class="ltx_Math" alttext="\displaystyle\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]" display="inline"><mrow><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E90.m2" class="ltx_Math" alttext="\displaystyle=\mathbb{E}\mu_{\bm{\theta}_{M+1}|Y_{1:M+1}}-\bm{\theta}_{M+1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(90)</span></td>
</tr></tbody>
<tbody id="A3.E91"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E91.m1" class="ltx_Math" alttext="\displaystyle=\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}}\mathbb{E}(\sigma_{M+1}^{-2}%
X_{M+1}^{\top}y_{2}+\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\mu_{\tau|Y_{1:M}})-\bm{%
\theta}_{M+1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>y</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(91)</span></td>
</tr></tbody>
<tbody id="A3.E92"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E92.m1" class="ltx_Math" alttext="\displaystyle=\Sigma_{\bm{\theta}_{M+1}|Y_{1:M+1}}(\sigma_{M+1}^{-2}X_{M+1}^{%
\top}X_{M+1}\bm{\theta}_{M+1}+\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\mathbb{E}\mu_{%
\tau|Y_{1:M}})-\bm{\theta}_{M+1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(92)</span></td>
</tr></tbody>
<tbody id="A3.E93"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E93.m1" class="ltx_math_unparsed" alttext="\displaystyle=(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}+\Sigma_{\theta+\tau|Y_{1%
:M}}^{-1})^{-1}\cdot" display="inline"><mrow><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo rspace="0.055em" stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⋅</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(93)</span></td>
</tr></tbody>
<tbody id="A3.E94"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E94.m1" class="ltx_Math" alttext="\displaystyle\hskip 56.9055pt(\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}\bm{\theta%
}_{M+1}+\Sigma_{\theta+\tau|Y_{1:M}}^{-1}\mathbb{E}\mu_{\tau|Y_{1:M}})-\bm{%
\theta}_{M+1}" display="inline"><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(94)</span></td>
</tr></tbody>
<tbody id="A3.E95"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E95.m1" class="ltx_Math" alttext="\displaystyle=(F+G)^{-1}(F\bm{\theta}_{M+1}+G\mu_{\tau|Y_{1:M}})-\bm{\theta}_{%
M+1}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>F</mi><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>G</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(95)</span></td>
</tr></tbody>
<tbody id="A3.E96"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E96.m1" class="ltx_Math" alttext="\displaystyle=(F+G)^{-1}F\bm{\theta}_{M+1}-(F+G)^{-1}(F+G)\bm{\theta}_{M+1}+(F%
+G)^{-1}G\mathbb{E}\mu_{\tau|Y_{1:M}}" display="inline"><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>F</mi><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>−</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>G</mi><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(96)</span></td>
</tr></tbody>
<tbody id="A3.E97"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.E97.m1" class="ltx_Math" alttext="\displaystyle=(F+G)^{-1}G(\mathbb{E}\mu_{\tau|Y_{1:M}}-\bm{\theta}_{M+1})," display="inline"><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>G</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(97)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where we wrote <math id="A3.SS3.SSS3.p1.m1" class="ltx_Math" alttext="F=\sigma_{M+1}^{-2}X_{M+1}^{\top}X_{M+1}" display="inline"><mrow><mi>F</mi><mo>=</mo><mrow><msubsup><mi>σ</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi>X</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math>, and <math id="A3.SS3.SSS3.p1.m2" class="ltx_Math" alttext="G=\Sigma_{\theta+\tau|Y_{1:M}}^{-1}" display="inline"><mrow><mi>G</mi><mo>=</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>θ</mi><mo>+</mo><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow></math>. Thus,</p>
<table id="A3.Ex78" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex78.m1" class="ltx_Math" alttext="\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]^{\top}\mathbb{E}[(\bm{%
\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]\leq\lVert(F+G)^{-1}\rVert_{2}^{2}%
\lVert G\rVert_{2}^{2}\lVert\mathbb{E}\mu_{\tau|Y_{1:M}}-\bm{\theta}_{M+1}%
\rVert_{2}^{2}" display="block"><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0.1389em">≤</mo><mrow><msubsup><mrow><mo fence="true" lspace="0.1389em" rspace="0em">∥</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo lspace="0em">⁢</mo><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mi>G</mi><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo lspace="0em">⁢</mo><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We can bound each term in turn. First, note that <math id="A3.SS3.SSS3.p1.m3" class="ltx_Math" alttext="\lVert G\rVert_{2}^{2}\leq 1/{\sigma_{\theta}^{2}}" display="inline"><mrow><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mi>G</mi><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup></mrow></mrow></math>, and we have bounded <math id="A3.SS3.SSS3.p1.m4" class="ltx_Math" alttext="\lVert(F+G)^{-1}\rVert_{2}^{2}" display="inline"><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>F</mi><mo>+</mo><mi>G</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></math> above. We can write,</p>
<table id="A4.EGx40" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.Ex79"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.Ex79.m1" class="ltx_Math" alttext="\displaystyle\lVert\mathbb{E}\mu_{\tau|Y_{1:M}}-\bm{\theta}_{M+1}\rVert_{2}^{2}" display="inline"><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex79.m2" class="ltx_Math" alttext="\displaystyle=\left\lVert\Sigma_{\tau|Y_{1:M}}\left(\sum^{M}_{i=1}X_{i}^{\top}%
\tilde{C}_{i}^{-1}X_{i}\theta_{i}\right)-\bm{\theta}_{M+1}\right\rVert_{2}^{2}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><msubsup><mrow><mo fence="true" lspace="0.1389em" rspace="0em" stretchy="true">∥</mo><mrow><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em" stretchy="true">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A3.Ex80"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.Ex80.m1" class="ltx_Math" alttext="\displaystyle\lVert\mathbb{E}\mu_{\tau|Y_{1:M}}-\bm{\theta}_{M+1}\rVert_{2}^{2}" display="inline"><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub></mrow><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex80.m2" class="ltx_Math" alttext="\displaystyle=\left\lVert\Sigma_{\tau|Y_{1:M}}\left(\sum^{M}_{i=1}X_{i}^{\top}%
\tilde{C}_{i}^{-1}X_{i}\theta_{i}\right)-\Sigma_{\tau|Y_{1:M}}\Sigma_{\tau|Y_{%
1:M}}^{-1}\bm{\theta}_{M+1}\right\rVert_{2}^{2}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><msubsup><mrow><mo fence="true" lspace="0.1389em" rspace="0em" stretchy="true">∥</mo><mrow><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo fence="true" lspace="0em" stretchy="true">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A3.Ex81"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex81.m1" class="ltx_Math" alttext="\displaystyle=\left\lVert\Sigma_{\tau|Y_{1:M}}\sum^{M}_{i=1}X_{i}^{\top}\tilde%
{C}_{i}^{-1}X_{i}(\theta_{i}-\bm{\theta}_{M+1})\right\rVert_{2}^{2}" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><msubsup><mrow><mo fence="true" lspace="0.1389em" rspace="0em" stretchy="true">∥</mo><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo fence="true" lspace="0em" stretchy="true">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A3.Ex82"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex82.m1" class="ltx_Math" alttext="\displaystyle\leq\left(\sum^{M}_{i=1}\lVert\Sigma_{\tau|Y_{1:M}}\rVert_{2}\ %
\lVert X_{i}^{\top}\tilde{C}_{i}^{-1}X_{i}(\theta_{i}-\bm{\theta}_{M+1})\rVert%
_{2}\right)^{2}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msub><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msub><mo lspace="0em">⁢</mo><msub><mrow><mo fence="true" rspace="0em">∥</mo><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A3.Ex83"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex83.m1" class="ltx_Math" alttext="\displaystyle\leq\left(\sum^{M}_{i=1}\lVert\Sigma_{\tau|Y_{1:M}}\rVert_{2}\ %
\lVert X_{i}^{\top}\tilde{C}_{i}^{-1}X_{i}\rVert_{2}\right)^{2}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><msub><mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn></msub><mo lspace="0em">⁢</mo><msub><mrow><mo fence="true" rspace="0em">∥</mo><mrow><msubsup><mi>X</mi><mi>i</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msubsup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>i</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mo fence="true" lspace="0em" rspace="0em">∥</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The last line follows from the fact that the parameters lie in a ball of unit radius. We now proceed by bounding the sum by <math id="A3.SS3.SSS3.p1.m5" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> times the supremum — with some light abuse of notation,</p>
<table id="A4.EGx41" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A3.Ex84"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A3.Ex84.m1" class="ltx_Math" alttext="\displaystyle\lVert\mu_{\tau|Y_{1:M}}-\bm{\theta}_{M+1}\rVert_{2}^{2}" display="inline"><msubsup><mrow><mo fence="true" rspace="0em">∥</mo><mrow><msub><mi>μ</mi><mrow><mi>τ</mi><mo fence="false">|</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>M</mi></mrow></msub></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo fence="true" lspace="0em">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex84.m2" class="ltx_Math" alttext="\displaystyle\leq({s_{\max}}(X^{\top}\tilde{C}^{-1}X){s_{\max}}(X\tilde{C}^{-1%
}X))^{2}" display="inline"><mrow><mi></mi><mo>≤</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A3.Ex85"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A3.Ex85.m1" class="ltx_Math" alttext="\displaystyle={s_{\max}}(X^{\top}\tilde{C}^{-1}X)^{4}\leq O(1)" display="inline"><mrow><mi></mi><mo>=</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>X</mi><mo>⊤</mo></msup><mo>⁢</mo><msup><mover accent="true"><mi>C</mi><mo>~</mo></mover><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow><mn>4</mn></msup></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, overall the convergence of the bias is bounded by,</p>
<table id="A3.Ex87" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex87.m1" class="ltx_Math" alttext="\mathbb{E}[(\bm{\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]^{\top}\mathbb{E}[(\bm{%
\hat{\theta}}_{M+1}-\bm{\theta}_{M+1})]\leq O({s_{\max}}({\Sigma^{\prime}})^{2%
})\leq O\left(d\left[{k}+\frac{M{n}}{\frac{{n}(M+\kappa^{2})s_{2}^{2}}{\alpha_%
{2}}+A}\right]^{-2}\right)" display="block"><mrow><mrow><mi>𝔼</mi><mo>⁢</mo><msup><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>𝜽</mi><mo mathvariant="bold">^</mo></mover><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>𝜽</mi><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mi>max</mi></msub><mo>⁢</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi mathvariant="normal">Σ</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>d</mi><mo>⁢</mo><msup><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mfrac><mrow><mi>M</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mfrac><mrow><mi>n</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>+</mo><msup><mi>κ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msub><mi>α</mi><mn>2</mn></msub></mfrac><mo>+</mo><mi>A</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">∎</p>
</div>
</div>
<div id="A3.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p">The proof of Theorem <a href="#Thmtheorem4" title="Theorem 4 (Meta Linear Regression Upper Bound). ‣ 5.2 Minimax upper bounds ‣ 5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is given by the combination of Lemma <a href="#Thmlemma12" title="Lemma 12. ‣ C.3.2 Variance upper bound ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and Lemma <a href="#Thmlemma13" title="Lemma 13. ‣ C.3.3 Bounding the Bias ‣ C.3 Upper bound for meta linear regression ‣ Appendix C Hierarchical Bayesian Linear Regression Upper Bounds ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, and the bias-variance decomposition of the risk .</p>
</div>
</section>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Experiment Details</h2>

<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Hierarchical Bayes Evaluation</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p class="ltx_p">We sample <math id="A4.SS1.p1.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> linear models according to the hierarchical model in Section <a href="#S5" title="5 Analysis of a hierarchical Bayesian model of meta-learning ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, with design matrices constructed by uniformly sampling points, <math id="A4.SS1.p1.m2" class="ltx_Math" alttext="x\sim U[-1,1]" display="inline"><mrow><mi>x</mi><mo>∼</mo><mrow><mi>U</mi><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow></math>, and storing the vector <math id="A4.SS1.p1.m3" class="ltx_Math" alttext="\mathbf{x}_{j}=x^{j}" display="inline"><mrow><msub><mi>𝐱</mi><mi>j</mi></msub><mo>=</mo><msup><mi>x</mi><mi>j</mi></msup></mrow></math>, for <math id="A4.SS1.p1.m4" class="ltx_Math" alttext="i=0,\ldots,d" display="inline"><mrow><mi>i</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi></mrow></mrow></math> in each row of <math id="A4.SS1.p1.m5" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math>.</p>
</div>
<div id="A4.SS1.p2" class="ltx_para">
<p class="ltx_p">To produce the plots in Figure <a href="#S6.F2" title="Figure 2 ‣ 6.1 Hierarchical Bayes polynomial regression ‣ 6 Empirical Investigations ‣ Theoretical bounds on estimation error for meta-learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we computed the average loss over 100 random draws of the training data and labels from the same set of fixed <math id="A4.SS1.p2.m1" class="ltx_Math" alttext="\bm{\theta}_{1:M+1}" display="inline"><msub><mi>𝜽</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>M</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></math> values. The <math id="A4.SS1.p2.m2" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math> values were sampled once from the hierarchical model with <math id="A4.SS1.p2.m3" class="ltx_Math" alttext="\tau=[0,1,2,0,0,3,1]" display="inline"><mrow><mi>τ</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></math>, and <math id="A4.SS1.p2.m4" class="ltx_Math" alttext="\sigma^{2}_{\theta}=0.1" display="inline"><mrow><msubsup><mi>σ</mi><mi>θ</mi><mn>2</mn></msubsup><mo>=</mo><mn>0.1</mn></mrow></math></p>
</div>
<div id="A4.SS1.p3" class="ltx_para">
<p class="ltx_p">Code to reproduce these plots is provided in the supplementary materials with our submission.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Sinusoid Regression with MAML</h3>

<figure id="A4.SS2.tab1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Hyper parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">Description</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="A4.SS2.m1" class="ltx_Math" alttext="\sigma" display="inline"><mi>σ</mi></math></th>
<td class="ltx_td ltx_align_center ltx_border_t">noise at test time.</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">M</th>
<td class="ltx_td ltx_align_center">number of tasks at the training tasks</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><math id="A4.SS2.m2" class="ltx_Math" alttext="M_{q}" display="inline"><msub><mi>M</mi><mi>q</mi></msub></math></th>
<td class="ltx_td ltx_align_center">number of tasks at the testing tasks</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">eps_per_batch</th>
<td class="ltx_td ltx_align_center">episode per batch</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">train_ampl_range</th>
<td class="ltx_td ltx_align_center">range of amplitude at training</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">train_phase_range</th>
<td class="ltx_td ltx_align_center">range of phase at training</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">val_ampl_range</th>
<td class="ltx_td ltx_align_center">range of amplitude at testing</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">val_phase_range</th>
<td class="ltx_td ltx_align_center">range of phase at testing</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">inner_steps</th>
<td class="ltx_td ltx_align_center">number of steps of Maml</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">inner_lr</th>
<td class="ltx_td ltx_align_center">learning rate used to optimize parameter of the model</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">meta_lr</th>
<td class="ltx_td ltx_align_center">used to optimize parameter of the meta-learner</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">n</th>
<td class="ltx_td ltx_align_center">number of datapoints at training tasks(support set)</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">k</th>
<td class="ltx_td ltx_align_center">number of datapoints at testing tasks (support set)</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><math id="A4.SS2.m3" class="ltx_Math" alttext="n_{q}" display="inline"><msub><mi>n</mi><mi>q</mi></msub></math></th>
<td class="ltx_td ltx_align_center">number of datapoints at training tasks (query set) .</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><math id="A4.SS2.m4" class="ltx_Math" alttext="k_{q}" display="inline"><msub><mi>k</mi><mi>q</mi></msub></math></th>
<td class="ltx_td ltx_align_center">number of datapoints at testing tasks (query set).</td>
</tr>
</tbody>
</table>
</figure>
<div id="A4.SS2.p1" class="ltx_para">
<p class="ltx_p">For all of these experiments we used a fully connected network with 6 layers and 40 hidden units per layer. The network is trained using the MAML algorithm <cite class="ltx_cite ltx_citemacro_citep">(Finn<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib25" title="Model-agnostic meta-learning for fast adaptation of deep networks" class="ltx_ref">2017</a>)</cite> with 5 inner steps using SGD with an inner learning rate of <math id="A4.SS2.p1.m1" class="ltx_Math" alttext="10^{-3}" display="inline"><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></math>. We used Adam for the outer loop learning with a learning rate of <math id="A4.SS2.p1.m2" class="ltx_Math" alttext="10^{-3}" display="inline"><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></math>.</p>
</div>
<div id="A4.SS2.p2" class="ltx_para">
<p class="ltx_p">Expected error was computed after 500 epochs of optimization and was averaged over 30 runs. We produced our results through a comprehensive grid search over 72 combinations of the settings below and it required around 30 minutes to produce the output of each setting, using a system with 1 gpu and 3 cpus. This experiment therefore lasted 20 hours in total.

<br class="ltx_break"><math id="A4.SS2.p2.m1" class="ltx_Math" alttext="M=50,n\in\{20,200\},k\in\{100,1000\},\sigma\in[10^{-8},1.5],M_{q}=100,\text{ %
eps\_per\_batch}=25,\text{ train\_ampl\_range}=[1,4],\text{ train\_phase\_%
range}=[0,\pi/2],\text{ val\_ampl\_range}=[3,5],\text{ val\_phase\_range}=[0,%
\pi/2],\text{ inner\_steps}=5,\text{ inner\_lr}=10^{-3},\text{ meta\_lr}=10^{-3}" display="inline"><mrow><mrow><mi>M</mi><mo>=</mo><mn>50</mn></mrow><mo>,</mo><mrow><mrow><mi>n</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>20</mn><mo>,</mo><mn>200</mn><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>100</mn><mo>,</mo><mn>1000</mn><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>σ</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup><mo>,</mo><mn>1.5</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>M</mi><mi>q</mi></msub><mo>=</mo><mn>100</mn></mrow><mo>,</mo><mrow><mrow><mtext> eps_per_batch</mtext><mo>=</mo><mn>25</mn></mrow><mo>,</mo><mrow><mrow><mtext> train_ampl_range</mtext><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mtext> train_phase_range</mtext><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mrow><mi>π</mi><mo>/</mo><mn>2</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mtext> val_ampl_range</mtext><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mtext> val_phase_range</mtext><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mrow><mi>π</mi><mo>/</mo><mn>2</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mtext> inner_steps</mtext><mo>=</mo><mn>5</mn></mrow><mo>,</mo><mrow><mrow><mtext> inner_lr</mtext><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><mo>,</mo><mrow><mtext> meta_lr</mtext><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Apr 10 10:29:12 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
