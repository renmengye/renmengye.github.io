\section{Conclusion}
\vspace{-0.1in}
Meta-learning algorithms identify the inductive bias from source tasks and make models more adaptive towards unseen novel distribution.
In this paper, we take initial steps towards characterizing the difficulty of meta-learning and understanding how these limitations present in practice. We have derived both lower bounds and upper bounds on the error of meta-learners, which are particularly relevant in the few-shot learning setting where $k$ is small. Our bounds capture key features of the meta-learning problem, such as the effect of increasing the number of shots or training tasks.
We have also identified a gap between our lower and upper bounds when there are a large number of training tasks, which we hypothesize is a limitation of the proof technique that we applied to derive the lower bounds --- suggesting an exciting direction for future research.

