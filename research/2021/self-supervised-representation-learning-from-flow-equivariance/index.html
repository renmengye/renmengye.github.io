<!DOCTYPE html><html><head>
<title>Self-Supervised Representation Learning from Flow Equivariance</title>
<!--Generated by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="index.css" type="text/css">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Self-Supervised Representation Learning from Flow Equivariance</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuwen Xiong, Mengye Ren, Wenyuan Zeng, Raquel Urtasun
<br class="ltx_break">
<br class="ltx_break">Waabi, University of Toronto
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yuwen, mren, wenyuan, urtasun}@cs.toronto.edu</span>

</span><span class="ltx_author_notes"><span>This work was done by all authors while at Uber ATG</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Self-supervised representation learning is able to learn semantically meaningful features; however,
much of its recent success relies on multiple crops of an image with very few objects. Instead of
learning view-invariant representation from simple images, humans learn representations in a complex
world with changing scenes by observing object movement, deformation, pose variation and ego motion.
Motivated by this ability, we present a new self-supervised learning representation framework that
can be directly deployed on a video stream of complex scenes with many moving objects. Our framework
features a simple flow equivariance objective that encourages the network to predict the features of
another frame by applying a flow transformation to the features of the current frame. Our
representations, learned from high-resolution raw video, can be readily used for downstream tasks on
static images. Readout experiments on challenging semantic segmentation, instance segmentation, and
object detection benchmarks show that we are able to outperform representations obtained from
previous state-of-the-art methods including SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite> and
BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Rich and informative visual representations epitomize the revolution of deep
learning in computer vision in the past decade. Deep neural nets deliver
surprisingly competitive performance on tasks such as object detection
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="Fast r-cnn" class="ltx_ref">15</a>, <a href="#bib.bib6" title="Faster r-cnn: towards real-time object detection with region proposal networks" class="ltx_ref">34</a>, <a href="#bib.bib65" title="R-fcn: object detection via region-based fully convolutional networks" class="ltx_ref">9</a>]</cite> and semantic
segmentation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs" class="ltx_ref">4</a>, <a href="#bib.bib11" title="Pyramid scene parsing network" class="ltx_ref">50</a>]</cite>. Until very recently,
visual representations have been learned by large scale supervised learning.
However, for more challenging tasks such as semantic or instance segmentation,
it is much more expensive to obtain labels compared to object classification.
On the other hand, the human brain learns generic visual representations from
raw video of the complex world without much explicit supervision. This is the
direction that we would like to get one step closer towards in this paper.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="figure/intro_new.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="810" height="503" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Our proposed self-supervised representation learning from Flow
Equivariance (FlowE).<span class="ltx_text ltx_font_medium"> Our method is based on
BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>, a state of the art method for static image
representation learning. We encourage the features to obey the same flow
transformation as the input image pairs.</span></span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Recent advances in self-supervised or unsupervised representation learning,
such as SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite> and BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>, seem
to point us to a bright path forward: by simply minimizing the feature distance
between two different views of a single image, and performing linear readout at
test time on top, state-of-the-art approaches are now able to match the
classification performance of networks trained with full supervision
end-to-end&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="Imagenet classification with deep convolutional neural networks" class="ltx_ref">23</a>, <a href="#bib.bib69" title="Very deep convolutional networks for large-scale image recognition" class="ltx_ref">36</a>, <a href="#bib.bib7" title="Deep residual learning for image recognition" class="ltx_ref">21</a>]</cite>. While
not using any class labels, these methods still rely on the dataset curation
process of carefully selecting clean and object-centered images with a balanced
class distribution. In contrast, videos in the wild feature crowded scenes and
severe data imbalance. As a result, different crops of the same frame can often
lead to either uninteresting regions or erroneous alignment of different
instances in crowded areas. Moreover, none of these methods leverage temporal
information, which contains a rich set of object movement, deformation, and
pose variations. While there has been a large body of literature on learning
representation from video &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="Unsupervised learning of visual representations using videos" class="ltx_ref">43</a>, <a href="#bib.bib22" title="Joint-task self-supervised learning for temporal correspondence" class="ltx_ref">24</a>, <a href="#bib.bib23" title="Learning correspondence from the cycle-consistency of time" class="ltx_ref">44</a>, <a href="#bib.bib24" title="Time-contrastive networks: self-supervised learning from video" class="ltx_ref">35</a>, <a href="#bib.bib25" title="Temporal cycle-consistency learning" class="ltx_ref">12</a>, <a href="#bib.bib26" title="Counting out time: class agnostic video repetition counting in the wild" class="ltx_ref">13</a>, <a href="#bib.bib27" title="Action recognition by dense trajectories" class="ltx_ref">41</a>, <a href="#bib.bib28" title="Action recognition with improved trajectories" class="ltx_ref">42</a>]</cite>, they typically focus on the predicting
correspondence across frames and have not shown better performance on generic
downstream tasks such as semantic and instance segmentation than pretrained
supervised representations from ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="Mask r-cnn" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper, we are interested in learning generic representation from raw
high-resolution videos that are directly useful for object detection as well as
semantic and instance segmentation. Whereas prior invariance-based learning
algorithms completely disregard ego-motion and flow transformations across
frames, we argue these are essential elements responsible for the learning of
visual representations in complex scenes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="Learning to see by moving" class="ltx_ref">1</a>]</cite>. Instead of
enforcing multiple crops of the same image (or adjacent frames) to be close in
the feature space, as advocated in prior literature
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">19</a>, <a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>, <a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>, <a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite>, we
propose a simple flow equivariance objective that can be applied densely at
every pixel on the feature map, summarized in Figure&nbsp;<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In
particular, given two consecutive video frames, we estimate an optical flow map
that denotes a pixel-wise transformation <span id="S1.p3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{T}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span></span></span></span></span> between the two frames.
We then train the network to minimize the distance between the the first frame
<span id="S1.p3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{h}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and the warped features of the second frame
<span id="S1.p3.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{T}^{-1}(\mathbf{h}_{2})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.288em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.663em; padding-left: 0.6em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. Using optical flow ensures that crowded
regions are handled with precise instance alignment. It is also worth noting
that off-the-shelf flow estimators can be trained using either from graphics
simulation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="A naturalistic open source movie for optical flow evaluation" class="ltx_ref">2</a>, <a href="#bib.bib52" title="Flownet: learning optical flow with convolutional networks" class="ltx_ref">11</a>, <a href="#bib.bib53" title="A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation" class="ltx_ref">26</a>]</cite>
or from ego-motion and depth estimation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="Competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation" class="ltx_ref">33</a>]</cite>, without
any human labeling effort.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Experiments are carried out on two complex driving video datasets,
BDD100K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="BDD100K: a diverse driving dataset for heterogeneous multitask learning" class="ltx_ref">48</a>]</cite> and our in-house dataset UrbanCity, which are
collected from a front camera on a moving car, just like seeing from a mobile
agent in the wild. Our approach, learning from raw videos, can achieve
competitive readout performance on semantic and instance segmentation tasks.
Surprisingly, we are also able to outperform pre-trained representations from
ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="Deep residual learning for image recognition" class="ltx_ref">21</a>]</cite>, likely because of the large domain gap between
ImageNet images and driving videos.

</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In past few years, there has been tremendous progress in learning visual
representations without class label supervision&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="Scaling and benchmarking self-supervised visual representation learning" class="ltx_ref">17</a>]</cite>.
Typically, networks are trained to predict certain held-out information about
the inputs, such as
context&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="Unsupervised visual representation learning by context prediction" class="ltx_ref">10</a>, <a href="#bib.bib61" title="Unsupervised learning of visual representations by solving jigsaw puzzles" class="ltx_ref">28</a>]</cite>,
rotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="Unsupervised representation learning by predicting image rotations" class="ltx_ref">14</a>]</cite>, colorization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="Colorful image colorization" class="ltx_ref">49</a>]</cite>
and counting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="Representation learning by learning to count" class="ltx_ref">29</a>]</cite>. Although they have shown to
learn interesting representations, they are still significantly behind
supervised representations on classification tasks.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">More recently, contrastive
learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="Representation learning with contrastive predictive coding" class="ltx_ref">30</a>, <a href="#bib.bib44" title="Contrastive multiview coding" class="ltx_ref">40</a>]</cite> has emerged as a
promising direction for representation learning, closing the gap with
supervised representation on ImageNet. The high level idea is to obtain
different views of the same image using random cropping and other data
augmentations to serve as positive labels, contrasting with other images that
serve as negative labels. MoCo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">19</a>]</cite> proposed to perform
momentum averaging on the network that encodes negative samples, and
SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite> proposed to add a non-linear projection head to
make the core representation more general.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Building along this line of work, BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite> removed the
need for negative samples by simply using a slow network with weights getting
slowly updated from the fast network. BYOL proposed to simply minimize the
feature distance between a pair of views of the same image. It is currently one
of the state-of-the-art methods for representation learning on ImageNet.
However, all of the above methods rely on clean static images, which cannot be
easily obtained through raw videos.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Applying contrastive learning on videos seems like a direct extension.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="Unsupervised learning of visual representations using videos" class="ltx_ref">43</a>]</cite> proposed to perform unsupervised tracking first to
obtain positive and negative crops of images from different frames in a video
sequence. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="Time-contrastive networks: self-supervised learning from video" class="ltx_ref">35</a>]</cite> proposed a multi-view approach that tries to
learn by matching different views from multiple cameras. More recently,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="Self-supervised learning through the eyes of a child" class="ltx_ref">31</a>]</cite> treated adjacent frames as positive pairs, whereas
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="Online learning of object representations by appearance space feature alignment" class="ltx_ref">32</a>]</cite> preprocessed videos by a class agnostic object detector.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite> proposed multi-label video contrastive learning.
While using video as input, these methods only considers the invariance
relation between frames and throw away transformations across frames.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">This drawback could potentially be complemented by another class of
self-supervised learning algorithms that aim to predict some level of
correspondence or transformation across
frames&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="Learning to see by moving" class="ltx_ref">1</a>, <a href="#bib.bib22" title="Joint-task self-supervised learning for temporal correspondence" class="ltx_ref">24</a>]</cite>. Cycle consistency is a popular
form of self-supervision that encourages both forward and backward flow on a
sequence of frames to be consistent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="Learning correspondence from the cycle-consistency of time" class="ltx_ref">44</a>, <a href="#bib.bib29" title="Space-time correspondence as a contrastive random walk" class="ltx_ref">22</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="Temporal cycle-consistency learning" class="ltx_ref">12</a>]</cite> looked at frame-wise correspondence and encourage
cycle consistency across different videos. Typically these approaches show
competitive performance in terms of video correspondence and label
propagation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="Learning correspondence from the cycle-consistency of time" class="ltx_ref">44</a>, <a href="#bib.bib29" title="Space-time correspondence as a contrastive random walk" class="ltx_ref">22</a>]</cite>, showing a rough
understanding of optical flow. While flow correspondence could be used as
representation for action recognition in the early
literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="Action recognition by dense trajectories" class="ltx_ref">41</a>, <a href="#bib.bib28" title="Action recognition with improved trajectories" class="ltx_ref">42</a>]</cite>, we would like to decouple the
two tasks between predicting flow correspondence and learning generic visual
representation by providing the flow predictions from off-the-shelf estimators.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="figure/model_v5.png" id="S2.F2.g1" class="ltx_graphics ltx_centering" width="810" height="236" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">The FlowE learning algorithm.</span> Given two images of a video <span id="S2.F2.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>
and <span id="S2.F2.m11" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>, a standalone flow network predicts a dense optical flow field
<span id="S2.F2.m12" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{M}_{1\rightarrow 2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">M</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. Two augmented version of images <span id="S2.F2.m13" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v=t(I_{1})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>
and <span id="S2.F2.m14" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v^{\prime}=t^{\prime}(I_{2})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> are fed into the online and target neural network
respectively. The spatial dimension is preserved during the forward pass. The
inverse transformation <span id="S2.F2.m15" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{T}^{-1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.288em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.663em; padding-left: 0.6em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> is then used to warp the projected
representation <span id="S2.F2.m16" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> to <span id="S2.F2.m17" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> to make it align with
<span id="S2.F2.m18" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>.</span></figcaption>
</figure>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">There has also been a large body of literature on equivariance learning. Just
like how the convolution operator is translational equivariant,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="Equivariance and invariance-an approach based on lie groups" class="ltx_ref">27</a>, <a href="#bib.bib43" title="Group equivariant convolutional networks" class="ltx_ref">7</a>, <a href="#bib.bib38" title="Cubenet: equivariance to 3d rotation and translation" class="ltx_ref">45</a>]</cite> enforce
strict equivariance over transformation groups. In comparison, we do not
enforce strict equivariance but instead encode it in our training objective to
achieve self-supervision. Our work is most similar to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="Unsupervised learning of object frames by dense equivariant image labelling" class="ltx_ref">39</a>]</cite> which
also warps feature maps using optical flow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="Deep feature flow for video recognition" class="ltx_ref">52</a>, <a href="#bib.bib47" title="Flow-guided feature aggregation for video object detection" class="ltx_ref">51</a>]</cite>.
Whereas &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="Unsupervised learning of object frames by dense equivariant image labelling" class="ltx_ref">39</a>]</cite> tried to directly regress the relative coordinates,
we make use of a simpler distance loss in the feature space. In the end,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="Unsupervised learning of object frames by dense equivariant image labelling" class="ltx_ref">39</a>]</cite> produced 3 dimensional image encoding and by contrast we
produce generic high dimensional visual representations.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">In the background section, we first review BYOL, a state-of-the-art
self-supervised reprensetation learning algorithm, upon which we will build our
FlowE algorithm on top. We then cover the basics on flow warping.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Bootstrap your own latent (BYOL):</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">BYOL performs representation learning by matching two different views of the
same image together. It consists of two neural networks: the online and target
networks. The online network gets updated every iteration and the target
network keeps a momentum averaged copy of the weights. During training, the
online network is going to predict the features produced by the target network,
and the motivation of having a separate target network is to avoid trivial
solution where all images collapse to the same representation.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">More specifically, two augmented views <span id="S3.SS1.SSS0.Px1.p2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS1.SSS0.Px1.p2.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> of the same sample are
fed into the encoder <span id="S3.SS1.SSS0.Px1.p2.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> of the online and target network, to get
representation <span id="S3.SS1.SSS0.Px1.p2.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{h}_{1},\mathbf{h}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. To keep the representations general for other readout tasks, BYOL adds a
projector <span id="S3.SS1.SSS0.Px1.p2.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="g"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span></span></span></span>, just like SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite>. <span id="S3.SS1.SSS0.Px1.p2.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="g"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span></span></span></span> transforms <span id="S3.SS1.SSS0.Px1.p2.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{h}_{1},\mathbf{h}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> into <span id="S3.SS1.SSS0.Px1.p2.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS1.SSS0.Px1.p2.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. Finally, the
predictor <span id="S3.SS1.SSS0.Px1.p2.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span></span></span></span></span></span> takes <span id="S3.SS1.SSS0.Px1.p2.m11" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and try to produce <span id="S3.SS1.SSS0.Px1.p2.m12" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> that
matches with <span id="S3.SS1.SSS0.Px1.p2.m13" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. Concretely, BYOL minimizes the squared L2 distance
between <span id="S3.SS1.SSS0.Px1.p2.m14" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{\mathbf{p}}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.07em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span></span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS1.SSS0.Px1.p2.m15" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{\mathbf{z}}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.006em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span></span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>:</p>
<div class="ltx_engrafo_equation_container"><table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S3.E1.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</tbody></table></div>
<p class="ltx_p">where <span id="S3.SS1.SSS0.Px1.p2.m16" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{\cdot}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op" style="margin-top: 0.092em; padding-left: 0.111em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span></span></span></span></span></span></span></span></span></span></span> denotes unit-normalization. Note that the target network
is only updated through moving average to avoid trivial solution of collapsed
representation. After the self-supervised training is finished, the projector
and predictor of the online network as well as the target network are discarded
and the encoder of the online network will be preserved for further readout of
downstream tasks such as object classification.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Warping via optical flow:</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">Optical flow is widely used in many video processing applications. A flow field
is a two dimensional vector field which defines dense pixel correspondences
between two different video frames. Given a flow field
<span id="S3.SS1.SSS0.Px2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{M}_{1\rightarrow 2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">M</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>, for each pixel on <span id="S3.SS1.SSS0.Px2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> we can find the
corresponding location on <span id="S3.SS1.SSS0.Px2.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> and obtain the pixel value via bilinear
interpolation. The warping operation can also be applied to convolutional
feature maps&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="Deep feature flow for video recognition" class="ltx_ref">52</a>, <a href="#bib.bib47" title="Flow-guided feature aggregation for video object detection" class="ltx_ref">51</a>]</cite>. In our work we use an
off-the-shelf optical flow predictor RAFT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="RAFT: recurrent all-pairs field transforms for optical flow" class="ltx_ref">38</a>]</cite> because of its
empirical success.

</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learning from Flow Equivariance</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">Our method learns dense pixel-level representations based on a flow
equivariance objective, which encourages the features to obey the same flow
transformation as the input image pairs. Our equivariance objective ensures
that a pair of pixels are sampled from the same object across two different
video frames.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Fig.&nbsp;<a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overview of our framework. Next, we will explain
how it works in detail.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Optical flow &amp; random affine transformation:</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Given two images <span id="S3.SS2.SSS0.Px1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS2.SSS0.Px1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> from a video, We use a frozen flow network
<span id="S3.SS2.SSS0.Px1.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{F}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.11em;">F</span></span></span></span></span></span></span></span></span> to predict a dense optical flow field <span id="S3.SS2.SSS0.Px1.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{M}_{1\rightarrow 2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">M</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> from the two images. We then obtain augmented versions of the two images by
performing random affine transformations <span id="S3.SS2.SSS0.Px1.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{A}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS2.SSS0.Px1.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{A}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>:</p>
<div class="ltx_engrafo_equation_container"><table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E2.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle v_{1} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span></span></span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.E2.m2" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle=\mathcal{A}_{1}(I_{1}), "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E3.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle v_{2} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span></span></span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.E3.m2" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle=\mathcal{A}_{2}(I_{2}). "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table></div>
<p class="ltx_p">Following&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>, <a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>, we further apply random
color distortion and Gaussian blurring on each image. The flow transformation
<span id="S3.SS2.SSS0.Px1.p1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{T}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span></span></span></span></span> between <span id="S3.SS2.SSS0.Px1.p1.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS2.SSS0.Px1.p1.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> is thus defined as the following:</p>
<div class="ltx_engrafo_equation_container"><table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E4.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle\mathcal{T}=\mathcal{A}_{1}^{-1}\circ\mathcal{M}_{1\rightarrow 2}%
\circ\mathcal{A}_{2}. "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-stack" style="vertical-align: -0.264em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.109em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∘</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">M</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∘</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.021em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.021em;">A</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table></div>
<p class="ltx_p">We then feed the two views into an online network <span id="S3.SS2.SSS0.Px1.p1.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_{\theta},g_{\theta}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span></span></span></span></span> and a
momentum updated network <span id="S3.SS2.SSS0.Px1.p1.m11" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_{\xi},g_{\xi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.005em;">ξ</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.005em;">ξ</span></span></span></span></span></span></span></span></span></span></span> to obtain the representation
<span id="S3.SS2.SSS0.Px1.p1.m12" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{h}_{1},\mathbf{h}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">h</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>, projection <span id="S3.SS2.SSS0.Px1.p1.m13" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{1},\mathbf{z}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> and
prediction <span id="S3.SS2.SSS0.Px1.p1.m14" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>, as shown in Fig.&nbsp;<a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Equivariance learning:</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">Our network is fully convolutional and the
spatial dimension of the feature maps are preserved to represent multiple
objects for complex video scenes. We propose to use equivariance as our
training objective. Concretely, we use the inverse flow <span id="S3.SS2.SSS0.Px2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{T}^{-1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.288em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.663em; padding-left: 0.6em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> to
warp back <span id="S3.SS2.SSS0.Px2.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{z}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> to obtain
<span id="S3.SS2.SSS0.Px2.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{2}=\mathcal{T}^{-1}(\mathbf{z}_{2})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.288em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.288em;">T</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.663em; padding-left: 0.6em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.372em;">z</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>. We use the online network output
<span id="S3.SS2.SSS0.Px2.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> from the predictor to match with <span id="S3.SS2.SSS0.Px2.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>. The objective
is simply a squared <span id="S3.SS2.SSS0.Px2.p1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> loss averaged over all spatial locations.</p>
<div class="ltx_engrafo_equation_container"><table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E5.m1" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle\mathcal{L} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">L</span></span></span></span></span></span></span></span></span></span></span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.E5.m2" class="ltx_DisplayMath"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: left;"><span class="mjx-math" aria-label=" \displaystyle=\frac{1}{HW}\left\lVert\tilde{\mathbf{p}}_{1}-\tilde{\mathbf{p}}%
_{2}\right\rVert_{2}^{2}, "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 2.136em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 2.136em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span><span class="mjx-denominator" style="width: 2.136em; bottom: -0.795em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.136em;" class="mjx-line"></span></span><span style="height: 2.163em; vertical-align: -0.795em;" class="mjx-vsize"></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.07em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span></span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.07em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span></span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span></span></span><span class="mjx-stack" style="vertical-align: -0.31em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.433em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span></span></span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table></div>
<p class="ltx_p">where <span id="S3.SS2.SSS0.Px2.p1.m7" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{\cdot}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span><span class="mjx-op" style="margin-top: 0.092em; padding-left: 0.111em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span></span></span></span></span></span></span></span></span></span></span> denotes the unit-normalization across the channel
dimension, and <span id="S3.SS2.SSS0.Px2.p1.m8" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="H"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span></span></span></span></span> and <span id="S3.SS2.SSS0.Px2.p1.m9" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span></span></span></span></span> denote the spatial resolution of the convolutional
feature map. Similar to the loss in Equation&nbsp;<a href="#S3.E1" title="(1) ‣ Bootstrap your own latent (BYOL): ‣ 3.1 Background ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we make sure that
every pixel pair in <span id="S3.SS2.SSS0.Px2.p1.m10" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> and <span id="S3.SS2.SSS0.Px2.p1.m11" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{p}_{2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.151em; padding-bottom: 0.593em;">p</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.36em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span> has a small squared L2
distance.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<table style="width:100%;">
<tbody><tr>
<td class="ltx_subfigure">
<figure id="S3.F2.sf1" class="ltx_figure ltx_align_center" style="margin: 0px;"><img src="figure/affine_a.jpg" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering" width="135" height="80" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Original</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F2.sf2" class="ltx_figure ltx_align_center" style="margin: 0px;"><img src="figure/affine_b.jpg" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering" width="135" height="80" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Scale (zoom in)</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F2.sf3" class="ltx_figure ltx_align_center" style="margin: 0px;"><img src="figure/affine_c.jpg" id="S3.F2.sf3.g1" class="ltx_graphics ltx_centering" width="135" height="80" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Scale (zoom out)</span></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S3.F2.sf4" class="ltx_figure ltx_align_center" style="margin: 0px;"><img src="figure/affine_d.jpg" id="S3.F2.sf4.g1" class="ltx_graphics ltx_centering" width="135" height="80" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">Rotate</span></figcaption>
</figure>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Random affine transformations.<span class="ltx_text ltx_font_medium"> We consider adding random
scaling and rotation before sending the images to the network.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Full Algorithm:</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">The full learning algorithm is summarized in Algorithm&nbsp;<a href="#algorithm1" title="Algorithm 1 ‣ Full Algorithm: ‣ 3.2 Learning from Flow Equivariance ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, in PyTorch style.
</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_float_algorithm ltx_framed_top ltx_framed_bottom" style="margin: 4em 0em 3em 0px;">

<div class="ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing" style="background-color: rgb(255, 255, 255); padding-left: 0em; margin: 1em 0px;">
<div class="ltx_listing_data"><a href="http://data:text/plain;base64,Zm9yIEkxLCBJMiBpbiBkYXRhX2xvYWRlcjoKICB3aXRoIG5vX2dyYWQoKToKICAgIGZsb3cgPSBmbG93bmV0KEkxLCBJMikgIyBbQiwgMiwgSCwgV10KCiAgdjEsIEExID0gZGF0YV9hdWcoSTEpCiAgdjIsIEEyID0gZGF0YV9hdWcoSTIpCgogIGgxID0gZW5jb2Rlcih2MSkgICAjIGZfdGhldGEKICB6MSA9IHByb2plY3RvcihoMSkgIyBnX3RoZXRhCiAgcDEgPSBwcmVkaWN0b3IoejEpICMgcV90aGV0YQoKICB3aXRoIG5vX2dyYWQoKToKICAgIGgyID0gdGFyZ2V0X2VuY29kZXIodjIpICMgZl94aQogICAgejIgPSB0YXJnZXRfcHJvamVjdG9yKGgyKSAjIGdfeGkKCiAgIyB1cHNhbXBsZSB0byBtYXRjaCBmbG93IHNoYXBlCiAgcDEgPSB1cHNhbXBsZShwMSkgIyBbQiwgQywgSCwgV10KICB6MiA9IHVwc2FtcGxlKHoyKSAjIFtCLCBDLCBILCBXXQoKICAjIHdhcnAgdGhlIGZlYXR1cmUgbWFwIHdpdGggZmxvdyB2aWEgYmlsaW5lYXIgaW50ZXJwCiAgVCA9IGFwcGx5KGFwcGx5KGludihBMSksIGZsb3cpLCBBMikKICBpbnZfVCA9IGludihUKQogIHAyID0gdHJhbnNmb3JtKHoyLCBpbnZfVCkKCiAgbCA9IGxvc3Mobm9ybWFsaXplKHAxKSwgbm9ybWFsaXplKHAyKSkgIyBFcS4gKDUpCiAgbC5iYWNrd2FyZCgpCgogIG9wdGltaXplci51cGRhdGUoW2VuY29kZXIsIHByb2plY3RvciwgcHJlZGljdG9yXSkKICBtb21lbnR1bV91cGRhdGUoW3RhcmdldF9lbmNvZGVyLCB0YXJnZXRfcHJvamVjdG9yXSk=">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span class="ltx_text ltx_lst_keyword ltx_font_typewriter">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I1</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_loader</span><span class="ltx_text ltx_font_typewriter">:</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">no_grad</span><span class="ltx_text ltx_font_typewriter">():</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">flow</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">flownet</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I1</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I2</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>[B,<span class="ltx_text ltx_lst_space">&nbsp;</span>2,<span class="ltx_text ltx_lst_space">&nbsp;</span>H,<span class="ltx_text ltx_lst_space">&nbsp;</span>W]</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">v1</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">A1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_aug</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I1</span><span class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">v2</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">A2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_aug</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">I2</span><span class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">h1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">encoder</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">v1</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>f_theta</span>
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">projector</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">h1</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>g_theta</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">predictor</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z1</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>q_theta</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">no_grad</span><span class="ltx_text ltx_font_typewriter">():</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">h2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">target_encoder</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">v2</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>f_xi</span>
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">target_projector</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">h2</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>g_xi</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>upsample<span class="ltx_text ltx_lst_space">&nbsp;</span>to<span class="ltx_text ltx_lst_space">&nbsp;</span>match<span class="ltx_text ltx_lst_space">&nbsp;</span>flow<span class="ltx_text ltx_lst_space">&nbsp;</span>shape</span>
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">upsample</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p1</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>[B,<span class="ltx_text ltx_lst_space">&nbsp;</span>C,<span class="ltx_text ltx_lst_space">&nbsp;</span>H,<span class="ltx_text ltx_lst_space">&nbsp;</span>W]</span>
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">upsample</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z2</span><span class="ltx_text ltx_font_typewriter">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>[B,<span class="ltx_text ltx_lst_space">&nbsp;</span>C,<span class="ltx_text ltx_lst_space">&nbsp;</span>H,<span class="ltx_text ltx_lst_space">&nbsp;</span>W]</span>
</div>
<div id="lstnumberx19" class="ltx_listingline">
</div>
<div id="lstnumberx20" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>warp<span class="ltx_text ltx_lst_space">&nbsp;</span>the<span class="ltx_text ltx_lst_space">&nbsp;</span>feature<span class="ltx_text ltx_lst_space">&nbsp;</span>map<span class="ltx_text ltx_lst_space">&nbsp;</span>with<span class="ltx_text ltx_lst_space">&nbsp;</span>flow<span class="ltx_text ltx_lst_space">&nbsp;</span>via<span class="ltx_text ltx_lst_space">&nbsp;</span>bilinear<span class="ltx_text ltx_lst_space">&nbsp;</span>interp</span>
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">T</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter">apply</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter">apply</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">inv</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">A1</span><span class="ltx_text ltx_font_typewriter">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">flow</span><span class="ltx_text ltx_font_typewriter">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">A2</span><span class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">inv_T</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">inv</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">T</span><span class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">transform</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">z2</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">inv_T</span><span class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_font_typewriter">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">loss</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">normalize</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p1</span><span class="ltx_text ltx_font_typewriter">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">normalize</span><span class="ltx_text ltx_font_typewriter">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">p2</span><span class="ltx_text ltx_font_typewriter">))</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span class="ltx_text ltx_lst_space">&nbsp;</span>Eq.<span class="ltx_text ltx_lst_space">&nbsp;</span>(5)</span>
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">l</span><span class="ltx_text ltx_font_typewriter">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">backward</span><span class="ltx_text ltx_font_typewriter">()</span>
</div>
<div id="lstnumberx27" class="ltx_listingline">
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">optimizer</span><span class="ltx_text ltx_font_typewriter">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">update</span><span class="ltx_text ltx_font_typewriter">([</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">encoder</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">projector</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">predictor</span><span class="ltx_text ltx_font_typewriter">])</span>
</div>
<div id="lstnumberx29" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">momentum_update</span><span class="ltx_text ltx_font_typewriter">([</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">target_encoder</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">target_projector</span><span class="ltx_text ltx_font_typewriter">])</span>
</div>
</div>
<p class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;">&nbsp;</span></p>
<figcaption class="ltx_caption" style="margin-bottom: 1em;"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> <span class="ltx_text" style="font-size:90%;">Pseudocode in a PyTorch-like style. </span></figcaption></figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation Details</h3>

<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Network architecture:</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We use ResNet-50 as our base encoder network.
We increase the resolution of the output. Following&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="Pyramid scene parsing network" class="ltx_ref">50</a>]</cite>, we
use dilated convolution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="Semantic image segmentation with deep convolutional nets and fully connected crfs" class="ltx_ref">3</a>]</cite> and remove the the downsampling
operation in the last two stages of the encoder, which leads to an encoder with
output stride 8. The number of channels in the projector and predictor are the
same as BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>. To preserve the spatial dimensions of
the output feature maps, we first remove the final global average pooling layer
in the encoder <span id="S3.SS3.SSS0.Px1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span>. The linear layers in the following projector <span id="S3.SS3.SSS0.Px1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="g"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span></span></span></span> and
predictor <span id="S3.SS3.SSS0.Px1.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span></span></span></span></span></span> are also replaced with <span id="S3.SS3.SSS0.Px1.p1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> convolutional layers to handle
the convolutional feature maps. Note that by using <span id="S3.SS3.SSS0.Px1.p1.m5" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1\times 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> convolution, we
do not increase any extra parameters comparing to the original BYOL.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Flow network:</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">For optical flow prediction, we use
RAFT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="RAFT: recurrent all-pairs field transforms for optical flow" class="ltx_ref">38</a>]</cite> as an off-the-shelf solution. The model is trained on
Flying Chair&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="Flownet: learning optical flow with convolutional networks" class="ltx_ref">11</a>]</cite>, Flying Things&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation" class="ltx_ref">26</a>]</cite>
and Sintel&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="A naturalistic open source movie for optical flow evaluation" class="ltx_ref">2</a>]</cite> datasets. All of these datasets
contain only synthetic data with no human labeling. The network is kept frozen
during our experiments.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data augmentation:</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">For color distortion and Gaussian blurring, we use the same parameters as the
ones used in SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite>. For affine transformation, we do
random scale for <span id="S3.SS3.SSS0.Px3.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.5\sim 2.0\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2.0</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> and rotate for <span id="S3.SS3.SSS0.Px3.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-30\sim 30"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">30</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">30</span></span></span></span></span></span></span> degrees.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Flow post-processing:</h4>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p">Since the affine transformation and flow operation are not strictly bijective
due to cropping and object occlusion, in the loss function we ignore any pixels
that have no correspondence. Occluded pixels then can be found by a
forward-backward flow consistency check&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="Dense point trajectories by gpu-accelerated large displacement optical flow" class="ltx_ref">37</a>]</cite>.

</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">UrbanCity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">BDD100K</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU<span id="S4.T1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">mAP<span id="S4.T1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU<span id="S4.T1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP<span id="S4.T1.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Rand Init</th>
<td class="ltx_td ltx_align_center ltx_border_t">9.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">27.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">9.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">22.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">5.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CRW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="Space-time correspondence as a contrastive random walk" class="ltx_ref">22</a>]</cite>
</th>
<td class="ltx_td ltx_align_center">19.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">31.6</td>
<td class="ltx_td ltx_align_center ltx_border_r">15.2</td>
<td class="ltx_td ltx_align_center">19.4</td>
<td class="ltx_td ltx_align_center">1.7</td>
<td class="ltx_td ltx_align_center">34.7</td>
<td class="ltx_td ltx_align_center">22.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VINCE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite>
</th>
<td class="ltx_td ltx_align_center">30.6</td>
<td class="ltx_td ltx_align_center">0.9</td>
<td class="ltx_td ltx_align_center">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_r">17.8</td>
<td class="ltx_td ltx_align_center">23.2</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">39.5</td>
<td class="ltx_td ltx_align_center">23.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FlowE (Ours)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">49.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">5.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">61.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">19.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">37.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">5.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">49.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">24.9</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">End-to-end supervised</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">63.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">67.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">16.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">20.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">Self-supervised learning results on UrbanCity and BDD100K</span>, in
comparison with other self-supervised video representation learning methods. All
readouts are done with a frozen backbone except for the “end-to-end
supervised” entry. Results with <span id="S4.T1.m6" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span> are obtained with heavier readout header.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2">Train data</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">UrbanCity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">BDD100K</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU<span id="S4.T2.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP<span id="S4.T2.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mIoU<span id="S4.T2.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP<span id="S4.T2.m4" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Supervised</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ImageNet</th>
<td class="ltx_td ltx_align_center ltx_border_t">39.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">57.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">34.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.6</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">52.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">24.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ImageNet</th>
<td class="ltx_td ltx_align_center">37.0</td>
<td class="ltx_td ltx_align_center">3.0</td>
<td class="ltx_td ltx_align_center">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">21.0</span></td>
<td class="ltx_td ltx_align_center">28.1</td>
<td class="ltx_td ltx_align_center">2.7</td>
<td class="ltx_td ltx_align_center">51.0</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">26.8</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ImageNet</th>
<td class="ltx_td ltx_align_center">35.4</td>
<td class="ltx_td ltx_align_center">2.4</td>
<td class="ltx_td ltx_align_center">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">19.5</td>
<td class="ltx_td ltx_align_center">28.3</td>
<td class="ltx_td ltx_align_center">2.8</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">52.4</span></td>
<td class="ltx_td ltx_align_center">26.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VINCE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R2V2</th>
<td class="ltx_td ltx_align_center">23.6</td>
<td class="ltx_td ltx_align_center">1.2</td>
<td class="ltx_td ltx_align_center">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_r">18.1</td>
<td class="ltx_td ltx_align_center">19.4</td>
<td class="ltx_td ltx_align_center">1.4</td>
<td class="ltx_td ltx_align_center">47.0</td>
<td class="ltx_td ltx_align_center">24.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">FlowE (Ours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">-</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">49.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">5.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">61.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">19.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">37.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">5.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb">24.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Readout results on UrbanCity and BDD100K<span class="ltx_text ltx_font_medium">, in comparison
with competitive representation learning methods trained on other data
sources.</span></span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We first train our model on two self-driving datasets UrbanCity and
BDD100K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="BDD100K: a diverse driving dataset for heterogeneous multitask learning" class="ltx_ref">48</a>]</cite> to evaluate the quality of the learned
representations, we target semantic and instance segmentation as well as object
detection as the readout tasks using labeled images.
Ablation experiments are conducted on
UrbanCity to verify the effectiveness of each component of our model.
We further test the transferability of the learned feature by solely performing readout experiments
on the Cityscapes dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">8</a>]</cite> with models trained on
UrbanCity and BDD100K.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">We evaluate our method on the following driving video datasets containing
complex visual scenes.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item">
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">UrbanCity</span> is an in-house large-scale self-driving datasets collected by
ourselves. It contains around 15,000 video snippets where each is about 25
seconds long at 1080p and 10 fps, with a total of 3.5 million frames. Within
these, 11,580 and 1,643 images are densely labeled, for training and validation
respectively. They contain 7 instance classes and 13 semantic classes. We
uniformly sampled 256,000 frames from the videos with a 0.4 second time
interval as training frame pairs. In readout setting, we use the annotated
train and val split to perform semantic and instance segmentation tasks.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item">
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">BDD100K</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="BDD100K: a diverse driving dataset for heterogeneous multitask learning" class="ltx_ref">48</a>]</cite> is a large-scale self-driving datasets
which contains 100,000 unlabeled raw video snippets for street scene, where
each is about 40 seconds long at 720p and 30 fps. It captures different weather
conditions, including sunny, overcast and rainy, as well as different times of
day including nighttime. The class definition is the same as
Cityscapes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">8</a>]</cite> which consists of 8 instance classes for
object detection, and 19 classes in total for semantic segmentation. 7,000
train and 1,000 val images are densely labeled for semantic segmentation;
70,000 train, 10,000 val images are labeled for object detection. We use the
70,000 video snippets in the official training split to perform self-supervised
learning. At each iteration we will randomly sample two frames with 0.5 second
time interval from a video, and no further filtering or frame picking strategy
is applied. For evaluation, we use the annotated images to perform readout
experiments on semantic segmentation and object detection.
</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item">
<div id="S4.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Cityscapes</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">8</a>]</cite> is another self-driving dataset
that contains 5000 images of ego-centric driving scenarios in urban settings
which are split into 2975, 500 and 1525 for training, validation and testing
respectively. It consists of 8 instance classes and 11 semantic classes. Due to
lack of large amount of labeled data, we are interested in investigating
whether the representations learned from other source video datasets can be
transferred to a new dataset easily. Therefore we perform readout experiments
on Cityscapes using models that are pretrained on UrbanCity and BDD100K.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Competitive Methods</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">We compare to the following recent competitive methods for representation
learning from video data:
</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item">
<div id="S4.I2.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">CRW</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="Space-time correspondence as a contrastive random walk" class="ltx_ref">22</a>]</cite> is a self-supervised approach to learn
representations for visual correspondence. We use 5 frames with a 0.1 second
time interval as inputs.
</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item">
<div id="S4.I2.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">VINCE</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite> is one of the latest video
representation learning method that leverages multi-label contrastive
objective. We also train a VINCE model which is a recent proposed approach that
extends MoCo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">19</a>]</cite> and learn representation from videos. We use
inputs of 4 frames with a 0.1 second time interval.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">For fair comparison, we train these methods on our driving video datasets and
we have tried our best to search for their best hyperparameters. Additionally,
we also compare our methods with pretrained SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite> and
BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite> from ImageNet. Note that we have also tried to
apply SimCLR and BYOL on driving videos but they tend to perform very poorly
since they are designed for clean visual scenes with mostly a single object. We
therefore defer these results to the supplementary materials.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental Setup</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FlowE:</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">We use 64 GPUs with 2 video frame pairs per GPU.
LARS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="Large batch training of convolutional networks" class="ltx_ref">47</a>]</cite> optimizer is used with a cosine decay learning rate
schedule without restart&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="Sgdr: stochastic gradient descent with warm restarts" class="ltx_ref">25</a>]</cite> and an initial learning rate
0.1 with weight decay 1e-6. The setting of the exponential moving average
parameter of the target network is kept the same as the original BYOL paper.
For UrbanCity, we will randomly scale the image pairs from <span id="S4.SS3.SSS0.Px1.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.75\sim 1.25\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.75</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1.25</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span>, and randomly crop a <span id="S4.SS3.SSS0.Px1.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="512\times 1024"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1024</span></span></span></span></span></span></span> patch pair at the same
location of the two images; models are run for 160,000 iterations (80 epochs).
For BDD100K, we first upsample the images to be <span id="S4.SS3.SSS0.Px1.p1.m3" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1080\times 1920"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1080</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1920</span></span></span></span></span></span></span>, and follow
the same setting for UrbanCity; model are run for 60,000 iterations (110
epochs), and it is worth noting that the performance has not saturated and
longer iteration may yield better performance.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Readout setup:</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">For the semantic segmentation task, we train models
for 60,000 iterations of SGD with batch size 16, initial learning rate 0.02
and the “poly” learning rate decay schedule&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs" class="ltx_ref">4</a>]</cite> on both
datasets. Patches of size <span id="S4.SS3.SSS0.Px2.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="512\times 1024"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1024</span></span></span></span></span></span></span> are randomly cropped from images
which are randomly resized with shorter side from 512 to 2048.</p>
</div>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">For the instance segmentation task on
UrbanCity, we train models for 32 epochs of SGD with batch size 8 and
initial learning rate 0.01 with a decay factor 0.1 at epoch 28. Multi-scale
training is used with shorter side from 800 to 1024.</p>
</div>
<div id="S4.SS3.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">For object detection task on BDD100K we train models for 12 epochs of SGD with
mini batch size 16 and initial learning rate 0.02 with a decay factor 0.1 at
epoch 8 and 11, respectively, we keep the image resolution as it is and do not
apply multi-scale training.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Standard readout header:</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">In our readout setting, the encoder is
frozen and only the newly added layers are trained. Just like the linear
evaluation protocol on
ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="Momentum contrast for unsupervised visual representation learning" class="ltx_ref">19</a>, <a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>, <a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite>, we aim to add
as few parameters as possible. Therefore, we use DeepLab
v1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="Semantic image segmentation with deep convolutional nets and fully connected crfs" class="ltx_ref">3</a>]</cite> as our semantic segmentation model as it has no
extra heavy decoder like DeepLab V3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="Rethinking atrous convolution for semantic image segmentation" class="ltx_ref">5</a>]</cite>. Besides dilated
convolutions are used in the encoder, only one convolutional layer is added on
top of the encoder to output per-pixel classification logits.</p>
</div>
<div id="S4.SS3.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p">Similarly, for object detection on BDD100K, we use Faster R-CNN with the
ResNet-C4 architecture which is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="Deep residual learning for image recognition" class="ltx_ref">21</a>]</cite>. Only a small
number of parameters are introduced: a small convnet RPN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="Faster r-cnn: towards real-time object detection with region proposal networks" class="ltx_ref">34</a>]</cite>
and two linear layers for bounding box classification and regression.</p>
</div>
<div id="S4.SS3.SSS0.Px3.p3" class="ltx_para">
<p class="ltx_p">For instance segmentataion on UrbanCity, the same ResNet-C4 architecture is
used with two more convolutional layers added for instance mask prediction as
was done in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="Mask r-cnn" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Heavier readout header:</h4>

<div id="S4.SS3.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p">While we believe that standard readout headers should be mainly used to
evaluate the quality of representations since there are less number of extra
parameters, they may not be capable enough to capture the complex output
structure for semantic and instance segmentation. To provide a stronger
comparison, following LoCo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="LoCo: local contrastive representation learning" class="ltx_ref">46</a>]</cite>, we also perform readout with a
<span class="ltx_text ltx_font_italic">heavier header</span> such DeepLab V3 decoder and FPN-style Faster and Mask
R-CNN, where results obtained with these models are denoted with mIoU<span id="S4.SS3.SSS0.Px4.p1.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
and mAP<span id="S4.SS3.SSS0.Px4.p1.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Main Results</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results trained on UrbanCity and BDD100K:</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">The results on
UrbanCity and BDD100K are shown in Table&nbsp;<a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We
compare
FlowE with various baselines, including Random initialization (readout
from a random projection), VINCE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite> and
CRW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="Space-time correspondence as a contrastive random walk" class="ltx_ref">22</a>]</cite>; and our method is able to surpass them by a large
margin. CRW has poor performance on semantic segmentation since it focuses on
video correspondence as its training objective, and the features for different
classes of static objects will not be easily differentiated. For VINCE, we can
see that it can successfully learn some useful features from video data.
However, our method is still significantly better.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results trained on other data:</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">We also compare FlowE with methods trained on other datasets like ImageNet, including
supervised learning, SimCLR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="A simple framework for contrastive learning of visual representations" class="ltx_ref">6</a>]</cite> and BYOL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Bootstrap your own latent: a new approach to self-supervised learning" class="ltx_ref">18</a>]</cite> on ImageNet,
and VINCE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite> on R2V2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="Watching the world go by: representation learning from unlabeled videos" class="ltx_ref">16</a>]</cite>. The results are shown in
Table&nbsp;<a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We simply freeze the pre-trained model weights and perform readout
experiments on UrbanCity and BDD100K. For supervised learning baseline, we use the ResNet-50
checkpoint provided by torchvision. For SimCLR, we use our own implementation and train a model with
69.8% top-1 accuracy on ImageNet. For BYOL and VINCE, we use weights released online by the
authors. Our methods can outperform or stay on par with other strong baselines in most cases,
especially when using a standard readout header. It is worth noting that Supervised/SimCLR/BYOL are
three very strong baselines that pretrained on ImageNet, a large scale and heavily curated dataset.
Although it is not easy to beat these state-of-the-art ImageNet methods, we still manage to surpass
them on three out of the four metrics. Importantly, our framework can directly learn semantically
meaningful representation from raw video data, making it practical for real world applications
where offline curated datasets are not available at hand.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_block ltx_align_center" style="width:433.6pt;vertical-align:-0.0pt;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Pixel</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">based</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Affine</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">transform</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<table class="ltx_tabular ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Optical</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">flow</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU<span id="S4.T3.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP<span id="S4.T3.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">✓</th>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">40.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">12.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">✓</th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center">28.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.7</td>
<td class="ltx_td ltx_align_center">45.9</td>
<td class="ltx_td ltx_align_center">15.1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">✓</th>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td class="ltx_td ltx_align_center">37.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.3</td>
<td class="ltx_td ltx_align_center">51.9</td>
<td class="ltx_td ltx_align_center">16.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td class="ltx_td ltx_align_center">17.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.7</td>
<td class="ltx_td ltx_align_center">33.1</td>
<td class="ltx_td ltx_align_center">10.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">✓</th>
<td class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">37.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text ltx_font_bold">3.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">53.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">16.5</span></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation studies on different design choices.<span class="ltx_text ltx_font_medium"> Numbers show
semantic segmentation and instance segmentation readout results on
UrbanCity.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p">We perform ablation studies and show the results in Table&nbsp;<a href="#S4.T3" title="Table 3 ‣ Results trained on other data: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. All
entries are trained with 16K iterations for faster experimentation. When
bringing video data with flow matching, we can see a huge performance
improvement, indicating the importance of equivariance objective derived from
videos. For non pixel-based variant, we simply use a global average pooling
after the encoder and get vector representation. Its poor performance on the
readout tasks suggests the necessity of keeping spatial dimension of the
representation. Random affine transformation can also bring some additional
gains. Finally, our full model achieve the best performance.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Representation Transferability on Cityscapes</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p class="ltx_p">When there is limited labeled data on a new driving dataset, it is often
desirable to learn unsupervised representations from a large scale unlabeled
driving videos of another source. However, standard self-supervised method only
works on static images with few objects. Although ImageNet pretrained
checkpoints are readily available online, there may exist a large domain gap.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p class="ltx_p">In this section, we further test the transferability of the learned
representations of
FlowE by performing semantic and instance segmentation readout
experiments on the Cityscapes dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">8</a>]</cite>. Our models
are pretrained on UrbanCity and BDD100K. Following the common practice, for
instance segmentation we train 64 epochs with batch size 8, initial learning
rate 0.01 with a decay by a factor of 10 at epoch 56; for semantic
segmentation, we train 40,000 iterations with batch size 8, initial learning
rate 0.01 with “poly” learning rate decay schedule. The results are shown in
Table&nbsp;<a href="#S4.T4" title="Table 4 ‣ 4.6 Representation Transferability on Cityscapes ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The results are highly consistent with the
evaluation in Table&nbsp;<a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our method can perform better or on
par compared to ImageNet pre-training, which suggests that our method can be
seen as a better alternative way to bootstrap representations from a large
number of unlabeled videos.
We also tried to use the intermediate activation of the RAFT model that is trained on Flying Chair,
Flying Things and Sintel (C+T+S) for semantic readout evaluation. Specifically, we use the RAFT
feature encoder as the backbone to replace ResNet-50, and add a DeepLab v1/v3 decoder as a
standard/heavier header for semantic segmentation readout. The results clearly show that the
representations from the optical flow model does not contain rich semantic information</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div class="ltx_block ltx_align_center" style="width:433.6pt;vertical-align:-0.0pt;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Train data</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mIoU<span id="S4.T4.m1" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP<span id="S4.T4.m2" class="ltx_Math"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="{}^{\dagger}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">†</span></span></span></span></span></span></span></span></span></span></span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Supervised</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ImageNet</th>
<td class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">59.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">25.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SimCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ImageNet</th>
<td class="ltx_td ltx_align_center">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.0</td>
<td class="ltx_td ltx_align_center">60.3</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">28.9</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BYOL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ImageNet</th>
<td class="ltx_td ltx_align_center">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_r">4.1</td>
<td class="ltx_td ltx_align_center">59.8</td>
<td class="ltx_td ltx_align_center">27.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VINCE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">R2V2</th>
<td class="ltx_td ltx_align_center">26.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.1</td>
<td class="ltx_td ltx_align_center">57.5</td>
<td class="ltx_td ltx_align_center">25.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RAFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">C+T+S</th>
<td class="ltx_td ltx_align_center">10.5</td>
<td class="ltx_td ltx_align_center ltx_border_r">-</td>
<td class="ltx_td ltx_align_center">32.4</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FlowE (Ours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BDD100K</th>
<td class="ltx_td ltx_align_center">45.6</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.7</td>
<td class="ltx_td ltx_align_center">56.6</td>
<td class="ltx_td ltx_align_center">25.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">FlowE (Ours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">UrbanCity</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">51.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text ltx_font_bold">7.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">63.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">28.1</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Readout results on Cityscapes<span class="ltx_text ltx_font_medium"> with representations learned
from other datasets. </span></span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/mask_rcnn_xenon/0a34ff3e-a17f-37a3-ab3b-1ccf1e718a9c.jpg" id="S4.F4.g1" class="ltx_graphics" width="252" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/faster_rcnn_bdd/b251064f-8d92db81.jpg" id="S4.F4.g2" class="ltx_graphics" width="236" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/deeplabv1_cityscapes/munster_000038_000019_leftImg8bit.jpg" id="S4.F4.g3" class="ltx_graphics" width="265" height="133" alt=""></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/mask_rcnn_xenon/0b34ee02-06de-7bbe-f127-7f6e3ad7fb1f.jpg" id="S4.F4.g4" class="ltx_graphics" width="252" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/faster_rcnn_bdd/b291cfd6-a6275049.jpg" id="S4.F4.g5" class="ltx_graphics" width="236" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/deeplabv1_cityscapes/munster_000000_000019_leftImg8bit.jpg" id="S4.F4.g6" class="ltx_graphics" width="265" height="133" alt=""></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/mask_rcnn_xenon/2ad472fd-617e-32ef-6a6d-5d709535a8dd.jpg" id="S4.F4.g7" class="ltx_graphics" width="252" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/faster_rcnn_bdd/b216243d-55963da2.jpg" id="S4.F4.g8" class="ltx_graphics" width="236" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/deeplabv1_cityscapes/frankfurt_000000_009969_leftImg8bit.jpg" id="S4.F4.g9" class="ltx_graphics" width="265" height="133" alt=""></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/mask_rcnn_xenon/04f4f05c-c728-85e2-eb8c-74048324976e.jpg" id="S4.F4.g10" class="ltx_graphics" width="252" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/faster_rcnn_bdd/b23f7012-32d284ce.jpg" id="S4.F4.g11" class="ltx_graphics" width="236" height="133" alt=""></td>
<td class="ltx_td ltx_align_center"><img src="figure/vis_results/deeplabv1_cityscapes/munster_000003_000019_leftImg8bit.jpg" id="S4.F4.g12" class="ltx_graphics" width="265" height="133" alt=""></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Readout visualization.<span class="ltx_text ltx_font_medium"> Left: Instance segmentation on
UrbanCity; middle: object detection on BDD100K; right: semantic
segmentation on Cityscapes. For
UrbanCity and BDD100K, models are trained on the corresponding dataset with
heavier readout headers. For Cityscapes, we use a UrbanCity pretrained
model with a standard readout header. </span></span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">% of labels</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">100%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">End-to-end supervised</th>
<td class="ltx_td ltx_align_center ltx_border_t">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">59.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">63.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">FlowE (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">53.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">68.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Semantic segmentation results with limited labeled data on UrbanCity.<span class="ltx_text ltx_font_medium">
We compare FlowE with the end-to-end supervised baseline. Our model is
pretrained on unlabeled videos and then finetuned on the labeled data. </span></span></figcaption>
</figure>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Learning with limited labeled data</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p class="ltx_p">Another very practical setting is semi-supervised learning, where a large
video dataset is recorded but only a very small portion of the dataset
is annotated.
To investigate whether our algorithm can reduce the reliance on labeled data,
we randomly subsample 1%, 10% labeled data from UrbanCity, and finetune our
pretrained models on the supervised task of semantic segmentation. We compare
it with the end-to-end supervised learning baseline. As shown in Table&nbsp;<a href="#S4.T5" title="Table 5 ‣ 4.6 Representation Transferability on Cityscapes ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, pretraining on unlabeled video data can
significantly boost the performance when the labels are scarce, and pretraining
is still beneficial even when using 100% of the labeled data.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Visualization</h3>

<div id="S4.SS8.p1" class="ltx_para">
<p class="ltx_p">We show the visualization results of instance segmentation on UrbanCity,
object detection on BDD100K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="BDD100K: a diverse driving dataset for heterogeneous multitask learning" class="ltx_ref">48</a>]</cite>, and semantic segmentation on
Cityscapes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="The cityscapes dataset for semantic urban scene understanding" class="ltx_ref">8</a>]</cite> in Fig.&nbsp;<a href="#S4.F4" title="Figure 4 ‣ 4.6 Representation Transferability on Cityscapes ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For
UrbanCity and BDD100K, models are trained on the corresponding dataset with
heavier readout headers. For Cityscapes, we use a UrbanCity pretrained
model with the standard readout header, which only has a simple linear
classification layer on all pixels. Our model can produce impressive results
for these segmentation tasks.</p>
</div>
</section>
<section id="S4.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span>Limitations</h3>

<div id="S4.SS9.p1" class="ltx_para">
<p class="ltx_p">We observe that when using heavier readout headers instead of standard readout
headers, the models trained on ImageNet are able to catch up with our model. We
notice that in these cases our method performs much worse on instance classes
like <span class="ltx_text ltx_font_italic">rider</span> and <span class="ltx_text ltx_font_italic">motorcycle</span>, which are usually rare in the datasets.
This might caused by the data imbalance when using our pixel-based objective,
whereas ImageNet has a balanced distribution across semantic classes. Solely
relying on equivariance objective and lack of invariance objectives may also
sacrifice some higher level representations, since when using heavier readout
headers, our method does not improve as much as ImageNet pretrained models do.

</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this paper, we present a new self-supervised representation learning
framework based on a flow equivariance objective. Our method is able to learn
pixel-level representations from raw high-resolution videos with complex
scenes. Large scale experiments on driving videos suggest that our unsupervised
representations are useful for object detection, semantic and instance
segmentation, and in many cases outperform state-of-the-art representations
obtained from ImageNet.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Agrawal, J. Carreira, and J. Malik</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to see by moving</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;37–45</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black</span><span class="ltx_text ltx_bib_year"> (2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A naturalistic open source movie for optical flow evaluation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;611–625</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS3.SSS0.Px2.p1" title="Flow network: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semantic image segmentation with deep convolutional nets and fully connected crfs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1412.7062</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS0.Px1.p1" title="Network architecture: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>,
<a href="#S4.SS3.SSS0.Px3.p1" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE transactions on pattern analysis and machine intelligence</span> <span class="ltx_text ltx_bib_volume">40</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;834–848</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.SS3.SSS0.Px2.p1" title="Readout setup: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Chen, G. Papandreou, F. Schroff, and H. Adam</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking atrous convolution for semantic image segmentation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1706.05587</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS3.SSS0.Px3.p1" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Chen, S. Kornblith, M. Norouzi, and G. Hinton</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A simple framework for contrastive learning of visual representations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2002.05709</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Self-Supervised Representation Learning from Flow Equivariance</span></span>,
<a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS1.SSS0.Px1.p2" title="Bootstrap your own latent (BYOL): ‣ 3.1 Background ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S3.SS2.SSS0.Px1.p1" title="Optical flow &amp; random affine transformation: ‣ 3.2 Learning from Flow Equivariance ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a href="#S3.SS3.SSS0.Px3.p1" title="Data augmentation: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>,
<a href="#S4.SS2.p2" title="4.2 Competitive Methods ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#S4.SS3.SSS0.Px3.p1" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>,
<a href="#S4.SS4.SSS0.Px2.p1" title="Results trained on other data: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.4</span></a>,
<a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Cohen and M. Welling</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Group equivariant convolutional networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International conference on machine learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2990–2999</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The cityscapes dataset for semantic urban scene understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;3213–3223</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.I1.i2.p1" title="2nd item ‣ 4.1 Datasets ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a href="#S4.I1.i3.p1" title="3rd item ‣ 4.1 Datasets ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a href="#S4.SS6.p2" title="4.6 Representation Transferability on Cityscapes ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.6</span></a>,
<a href="#S4.SS8.p1" title="4.8 Visualization ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.8</span></a>,
<a href="#S4.p1" title="4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Dai, Y. Li, K. He, and J. Sun</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">R-fcn: object detection via region-based fully convolutional networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in neural information processing systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;379–387</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Doersch, A. Gupta, and A. A. Efros</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised visual representation learning by context prediction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1422–1430</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Flownet: learning optical flow with convolutional networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2758–2766</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS3.SSS0.Px2.p1" title="Flow network: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Dwibedi, Y. Aytar, J. Tompson, P. Sermanet, and A. Zisserman</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Temporal cycle-consistency learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1801–1810</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Dwibedi, Y. Aytar, J. Tompson, P. Sermanet, and A. Zisserman</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Counting out time: class agnostic video repetition counting in the wild</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;10387–10396</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Gidaris, P. Singh, and N. Komodakis</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised representation learning by predicting image rotations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1803.07728</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Girshick</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast r-cnn</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1440–1448</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Gordon, K. Ehsani, D. Fox, and A. Farhadi</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Watching the world go by: representation learning from unlabeled videos</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2003.07990</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.I2.i2.p1" title="2nd item ‣ 4.2 Competitive Methods ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a href="#S4.SS4.SSS0.Px1.p1" title="Results trained on UrbanCity and BDD100K: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.4</span></a>,
<a href="#S4.SS4.SSS0.Px2.p1" title="Results trained on other data: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.4</span></a>,
<a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Goyal, D. Mahajan, A. Gupta, and I. Misra</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scaling and benchmarking self-supervised visual representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;6391–6400</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bootstrap your own latent: a new approach to self-supervised learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2006.07733</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Self-Supervised Representation Learning from Flow Equivariance</span></span>,
<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS2.SSS0.Px1.p1" title="Optical flow &amp; random affine transformation: ‣ 3.2 Learning from Flow Equivariance ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a href="#S3.SS3.SSS0.Px1.p1" title="Network architecture: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>,
<a href="#S4.SS2.p2" title="4.2 Competitive Methods ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#S4.SS3.SSS0.Px3.p1" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>,
<a href="#S4.SS4.SSS0.Px2.p1" title="Results trained on other data: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.4</span></a>,
<a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Momentum contrast for unsupervised visual representation learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;9729–9738</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.I2.i2.p1" title="2nd item ‣ 4.2 Competitive Methods ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a href="#S4.SS3.SSS0.Px3.p1" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. He, G. Gkioxari, P. Dollár, and R. Girshick</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mask r-cnn</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2961–2969</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.SS3.SSS0.Px3.p3" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. He, X. Zhang, S. Ren, and J. Sun</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep residual learning for image recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;770–778</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.SS3.SSS0.Px3.p2" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Jabri, A. Owens, and A. A. Efros</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Space-time correspondence as a contrastive random walk</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2006.14613</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.I2.i1.p1" title="1st item ‣ 4.2 Competitive Methods ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a href="#S4.SS4.SSS0.Px1.p1" title="Results trained on UrbanCity and BDD100K: ‣ 4.4 Main Results ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.4</span></a>,
<a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Krizhevsky, I. Sutskever, and G. E. Hinton</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Imagenet classification with deep convolutional neural networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Communications of the ACM</span> <span class="ltx_text ltx_bib_volume">60</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;84–90</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M. Yang</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Joint-task self-supervised learning for temporal correspondence</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;318–328</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Loshchilov and F. Hutter</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sgdr: stochastic gradient descent with warm restarts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1608.03983</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS3.SSS0.Px1.p1" title="FlowE: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;4040–4048</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS3.SSS0.Px2.p1" title="Flow network: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Nordberg and G. Granlund</span><span class="ltx_text ltx_bib_year"> (1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Equivariance and invariance-an approach based on lie groups</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of 3rd IEEE International Conference on Image Processing</span> <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;181–184 vol.3</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Noroozi and P. Favaro</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised learning of visual representations by solving jigsaw puzzles</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;69–84</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Noroozi, H. Pirsiavash, and P. Favaro</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Representation learning by learning to count</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;5898–5906</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. v. d. Oord, Y. Li, and O. Vinyals</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Representation learning with contrastive predictive coding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1807.03748</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. E. Orhan, V. V. Gupta, and B. M. Lake</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Self-supervised learning through the eyes of a child</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2007.16189</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Pirk, M. Khansari, Y. Bai, C. Lynch, and P. Sermanet</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Online learning of object representations by appearance space feature alignment</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, and M. J. Black</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;12240–12249</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Ren, K. He, R. Girshick, and J. Sun</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Faster r-cnn: towards real-time object detection with region proposal networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in neural information processing systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;91–99</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.SS3.SSS0.Px3.p2" title="Standard readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Time-contrastive networks: self-supervised learning from video</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2018 IEEE International Conference on Robotics and Automation (ICRA)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;1134–1141</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Simonyan and A. Zisserman</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Very deep convolutional networks for large-scale image recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1409.1556</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Sundaram, T. Brox, and K. Keutzer</span><span class="ltx_text ltx_bib_year"> (2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dense point trajectories by gpu-accelerated large displacement optical flow</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;438–451</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS0.Px4.p1" title="Flow post-processing: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Teed and J. Deng</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RAFT: recurrent all-pairs field transforms for optical flow</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2003.12039</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.SSS0.Px2.p1" title="Warping via optical flow: ‣ 3.1 Background ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a href="#S3.SS3.SSS0.Px2.p1" title="Flow network: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Thewlis, H. Bilen, and A. Vedaldi</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised learning of object frames by dense equivariant image labelling</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems 30</span>,  <span class="ltx_text ltx_bib_editor">I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;844–855</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://papers.nips.cc/paper/6686-unsupervised-learning-of-object-frames-by-dense-equivariant-image-labelling.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Tian, D. Krishnan, and P. Isola</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contrastive multiview coding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1906.05849</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Wang, A. Kläser, C. Schmid, and C. Liu</span><span class="ltx_text ltx_bib_year"> (2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Action recognition by dense trajectories</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR 2011</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;3169–3176</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Wang and C. Schmid</span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Action recognition with improved trajectories</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;3551–3558</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wang and A. Gupta</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised learning of visual representations using videos</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2794–2802</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wang, A. Jabri, and A. A. Efros</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning correspondence from the cycle-consistency of time</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2566–2576</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Worrall and G. Brostow</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cubenet: equivariance to 3d rotation and translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the European Conference on Computer Vision (ECCV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;567–584</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Xiong, M. Ren, and R. Urtasun</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LoCo: local contrastive representation learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2008.01342</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS3.SSS0.Px4.p1" title="Heavier readout header: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. You, I. Gitman, and B. Ginsburg</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Large batch training of convolutional networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1708.03888</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS3.SSS0.Px1.p1" title="FlowE: ‣ 4.3 Experimental Setup ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell</span><span class="ltx_text ltx_bib_year"> (2020-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BDD100K: a diverse driving dataset for heterogeneous multitask learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S4.I1.i2.p1" title="2nd item ‣ 4.1 Datasets ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a href="#S4.SS8.p1" title="4.8 Visualization ‣ 4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.8</span></a>,
<a href="#S4.p1" title="4 Experiments ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Zhang, P. Isola, and A. A. Efros</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Colorful image colorization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;649–666</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pyramid scene parsing network</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2881–2890</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S3.SS3.SSS0.Px1.p1" title="Network architecture: ‣ 3.3 Implementation Details ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.3</span></a>.
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Flow-guided feature aggregation for video object detection</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;408–417</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS1.SSS0.Px2.p1" title="Warping via optical flow: ‣ 3.1 Background ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep feature flow for video recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2349–2358</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S3.SS1.SSS0.Px2.p1" title="Warping via optical flow: ‣ 3.1 Background ‣ 3 Methodology ‣ Self-Supervised Representation Learning from Flow Equivariance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>


<script src="index.js" type="text/javascript"></script></body></html>