% !TEX root = ../main.tex
\section{Dataset Details}
\label{app:data}

\subsection{Benchmark comparison}
We include Table~\ref{tab:benchmark} to compare existing continual and few-shot learning paradigms.

\subsection{\ourchar{} \& \ourimg{} Sampler Details}
For the \ourchar{} and the \ourimg{} experiments, we use sequences with maximum 150 images, from 5 environments. For
individual environment, we use a Chinese restaurant process to sample the class distribution. In
particular, the probability of sampling a new class is:
\begin{align}
p_\text{new} = \frac{k \alpha + \theta}{m + \theta},
\end{align}
where $k$ is the number of classes that we have already sampled in the environment, and $m$ is the
total number of instances we have in the environment. $\alpha$ is set to 0.2 and $\theta$ is set to
1.0 in all experiments.

The environment switching is implemented by a Markov switching process. At each step in the
sequence there is a constant probability $p_\text{switch}$ that switches to another environment. For
all experiments, we set $p_\text{switch}$ to 0.2. We truncate the maximum number of appearances
per class to 6. If the maximum appearance is reached, we will sample another class.

\subsection{Metrics}
\label{sec:metrics}
\paragraph{Average precision:} We chose to use AP (average precision or area under the precision-recall curve) as a way of integrating two aspects of performance:

\begin{enumerate}
    \item the binary accuracy of whether an instance belongs to a known or unknown class (KU-Assign for short), and
    \item the accuracy of assigning an instance the correct class label given it is from a known class (Class-Assign for short).
\end{enumerate}
The procedure to calculate AP is as follows. We first sort all the {KU-Assign, Class-Assign} predictions across all sequences in descending order based on KU-Assign probability, where the high ranked predictions should be known (not novel) classes. For the N top ranked instances in the sorted list, we compute:
\begin{enumerate}
    \item precision@N = correct(Class-Assign)@N / N
    \item recall@N = correct(Class-Assign)@N / K,
\end{enumerate}
where K is the true number of known instances and correct(Class-Assign)@N is the count of the number of correct class assignments among the top N. (The class assignment for an unknown instance is always incorrect.) To obtain the AP, we compute the integral of the function (y=precision@N, x=recall@N) across all Nâ€™s.

\paragraph{N-shot accuracy:} We define N-shot accuracy as the number of times an instance that has been seen N times thus far in the sequence is classified correctly. We compute the mean and standard error of this over all sequences.

\subsection{Additional \ourroom{} Statistics} 
Statistics of the \ourroom{} are included in Table~\ref{tab:dataset_stats}, in comparison to other
few-shot and continual learning datasets. Note that since \ourroom{} is collected from a simulated
environment, with 90 indoor worlds consisting of 1.2K panorama images and 1.22M video frames. The
dataset contains about 6.9K random walk sequences with a maximum of 200 frames per sequence. For
training we randomly crop 100 frames to form a training sequence. There are 7.0K unique instance
classes.

Plots of additional statistics of \ourroom{} are shown in Figure~\ref{fig:additionalstats}. In
addition to the ones shown in the main paper, instances and viewpoints also follow long tail
distributions. The number of objects in each frame follows an exponential distribution.

\input{tables/dataset_stats}

\subsection{\ourroom{} Simulator Details}
We generate our episodes with a two-stage process using two simulators -- HabitatSim~\citep{habitat}
and MatterSim~\citep{mattersim} -- because HabitatSim is based on 3D meshes and using HabitatSim
alone will result in poor image quality due to incorrect mesh reconstruction. Therefore we
sacrificed the continuous movement of agents within HabitatSim and base our environment navigation
on the discrete viewpoints in MatterSim, which is based on real panoramic images. The horizontal
field of view is set to 90 degrees for HabitatSim and 100 degrees for MatterSim, and we simulate
with\ 800$\times$600 resolution.

The first stage of generation involves randomly picking a sequence of viewpoints on the connectivity
graph within MatterSim. For each viewpoint, the agent scans the environment along the yaw and pitch
axes for a random period of time until a navigable viewpoint is within view. The time spent in a
single viewpoint follows a Gaussian distribution with mean 5.0 and standard deviation 1.0. At the
start of each new viewpoint, the agent randomly picks a direction to rotate and takes 12.5 degree
steps along the yaw axis, and with 95\% probability, a 5 degree rotation along the pitch axis is
applied in a randomly chosen direction. When a navigable viewpoint is detected, the agent will
navigate to the new viewpoint and reset the direction of scan. When multiple navigable viewpoints
are present, the agent uniformly samples one.

In the second stage, an agent in HabitatSim retraces the viewpoint path and movements of the first
stage generated by MatterSim, collecting mesh-rendered RGB and instance segmentation sensor data.
The MatterSim RGB and HabitatSim RGB images are then aligned via FLANN-based feature matching
~\citep{muja2009flann}, resulting in an alignment matrix that is used to place the MatterSim RGB and
HabitatSim instance segmentation maps into alignment. The sequence of these MatterSim RGB and
HabitatSim instance segmentation maps constitute an episode.

We keep objects of the following categories: \texttt{picture, chair, lighting, cushion, table,
plant, chest of drawers, towel, sofa, bed, appliances, stool, tv monitor, clothes, toilet,
fireplace, furniture, bathtub, gym equipment, blinds, board panel}. We initially generate 600 frames
per sequence and remove all the frames with no object. Then we store every 200 image frames into a
separate file.

During training and evaluation, each video sequence is loaded, and for each image we go through each
object present in the image. We create the attention map using the segmentation groundtruth of the
selected object. The attention map and the image together form a \textit{frame} in our model input.
For training, we randomly crop 100 frames from the sequence, and for evaluation we use the first 100
frames for deterministic results.

Please visit our released code repository to download the \ourroom{} dataset.

\input{tables/benchmark_compare}
\subsection{Semi-supervised Labels:}
Here we describe how we sample the labeled vs. unlabeled flag for each example in the
semi-supervised sequences in both \ourchar{} and \ourroom{} datasets. Due to the imbalance in our
class distribution (from both the Chinese restaurant process and real data collection), directly
masking the label may bias the model to ignore the rare seen classes. Ideally, we would like to
preserve at least one labeled example for each class. Therefore, we designed the following
procedure.

First, for each class $k$, suppose $m_k$ is the number of examples in the sequence that belong to
the class. Let $\alpha$ be the target label ratio. Then the class-specific label ratio $\alpha_k$
is:
\begin{align}
\alpha_k = (1 - \alpha) \exp(-0.5 (m_k - 1)) + \alpha.
\label{eq:semisup}
\end{align}
We then for each class $k$, we sample a binary Bernoulli sequence based on $\Ber(\alpha_k)$. If a
class has all zeros in the Bernoulli sequence, we flip the flag of one of the instances to 1 to make
sure there is at least one labeled instance for each class.
For all experiments, we set $\alpha = 0.3$.

\subsection{Dataset Splits}
We include details about our dataset splits in Table~\ref{tab:omniglotsplit} and
\ref{tab:matterportsplit}.

\section{Experiment Details}
\label{app:exp}
\subsection{Network Architecture}
For the \ourchar{} experiment we used the common 4-layer CNN for few-shot learning with 64 channels
in each layer, resulting in a 64-d feature vector~\citep{protonet}. For the \ourroom{} experiment we
resize the input to 120$\times$160 and we use the ResNet-12 architecture~\citep{tadam} with
\{32,64,128,256\} channels per block. To represent the feature of the input image with an attention
mask, we concatenate the global average pooled feature with the attention ROI feature, resulting in
a 512d feature vector. For the contextual RNN, in both experiments we used an LSTM~\citep{lstm} with
a 256d hidden state. 

We use a linear layer to map from the output of the RNN to the features and control variables. We
obtain $\gamma^{r,w}$ by adding 1.0 to the linear layer output and then applying the softplus
activation. The bias units for $\beta^{r,w}$ are initialized to 10.0. We
also apply the softplus activation to $\bmm$ from the linear layer output.

\input{tables/table_omniglot_split}
\input{tables/table_matterport_split}

\begin{figure}[t]
\vspace{-0.2in}
\centering
\iflatexml
\includegraphics[width=6\textwidth]{figures/statsfull.png}
\else
\includegraphics[width=0.9\textwidth,trim={0cm 7cm 10.2cm 0cm},clip]{figures/statsfull.pdf}
\fi
\caption{Additional statistics about our \ourroom{} dataset.}
\label{fig:additionalstats}
\end{figure}


\subsection{Training Procedure}
We use the Adam optimizer~\citep{adam} for all of our experiments, with a gradient cap of 5.0. For
\ourchar{} we train the network for 40k steps with a batch size 32 and maximum sequence length 150
across 2 GPUs and an initial learning rate 2e-3 decayed by 0.1$\times$ at 20k and 30k steps. For
\ourroom{} we train for 20k steps with a batch size 8 and maximum sequence length 100 across 4 GPUs
and an initial learning rate 1e-3 decayed by 0.1$\times$ at 8k and 16k steps. We use the BCE coefficient
$\lambda=1$  for all experiments. In semi-supervised experiments, around 30\% examples are labeled when the number of examples grows large ($\alpha = 0.3$, see Equation~\ref{eq:semisup}). Early stopping is used in \ourroom{} experiments
where the checkpoint with the highest validation AP score is chosen.
For \ourroom{}, we sample Bernoulli sequences on unlabeled inputs to 
gradually allow semi-supervised writing to the prototype memory and we find it helps training stability. The probability starts with 0.0 and increase by 0.2 every 2000 training steps until reaching 1.0.

\subsection{Data Augmentation}
For \ourchar{}, we pad the 28$\times$28 image to 32$\times$32 and then apply random cropping.

For \ourroom{}, we apply random cropping in the time dimension to get a chunk of 100 frames per
input example. We also apply random dropping of 5\% of the frames. We pad the 120$\times$160 images
to 126 $\times$ 168 and apply random cropping in each image frame. We also randomly flip the order
of the sequence (going forward or backward).


\subsection{Spatiotemporal context experiment details}
We use the Kylberg texture dataset~\citep{uppsala} without rotations. Texture classes are split into train, val, and test, defined in Table~\ref{tab:uppsalasplit}. We resize all images first to 256$\times$256. For each Omniglot image, a 28$\times$28 patch is randomly cropped from a texture image to serve as background. Random Gaussian noises with mean zero and standard deviation 0.1 are added to the background images.

For spatial background experiments, we added an additional learnable network of the same size as the main network to take the background image as input, and output the same sized embedding vector. This embedding vector is further concatenated with the main embedding vector to form the final embedding of the input. We also found that using spatially overlayed images with a single CNN can achieve similar performance as well. The final numbers are reported using the concatenation approach since it is less prone to overlay noises and is more similar to the implementation we use in the RoamingRooms experiments.

\input{tables/table_uppsala}

\subsection{Baseline implementation details}
\paragraph{Online meta-learning (OML):} The OML  model performs one gradient descent step for each
input. In order for the model to predict unknown, we use the probability output from the softmax
layer summing across the unused units. For example, if the softmax layer has 40 units and we have
only seen 5 classes so far, then we sum the probability from the 6th to the last units. This summed
probability is separately trained with a binary cross entropy, same as in Equation~\ref{eq:loss}.

The inner learning rate is set to 1e-2 and we truncate the number of unrolled gradient descent steps
to 5/20 (\ourchar{}/\ourroom{}), in order to make the computation feasible. For \ourchar{}, the
network is trained with a batch size 32 across 2 GPUs, for a total of 20k steps, with an initial
learning rate 2e-3 decayed by 0.1 at 10k and 16.7k steps. For \ourroom{}, the network is trained
with a batch size 8 across 4 GPUs, for a total of 16k steps, with an initial learning rate 1e-3
decayed by 0.1 at 6.4k and 12.8k steps.

\paragraph{Long short-term memory (LSTM):} We apply a stacked two layer LSTM with 256 hidden
dimensions. Inputs are $\bh_t^{\text{CNN}}$ concatenated with the label one-hot vector. If an
example is unlabeled, then the label vector is all-zero. We directly apply a linear layer on top of
the LSTM to map the LSTM memory output into classification logits, and the last logit is the binary
classification logit reserved for unknown. The training procedure is the same as our CPM model.

\paragraph{Differentiable neural computer (DNC):} In order to make the DNC model work properly, we
found that it is sometimes helpful to pretrain the CNN weights. Simply initializing from scratch and
train CNN+DNC end-to-end sometimes results in poor performance. We hypothesize that the attention
structure in the DNC model is detrimental to representation learning. Therefore, for \ourchar{}
experiments, we use pretrained ProtoNet weights for solving 1-shot 5-way episodes to initialize the
CNN, and we keep finetuning the CNN weights with 10\% of the full learning rate. For \ourroom{}
experiments, we train the full model end-to-end from scratch.

The DNC is also modified so that it is more effective using the label information from the input. In
the original MANN paper~\citep{mann} for one-shot learning, the input features $\bh_t^{\text{CNN}}$
and the label one-hot ID are simply concatenated to feed into the LSTM controller of MANN. We find
that it is beneficial to directly add label one-hot vector as an input to the write head that
generates the write attention and the write content.  Similar to the LSTM model, the memory readout is also sent to a
linear layer in order to get the final classification logits, and the last logit is the binary
classification logit reserved for the unknowns. Finally we remove the linkage prediction part of the DNC
due to training instability.

The controller LSTM has 256 hidden dimensions, and the memory has 64 slots each with 64 dimensions.
There are 4 read heads and 4 write heads. The training procedure is the same as CPM.

\paragraph{Online ProtoNet:} Online ProtoNet is our modification of the original
ProtoNet~\citep{protonet}. It is similar to our CPM model without the contextual RNN. The feature
from the CNN is directly written to the prototype memory. In addition, we do not predict the control
hyperparameters $\beta^{\{r,w\}}_t,\gamma^{\{r,w\}}_t$ from the RNN and they are learned as regular parameters. The training procedure is the same as CPM.

\paragraph{Online MatchingNet:} Online MatchingNet is our modification of the original
MatchingNet~\citep{matchingnet}. We do not consider the context embedding in the MatchingNet paper
since it was originally designed for the entire episode using an attentional RNN encoder. It is
similar to online ProtoNet but instead of doing online averaging, it directly stores each example
and its class. Since it is an example-based storage, we did not extend it to learn from
unlabeled examples, and all unlabeled examples are skipped. We use a similar decision rule to
determine whether an example belongs to a known cluster by looking at the distance to the nearest
exemplar stored in the memory, shifted by $\beta$ and scaled by $1/\gamma$. Note that online
MatchingNet is not efficient at memory storage since it scales with the number of steps in the
sequence. In addition, we use the negative Euclidean distance as the similarity function. The training
procedure is the same as CPM.

\paragraph{Online infinite mixture prototypes (IMP):} Online IMP is proposed as a mix of prototype and example-based storage by allowing a class to have multiple clusters. If an
example is classified as unknown or it is unlabeled, we will assign its cluster based on our
prediction, which either assigns it to one of the existing clusters or creates a new cluster,
depending on its distance to the nearest cluster. If a cluster with an unknown label later is
assigned with an example with a known class, then the cluster label will also be updated. We use the same
decision rule as online ProtoNet to determine whether an example belongs to a known cluster by
looking at the distance to the nearest cluster, shifted by $\beta$ and scaled by $1/\gamma$. As
described above, online IMP has the capability of learning from unlabeled examples, unlike online
MatchingNet. However similar to online MatchingNet, online IMP is also not efficient at memory
storage since in the worst case it also scales with the number of steps in the sequence. Again, the
training procedure is the same as CPM.

\section{Additional Experimental Results}
\label{sec:additionalresults}
\subsection{Effect of Forgetting}
We report the effect of forgetting of \ourroom{} and \ourimg{} in Table~\ref{tab:forgetroom} and \ref{tab:forgetimagenet}.
\input{tables/effect_forget_more}

% \subsection{Video Visualization}
% We include video visualization of \ourroom{} sequences here:
% \footnote{\url{https://drive.google.com/drive/folders/1gBJBFdNb0EOvK6CEYKxIL1Og0_jrqrbK}}. Our CPM
% model prediction can be found here:
% \footnote{\url{https://drive.google.com/drive/folders/1rp9xxAccrZyffngFdtoS9Bl6P9uxJ9xN}}.

\subsection{Embedding Visualization} 
Figure~\ref{fig:tsne} shows the learned embedding of each example in Online ProtoNet vs. our CPM
model in \ourchar{} sequences, where colors indicate environment IDs. In Online ProtoNet, the
example features does not reflect the temporal context, and as a result, colors are scattered across
the space. By contrast, in the CPM embedding visualization, colors are clustered together and we see
a smoother transition of environments in the embedding space.

\input{sections/fig_tsne}

\subsection{Control Parameters vs. Time}
Finally we visualize the control parameter values predicted by the RNN in
Figure~\ref{fig:betagamma}. We verify that we indeed need two sets of $\beta$ and $\gamma$ for read
and write operations separately as they learn different values. $\beta^w$ is smaller than $\beta^r$
which means that the network is more conservative when writing to prototypes. $\gamma^w$ grows
larger over time, which means that the network prefers a softer slope when writing to prototypes
since in the later stage the prototype memory has already stored enough content and it can grow
faster, whereas in the earlier stage, the prototype memory is more conservative to avoid embedding
vectors to be assigned to wrong clusters.

\input{sections/fig_betagamma_time}
