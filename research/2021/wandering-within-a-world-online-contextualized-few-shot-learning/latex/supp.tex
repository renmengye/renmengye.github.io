\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{iclr2021_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{enumitem}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{sidecap}

\input{sections/defs}

\newcommand{\MR}[1]{{\color{orange}MR: #1}}
% \newcommand{\MRedit}[1]{{\color{orange} [#1]}}
\newcommand{\MRedit}[1]{{\color{orange} #1}}
\newcommand{\MCM}[1]{{\color{blue} #1}}
\newcommand{\RZ}[1]{{\color{magenta}RZ: #1}}

\title{
% Continual Few-Shot Learning with \\Prototypical Contextual Memory Networks
% A Wandering Few-Shot Learner with Temporal Contextual Memory
% Roaming Room to Room: \\Online Contextualized Few-Shot Learning
Wandering Within a World: \\Online Contextualized Few-Shot Learning \\
Supplementary Materials
}

%% NOTE: when we make an arxiv version, please uncomment acknowledgement section below
\author{
  David S.~Hippocampus\\
  \thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu}
}

\begin{document}

\maketitle

\begin{abstract}
    In this document we include additional dataset statistics and experimental details. We also include additional visualization to better understand the embeddings and control parameters learned by our CPM model. Note that separate video visualizations can be found in the supplementary folder.
\end{abstract}

% \section{Dataset Statistics}

% \paragraph{\ourchar{} details:}
% For the \ourchar experiments, we use sequences with maximum 150 images, from 5 environments.
% For individual environment, we use a Chinese restaurant process to sample the class distribution. In particular, the probability of sampling a new class is:
% \begin{align}
% p_\text{new} = \frac{k \alpha + \theta}{m + \theta},
% \end{align}
% where $k$ is the number of classes that we have already sampled in the environment, and $m$ is the
% total number of instances we have in the environment. $\alpha$ is set to 0.2 and $\theta$ is set to
% 1.0 for all of our experiments.

% The switching between environments is implemented by a Markov switching process. At each step in the
% sequence there is a constant probability $p_\text{switch}$ that switch to another environment. For
% all the experiments, we set $p_\text{switch}$ to 0.2.

% \paragraph{Additional \ourroom{} statistics:} 
% Statistics of the \ourroom{} is included in Table~\ref{tab:dataset_stats}, in comparison to other
% few-shot and continual learning datasets. Note that since \ourroom{} is collected from a simulated
% environment, with 90 indoor worlds consists of 1.2K panorama images and 1.22M video frames. The
% dataset contains about 6.9K random walk sequences with a maximum of 200 frames per sequence. For
% training we randomly crop 100 frames to form a training sequence. There are 7.0K unique instance
% classes.

% Plots of additional statistics of \ourroom{} are shown in Figure~\ref{fig:additionalstats}. In
% addition to the ones shown in the main paper, the instance and viewpoint also follows long tail
% distributions. The number of objects in each frame follow an exponential distribution.

% \paragraph{Semisupervised labels:}
% Here we describe how we sample the labeled vs. unlabeled flag for each example in the semisupervised
% sequences in both \ourchar{} and \ourroom{} datasets. Due to the imbalance in our class distribution
% (from both the Chinese restaurant process and real data collection), directly masking the label may
% bias the model to ignore the rare seen classes. Ideally, we would like to preserve at least one
% labeled example for each class. Therefore, we designed the following procedure.

% First, for each class $k$, suppose $m_k$ is the number of examples in the sequence that belong to
% the class. Let $\alpha$ be the target label ratio. Then the class-specific label ratio $\alpha_k$
% is:
% \begin{align}
% \alpha_k = (1 - \alpha) \exp(-0.5 (m_k - 1)) + \alpha.
% \label{eq:semisup}
% \end{align}
% We then for each class $k$, we sample a binary Bernoulli sequence based on $\Ber(\alpha_k)$. If a
% class has all zeros in the Bernoulli sequence, we flip the flag of one of the instances to 1 to make
% sure there is at least one labeled instance for each class.

% \paragraph{Dataset splits}
% We include details about our dataset splits in Table~\ref{tab:omniglotsplit} and
% \ref{tab:matterportsplit}.

% \section{Experiment Details}
% \paragraph{Network architecture:}
% For the \ourchar{} experiment we used the common 4-layer CNN for few-shot learning with 64 channels
% in each layer, resulting in a 64-d feature vector~\cite{protonet}. For the \ourroom{} experiment we
% resize the input to 120$\times$160 and we use the ResNet-12 architecture~\cite{tadam} with
% \{32,64,128,256\} channels per block. To represent the feature of the input image with an attention
% mask, we concatenate the global average pooled feature with the attention ROI feature, resulting in
% a 512d feature vector. For the contextual RNN, in both experiments we used an LSTM~\cite{lstm} with
% a 256d hidden state. 

% We use a linear layer to map from the output of the RNN to the features and control variables. We
% obtain $\gamma^{r,w}$ by adding 1.0 to the linear layer output and then applying the softplus
% activation. The bias units for $\beta^{r,w}$ are initialized to 10.0 for all of our experiments. We
% also apply the softplus activation to $\bm$ from the linear layer output.

% \input{tables/dataset_stats}

% \begin{figure}
% \centering
% % \fbox{
% \includegraphics[width=0.95\textwidth,trim={4.5cm 3cm 4.2cm 10cm},clip]{figures/combined_catv2_counts.pdf}
% % }
% \caption{Additional statistics about our \ourroom{} dataset.}
% \label{fig:additionalstats}
% \end{figure}

% \input{tables/table_omniglot_split}
% \input{tables/table_matterport_split}

% \paragraph{Training details:}
% We use the Adam optimizer~\cite{adam} with initial learning rate 1e-3 for all experiments. For
% Omniglot we train the network for 40k steps with batch size of 32 with maximum sequence length 150
% across 2 GPUs and learning rate decay by 0.1$\times$ at 20k and 30k steps. For Matterport 3D we
% train for 20k steps with batch size 8 with maximum sequence length 100 across 4 GPUs and learning
% rate decay by 0.1$\times$ at 8k and 16k steps. We use BCE coeffcient $\lambda=1$  for all
% experiments. In semisupervised experiments, around 30\% examples are labeled ($\alpha = 0.3$, see
% Eq.~\ref{eq:semisup}).

% \paragraph{Data augmentation details:}
% For \ourchar{}, we pad the 28$\times$28 image to 32$\times$32 and then apply random cropping.

% For \ourroom{}, we apply random cropping in the time dimension to get a chunk of 100 frames per input example. We also apply random dropping of 5\% of the frames. We pad the 120$\times$160 images to 126 $\times$ 168 and apply random cropping in each image frame. We also randomly flip the order of the sequence (going forward or backward).

% \section{Additional Visualization of Experimental Results}

% \paragraph{Video visualization:}
% We include video visualization of \ourroom{} sequences in the supplementary zip folder. The class label is shown on the top left corner, and the CPM model prediction is right below the class label. Labeled objects are shown with red solid boxes, and unlabeled ones are shown with gray dashed boxes. Correct model predictions are colored in green whereas wrong ones are colored in red. 

% \paragraph{Prediction accuracy vs. time:}
% Figure~\ref{fig:acctimefull} shows the prediction accuracy of closed-set classes over time. We
% included supervised settings in addition to the unsupervised settings in the main paper.

% \input{sections/fig_acctime_full}

% \paragraph{Embedding visualization:}
% Figure~\ref{fig:tsne} shows the learned embedding of each examples in Online ProtoNet vs. our CPM
% model in \ourchar{} sequences, where colors indicate the environment ID. In Online ProtoNet, the
% example features does not reflect the temporal context, and as a result, colors are scattered across
% the space. By contrast, in the CPM embedding visualization, colors are clustered together and we see
% a smoother transition of environments in the embedding space.
% \input{sections/fig_tsne}

% \paragraph{Control parameters vs. time:}
% Finally we visualize the control parameter values predicted by the RNN in
% Figure~\ref{fig:betagamma}. We verify that we indeed need two sets of $\beta$ and $\gamma$ for read
% and write operations separately as they learn different values. $\beta^w$ is smaller than $\beta^r$
% which means that the network is more conservative when writing to prototypes. $\gamma^w$ grows
% larger over time, which means that the network prefers a softer slope when writing
% to prototypes since in the later stage the prototype memory has already stored enough content and it can grow faster, whereas in the earlier stage, the prototype memory is more conservative to avoid embedding vectors to be assigned to wrong clusters.

% \input{sections/fig_betagamma_time}
\input{sections/appendix}

\clearpage
\newpage
{
\setstretch{0.93}
\bibliography{ref}
\bibliographystyle{iclr2021_conference}
}

\setstretch{1.0}
\end{document}