<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="../style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<body>
    <div class="ribbon">
 
</div>
<h1 id="talks-by-year">Talks by Year</h1>
<hr />
<h2 id="section">2022</h2>
<ul>
<li><p>Meta-learning within a lifetime. NeurIPS 2022 MetaLearn Workshop invited talk. New Orleans, LA. 2022/12. [<a href="https://drive.google.com/file/d/1gA968oKiO1ufAtX3ogGsQbqJVkW0ztry/view?usp=sharing">slides</a>]</p></li>
<li><p>Biologically plausible learning using local activity perturbation. New York University, Center for Data Science. New York, NY. 2022/10. [<a href="https://drive.google.com/file/d/1dR9eHXZPVBh6T_2R_eFZ1vU6fQt42yK-/view?usp=sharing">slides</a>]</p></li>
<li><p>Visual learning in the open world. 19th Conference on Robotics and Vision (CRV) Symposium. Toronto, ON. 2022/06. [<a href="https://drive.google.com/file/d/1cWMBA3nsrwXCuCOCIf5PuYjPSXf25nVa/view?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-1">2021</h2>
<ul>
<li><p>Visual learning in the open world. University of Oxford. Oxford, UK. 2021/11. [<a href="https://drive.google.com/file/d/10_vWl_ETc_dNXFNcyt6Ft-4uvRyxaLAM/view?usp=sharing">slides</a>]</p></li>
<li><p>Visual learning in the open world. Google Brain. Toronto, ON, Canada. 2021/11. [<a href="https://drive.google.com/file/d/10AQdRPe6va2-FxCrPM3bhqKW3FMRvMHU/view?usp=sharing">slides</a>]</p></li>
<li><p>Visual learning in the open world. Stanford University. Stanford, CA, USA. 2021/10. [<a href="https://drive.google.com/file/d/10-WWd-GQ3Udf_IL_d6TIlq738tj_MKtt/view?usp=sharing">slides</a>]</p></li>
<li><p>Visual learning in the open world. Huawei. Markham, ON, Canada. 2021/09. [<a href="https://drive.google.com/file/d/1-tbhhaWudQ1f-CsYXuZp5cfN3HE5W0Gr/view?usp=sharing">slides</a>]</p></li>
<li><p>Steps towards making machine learning more natural. Job talk. 2021/02 - 2021/04. [<a href="https://drive.google.com/file/d/178bE_0I_ZdSaxm6OsoxEVNJ9d12XCECS/view?usp=sharing">slides</a>]</p></li>
<li><p>A tutorial on few-shot learning and unsupervised representation learning. Vector Institute. Toronto, ON, Canada. 2021/01. [<a href="https://drive.google.com/file/d/1T-pBg9otkmsJIU6aCaaSEnQ7Ty0Gvhj2/view?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-2">2020</h2>
<ul>
<li><p>How can we apply few-shot learning? Vector Institute. Toronto, ON, Canada. 2020/10. [<a href="https://drive.google.com/file/d/1ih28GXJmf18EpghEq716sIL0F4BkghJS/view?usp=sharing">slides</a>]</p></li>
<li><p>Towards continual and compositional few-shot learning. Stanford University. Stanford, CA, USA. 2020/10. [<a href="https://drive.google.com/file/d/1Y8jXp0wTlWqn9pBE97btRJX7FutQOqP1/view?usp=sharing">slides</a>]</p></li>
<li><p>Towards continual and compositional few-shot learning. Brown University. Providence, RI, USA. 2020/09. [<a href="https://drive.google.com/file/d/1GjiRkDnMol3PdoxLKb5q7Oy4rDMnC0kT/view?usp=sharing">slides</a>]</p></li>
<li><p>Towards continual and compositional few-shot learning. MIT. Cambridge, MA, USA. 2020/09. [<a href="https://drive.google.com/file/d/16GXux_cX6AahqQ2yLQIWtEP8AdpDKezA/view?usp=sharing">slides</a>] [<a href="https://www.youtube.com/watch?v=PhKBAkINm40">video</a>]</p></li>
<li><p>Towards continual and compositional few-shot learning. Mila. Montréal, QC, Canada. 2020/08. [<a href="https://drive.google.com/file/d/1LNXPTJEPhzK-wNPJrev-9EaButZrYRfr/view?usp=sharing">slides</a>]</p></li>
<li><p>Wandering within a world: Online contextualized few-shot learning (with M. Mozer). Google Brain. Montréal, QC, Canada. 2020/08. [<a href="https://drive.google.com/file/d/1a3ceBDpyMqs8oF9WzSLjAj8-RGALdu4h/view?usp=sharing">slides</a>]</p></li>
<li><p>Wandering within a world: Online contextualized few-shot learning. ICML 2020 Lifelong Learning Workshop. Virtual webinar. 2020/07. [<a href="https://drive.google.com/file/d/1SJusk2ILF-I3q3RGSU2nEz_6BYG6GY_Q/view?usp=sharing">slides</a>]</p></li>
<li><p>Wandering within a world: Online contextualized few-shot learning. ICML 2020 Continual Learning Workshop. Virtual webinar. 2020/07. [<a href="https://drive.google.com/file/d/1HhXSVx7pJsSp1LO7W880BqMZfFSo48od/view?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-3">2019</h2>
<ul>
<li><p>Jointly learnable behavior and trajectory planning for self-driving vehicles. IROS 2019. Macau, China. 2019/11. [<a href="https://drive.google.com/file/d/1QzrgV5uHaoEpEMUrbPrlHE_nERHfuNvT/view?usp=sharing">slides</a>]</p></li>
<li><p>Meta-learning for more human-like learning algorithms. Columbia University, Department of Statistics. New York, NY, USA. 2019/10. [<a href="https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-4">2018</h2>
<ul>
<li><p>Learning to reweight examples for robust deep learning. CIFAR deep learning and reinforcement learning summer school. Toronto, Ontario, Canada. 2018/08. [<a href="https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing">slides</a>]</p></li>
<li><p>Meta-learning for weakly supervised learning. INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07. [<a href="https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing">slides</a>]</p></li>
<li><p>Learning to reweight examples for robust deep learning. ICML 2018. Stockholm, Sweden. 2018/07. [<a href="https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing">slides</a>] [<a href="https://vimeo.com/287808016">video</a>]</p></li>
<li><p>Meta-learning and learning to reweight examples. Max Planck Institute for Intelligent Systems. Tübingen, Germany. 2018/06. [<a href="https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing">slides</a>]</p></li>
<li><p>Meta-learning for weakly supervised learning. NEC Laboratories America. Princeton, NJ, USA. 2018/06. [<a href="https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing">slides</a>]</p></li>
<li><p>SBNet: Sparse blocks network for fast inference. Borealis AI Lab. Toronto, ON, Canada. 2018/02. [<a href="https://docs.google.com/presentation/d/1mTo8Dv3BjQwh2lNerLnwQgsa4YTrDb-O8kkAN4lcCI4/edit?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-5">2017</h2>
<ul>
<li><p>Meta-learning for semi-supervised few-shot classification. Vector Institute. Toronto, ON, Canada. 2017/11. [<a href="https://docs.google.com/presentation/d/16im80t2tl1mJHyvTrgMmqqWbCvPBBCmALK4tbTJFq-o/edit?usp=sharing">slides</a>]</p></li>
<li><p>End-to-end instance segmentation with recurrent attention. CVPR 2017. Honolulu, HI, USA. 2017/07. [<a href="https://drive.google.com/file/d/1SgLf0Llf9dJWx9YAQAJccVffmhahP6V3/view?usp=sharing">slides</a>] [<a href="https://www.youtube.com/watch?v=oHgUowLph7E">video</a>]</p></li>
<li><p>Sequence-to-sequence deep learning with recurrent attention. Queen’s University. Kingston, ON, Canada. 2017/05. [<a href="https://docs.google.com/presentation/d/1lAKvNL4RWk00Ad4aAInniAkiWxaEwJn7cLoCFVwQuMs/edit?usp=sharing">slides</a>]</p></li>
<li><p>Recurrent neural networks. CSC 2541 Guest Lecture. University of Toronto. Toronto, ON, Canada. 2017/01. [<a href="https://docs.google.com/presentation/d/1cTfhrPa5EFtRsbKXSKv4AAmAi9lZoe0vq0Yt4oZtElc/edit?usp=sharing">slides</a>]</p></li>
</ul>
<hr />
<h2 id="section-6">2016</h2>
<ul>
<li>Deep dashboard tutorial. University of Toronto. 2016/02. University of Guelph. Guelph, ON, Canada. 2016/03. [<a href="https://docs.google.com/presentation/d/1hWINp0UY6aAINjgmWqHmYg_Qtt13DsHL8X6J6xGq1jc/edit?usp=sharing">slides</a>]</li>
</ul>
<hr />
<h2 id="section-7">2015</h2>
<ul>
<li>Exploring data and models for image question answering. ICML 2015 Deep Learning Workshop. Lille, France. 2015/07. [<a href="https://docs.google.com/presentation/d/1jEtaqod5-QgHuK09pQv2U1HrkCxFHiRbBL7a-2C6dfw/edit?usp=sharing">slides</a>]</li>
</ul>
<div class="ribbon">

</div>

</body>
</html>
