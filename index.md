<title>Mengye Ren</title>
<div class="ribbon">&nbsp;</div>
<div class="headdiv">
<div class="txt-panel">
<h1>Mengye Ren</h1>
<p><span class="title">Assistant Professor</span>
<br/>
Department of Computer Science
<br/>
Courant Institute of Mathematical Sciences
<br/>
Center for Data Science (joint)
<br/>
New York University</p>
<p>Email: mengye@nyu.edu
<br/>Tel: +1 (212) 998-3369
<br/>Office: 60 5th Ave, Rm 508, New York, NY, 10011</p>
<p>
<a href="https://www.linkedin.com/in/mengye-ren-593b3546">LinkedIn</a>&nbsp;
<a href="https://twitter.com/mengyer">Twitter</a>&nbsp;
<!-- <a href="https://github.com/renmengye">GitHub</a>&nbsp; -->
<a href="https://www.youtube.com/@mengyetalks">YouTube</a>&nbsp;
<!-- <a href="https://mengyeren.substack.com">Substack</a>&nbsp; -->
<a href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google
Scholar</a>&nbsp;
<a href="cv/cv_mengye_ren.pdf">CV</a>&nbsp;
<!-- <a href="http://blog.mengyer.com">Blog</a> -->
</p>
</div>
<div class="img-panel"><img class="round-pic" src="img/profile_pic3.jpg" /></div>
</div>
</div>

-------------------------------------------------------------------------------

<div class='nav-bar'>
<a href="#bio">Bio</a> |
<a href="#research">Research</a> |
<a href="#teaching">Teaching</a> |
<a href="#news">News</a> |
<a href="#group">Group</a> |
<!--<a href="#preprints">Preprints</a> |-->
<a href="#papers">Selected Papers</a> |
<!--<a href="#soft">Software</a> |-->
<a href="#talks">Selected Talks</a>
<!--<a href="#service">Service</a> -->
<!--<a href="#media">Media</a>-->
</div>

-------------------------------------------------------------------------------

## <a name="bio">Bio</a>

Mengye Ren is an assistant professor of computer science and data science at
New York University (NYU). Before joining NYU, he was a visiting faculty
researcher at Google Brain Toronto working with Prof. [Geoffrey
Hinton](https://www.cs.toronto.edu/~hinton/). He received B.A.Sc. in
Engineering Science (2015), and M.Sc. (2017) and Ph.D. (2021) in Computer
Science from the University of Toronto, advised by Prof. [Richard
Zemel](http://www.cs.toronto.edu/~zemel/) and Prof. [Raquel
Urtasun](http://www.cs.toronto.edu/~urtasun/). From 2017 to 2021, he was also a
senior research scientist at Uber Advanced Technologies Group (ATG) and Waabi,
working on self-driving vehicles. His research focuses on making machine
learning more natural and human-like, in order for AIs to continually learn,
adapt, and reason in naturalistic environments.

-------------------------------------------------------------------------------

## <a name="research">Research</a>

Areas: machine learning, computer vision, representation learning,
meta-learning, few-shot learning, brain & cognitively inspired learning, robot
learning, self-driving vehicles

My key research question is: how do we enable human-like, agent-based machine
intelligence to continually learn, adapt, and reason in naturalistic
environments? I am interested in the emergence of intelligence by learning from
a point-of-view experience. Current research topics in my group are:

* Memorization and forgetting in sequential and continual learning

* Visual representation learning in the wild using egocentric videos

* Few-shot learning, reasoning, and abstraction in vision and language

<!-- Towards this goal of building a more general and flexible AI, my
research has centered on developing *representation learning* and
*meta-learning* algorithms. -->

<!-- Some recent research highlights include:

* Naturalistic paradigms for learning representations, classes, and attributes
  in an online continual data stream and very few labeled examples (few-shot 
  learning FSL): 
  [semi-supervised FSL](https://arxiv.org/abs/1803.00676), 
  [incremental FSL](https://arxiv.org/abs/1810.07218), 
  [online contextualized FSL](https://arxiv.org/abs/2007.04546),
  [attribute FSL](https://arxiv.org/abs/2012.05895),
  [online self-supervised learning](https://arxiv.org/abs/2109.05675)

* Meta-learning algorithms:
  [contextual prototypical memory](https://arxiv.org/abs/2007.04546),
  [unsupervised prototypical memory](https://arxiv.org/abs/2109.05675),
  [learning regularization functions](https://arxiv.org/abs/1810.07218),
  [learning to reweight examples](https://arxiv.org/abs/1803.09050),
  [graph hypernetworks](https://arxiv.org/abs/1810.05749)

* Brain and cognitively inspired representation learning:
  [local activity perturbation](https://arxiv.org/abs/2210.03310),
  [local self-supervised learning](https://arxiv.org/abs/2008.01342),
  [self-supervised learning from video](https://arxiv.org/abs/2101.06553),
  [recurrent attention](https://arxiv.org/abs/1605.09410),
  [learning to imitate drawing](https://arxiv.org/abs/2009.04806),
  [divisive normalization](https://arxiv.org/abs/1611.04520) -->

-------------------------------------------------------------------------------

## <a name="teaching">Teaching</a>

* NYU DS-GA 1008 / CSCI-GA 2572: Deep Learning (2024 spring)

* NYU CSCI-GA 2565: Machine Learning (2023 fall) [[website](https://nyu-cs2565.github.io/2023-fall)]

* NYU DS-GA 1003: Machine Learning (2023 spring) [[website](https://nyu-ds1003.github.io/spring2023)]

* Vector Institute: Deep Learning II (2020 fall)

* UofT CSC 411: Machine Learning and Data Mining (2019 winter)
[[website](teach/csc411_19s)]

-------------------------------------------------------------------------------

## <a name="news">News</a>

* 2023/06: I am co-organizing [Localized Learning Workshop](https://sites.google.com/view/localized-learning-workshop) at ICML 2023.

* 2023/04: One paper is accepted at CogSci 2023.

* 2023/02: One paper is accepted at CVPR 2023.

* 2023/01: Two papers are accepted at ICLR 2023.

* 2022/12: I gave an invited [talk](https://youtu.be/bYZ_lO8nNf0) at NeurIPS 2022 Meta-Learn workshop.

* 2022/10: New [preprint](https://arxiv.org/abs/2210.03310) on biologically
  plausible learning with local activity perturbation.

* 2022/10: One [paper](https://arxiv.org/abs/2210.02615) accepted at MATH-AI workshop at NeurIPS.

* 2022/10: One paper accepted at MemARI workshop at NeurIPS.

* 2022/09: I have moved to New York and officially joined NYU.

<!-- * 2022/07: One [paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990259.pdf) accepted at ECCV 2022.

* 2022/01: I started working at Google Brain Toronto as a visiting faculty researcher.

* 2021/11: I will visit the University of Oxford and give a talk on Nov 17, 2021.

* 2021/10: I will visit Stanford University and give a talk on Oct 20, 2021.

* 2021/10: I defended my Ph.D. thesis ["Open World Machine Learning with
  Limited Labeled
  Data"](https://tspace.library.utoronto.ca/bitstream/1807/123215/2/Ren_Mengye_202206_PhD_thesis.pdf)
  on Oct 19, 2021.

* 2021/09: Two papers [[1](https://arxiv.org/abs/2104.03956),
  [2](https://arxiv.org/abs/2101.06784)] are accepted at CoRL 2021.

* 2021/07: Two papers [[1](https://arxiv.org/abs/2101.06553),
  [2](https://arxiv.org/abs/2101.06560)] are accepted at ICCV 2021.

* 2021/05: I will join as an assistant professor at [NYU Courant Computer
  Science](https://cs.nyu.edu/home/index.html) and [Center for Data
  Science](https://cds.nyu.edu) starting Sept 2022.

* 2021/05: One [paper](https://arxiv.org/abs/2009.04806) is accepted at ICML 2021.

* 2021/02: Two papers [[1](https://arxiv.org/abs/2101.06549),
  [2](https://arxiv.org/abs/2101.06541)] are accepted at CVPR 2021.

* 2021/02: One [paper](https://arxiv.org/abs/2011.01153) is accepted at ICRA 2021.

* 2021/01: Two papers [[1](https://arxiv.org/abs/2007.04546),
  [2](https://arxiv.org/abs/2010.07140)] are accepted at ICLR 2021.

* 2020/10: One [paper](https://arxiv.org/abs/2011.05289) is accepted at CoRL 2020.

* 2020/09: One [paper](https://arxiv.org/abs/2008.01342) is accepted at NeurIPS 2020.

* 2020/09: I will visit Stanford University and give a talk on Oct 12, 2020.

* 2020/09: I will visit Brown University and give a talk on Sept 25, 2020.

* 2020/08: I will visit [MIT](https://sites.google.com/view/visionseminar) and
  give a talk on Sept 22, 2020.

* 2020/08: I will give a talk at [Mila](https://mila.quebec/en/cours/rdv) on
  Aug 28, 2020. -->

<!-- * 2020/07: One [paper](https://arxiv.org/abs/2008.05930) is accepted at ECCV 2020.

* 2020/07: One [paper](https://arxiv.org/abs/2008.05927) is accepted at IROS 2020.

* 2020/06: One [paper](https://arxiv.org/abs/2007.05096) is accepted at ICML 2020.

* 2020/02: One [paper](https://arxiv.org/abs/2004.00543) is accepted at CVPR 2020.

* 2019/09: One [paper](https://arxiv.org/abs/1910.11296) is accepted at CoRL 2019.

* 2019/09: One [paper](https://arxiv.org/abs/1810.07218) is accepted at NeurIPS 2019.

* 2019/09: I will visit [Columbia University](http://stat.columbia.edu/student-seminar-fall-2019) in
  NYC on Oct 9, 2019.

* 2019/06: One [paper](https://arxiv.org/abs/1910.04586) is accepted at IROS 2019.

* 2018/12: One [paper](https://arxiv.org/abs/1810.05749) is accepted at ICLR 2019.

* 2018/10: I will be teaching CSC 411 (Machine Learning and Data Mining) in the winter semester of 2019. [[course website](teach/csc411_19s)]

* 2018/06: I will visit INRIA Grenoble Rhône-Alpes and give a talk on July 19, 2018.

* 2018/06: I will visit TU Berlin on July 16, 2018.

* 2018/05: I will visit NEC lab in Princeton, NJ and give a talk on June 4, 2018.

* 2018/04: I will visit the University of Tübingen and MPI for Intelligent Systems from June 25 to
  July 20, 2018.
 -->

----------------------------------------------------------------------------

## <a name="group">Group</a>

* [Ryan Teehan](https://rteehas.github.io/) (2022-)
* [Alex Wang](https://www.cs.toronto.edu/~alexw/) (2022-)
* [Yanlai Yang](https://yanlai00.github.io/) (2022-)

----------------------------------------------------------------------------

## <a name="papers">Selected Papers</a>
[[Full List](research)]
[[Google Scholar](https://scholar.google.com/citations?user=XcQ9WqMAAAAJ)]
[[dblp](https://dblp.org/pers/hd/r/Ren:Mengye)]

* [Scaling forward gradient with local losses](research/2023/scaling-forward-gradient-with-local-losses).
Mengye Ren, Simon Kornblith, Renjie Liao, Geoffrey Hinton.
<!-- *arXiv preprint 2210.03310*, 2022. -->
*ICLR*, 2023.
[[arxiv](https://arxiv.org/abs/2210.03310)]
[[code](https://github.com/google-research/google-research/tree/master/local_forward_gradient)]
[[html](research/2023/scaling-forward-gradient-with-local-losses)]

* [Online unsupervised learning of visual representations and categories](research/2022/online-unsupervised-learning-of-visual-representations-and-categories).
Mengye Ren, Tyler R. Scott, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel.
*arXiv preprint 2109.05675*, 2022.
[[arxiv](https://arxiv.org/abs/2109.05675)]
[[code](https://github.com/renmengye/online-unsup-proto-net)]
[[html](research/2022/online-unsupervised-learning-of-visual-representations-and-categories)]

* [Self-supervised representation learning from flow equivariance](research/2021/self-supervised-representation-learning-from-flow-equivariance).
Yuwen Xiong, Mengye Ren, Wenyuan Zeng, Raquel Urtasun.
*ICCV*, 2021.
[[arxiv](https://arxiv.org/abs/2101.06553)]
[[html](research/2021/self-supervised-representation-learning-from-flow-equivariance)]

* [SketchEmbedNet: Learning novel concepts by imitating drawings](research/2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings).
Alexander Wang``*``, Mengye Ren``*``, Richard Zemel.
*ICML*, 2021.
[[arxiv](https://arxiv.org/abs/2009.04806)]
[[code](https://github.com/alexnwang/SketchEmbedNet-public)]
[[html](research/2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings)]

* Wandering within a world: Online contextualized few-shot learning.
Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel.
*ICLR*, 2021.
[[arxiv](https://arxiv.org/abs/2007.04546)]
[[code](https://github.com/renmengye/oc-fewshot-public)]
[[video](https://slideslive.com/38931573/wandering-within-a-world-online-contextualized-fewshot-learning)]

* Probing few-shot generalization with attributes.
Mengye Ren``*``, Eleni Triantafillou``*``, Kuan-Chieh Wang``*``, James Lucas``*``, Jake Snell, Xaq Pitkow, Andreas S. Tolias, Richard Zemel.
*arXiv preprint 2012.05895*, 2020.
[[arxiv](https://arxiv.org/abs/2012.05895)]
[[video](https://slideslive.at/38941548/flexible-fewshot-learning-of-contextual-similarities)]

* LoCo: Local contrastive representation learning.
Yuwen Xiong, Mengye Ren, Raquel Urtasun.
*NeurIPS*, 2020.
[[arxiv](https://arxiv.org/abs/2008.01342)]
[[video](https://slideslive.com/38936405/loco-local-contrastive-representation-learning)]

* Multi-agent routing value iteration networks.
Quinlan Sykora``*``, Mengye Ren``*``, Raquel Urtasun.
*ICML*, 2020.
[[arxiv](https://arxiv.org/abs/2007.05096)]
[[code](https://github.com/uber-research/MARVIN)]
[[video](https://slideslive.com/38927801/multiagent-routing-value-iteration-network-marvin)]

* Incremental few-shot learning with attention attractor networks.
Mengye Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel.
*NeurIPS*, 2019.
[[arxiv](https://arxiv.org/abs/1810.07218)]
[[code](https://github.com/renmengye/inc-few-shot-attractor-public)]

* Graph hypernetworks for neural architecture search.
Chris Zhang, Mengye Ren, Raquel Urtasun.
*ICLR*, 2019.
[[arxiv](https://arxiv.org/abs/1810.05749)]

* [Learning to reweight examples for robust deep learning](research/2018/learning-to-reweight-examples-for-robust-deep-learning).
Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun.
*ICML*, 2018.
[[arxiv](https://arxiv.org/abs/1803.09050)]
[[code](https://github.com/uber-research/learning-to-reweight-examples)]
[[video](https://vimeo.com/287808016)]
[[html](research/2018/learning-to-reweight-examples-for-robust-deep-learning)]

* Meta-learning for semi-supervised few-shot classification.
Mengye Ren, Eleni Triantafillou``*``, Sachin Ravi``*``, Jake Snell, Kevin
Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel.
*ICLR*, 2018.
[[link](research/fewshotssl/index.html)]
[[arxiv](https://arxiv.org/abs/1803.00676)]
[[code](https://github.com/renmengye/few-shot-ssl-public)]

* End-to-end instance segmentation with recurrent attention.
Mengye Ren, Richard S. Zemel.
*CVPR*, 2017.
[[link](research/recattend/index.html)]
[[arxiv](https://arxiv.org/abs/1605.09410)]
[[code](https://github.com/renmengye/rec-attend-public)]
[[video](https://www.youtube.com/watch?v=oHgUowLph7E)]

* Exploring models and data for image question answering.
Mengye Ren, Ryan Kiros, Richard S. Zemel.
*NIPS*, 2015.
[[link](research/imageqa/index.html)]
[[arxiv](https://arxiv.org/abs/1505.02074)]
[[results](research/imageqa/results)]
[[dataset](research/imageqa/data/cocoqa)]
[[code](https://github.com/renmengye/imageqa-public)]
[[question generation](https://github.com/renmengye/imageqa-qgen)]


<!--
## <a name="soft">Software</a>
* Forward-mode automatic differentiation for TensorFlow.
[[github]](https://github.com/renmengye/tensorflow-forward-ad)

* Python-based light weight pipeline scheduler for slurm jobs.
[[github]](https://github.com/renmengye/pysched)

* Deep Dashboard: Visualize training process in real time.
[[github](https://github.com/renmengye/deep-dashboard)]
-->

-------------------------------------------------------------------------------

## <a name="talks">Selected Talks</a>

[[Full List](talks)]

* Visual learning in the open world / meta-learning within a lifetime.
    * NeurIPS 2022 MetaLearn. New Orleans, LA, USA. 2022/12.
    [[slides](https://drive.google.com/file/d/1gA968oKiO1ufAtX3ogGsQbqJVkW0ztry/view?usp=sharing)] [[video](https://youtu.be/bYZ_lO8nNf0)]
    * University of Oxford. Oxford, UK. 2021/11.
    [[slides](https://drive.google.com/file/d/10_vWl_ETc_dNXFNcyt6Ft-4uvRyxaLAM/view?usp=sharing)]
    * Google Brain. Toronto, ON, Canada. 2021/11.
    [[slides](https://drive.google.com/file/d/10AQdRPe6va2-FxCrPM3bhqKW3FMRvMHU/view?usp=sharing)]
    * Stanford University. Stanford, CA, USA. 2021/10.
    [[slides](https://drive.google.com/file/d/10-WWd-GQ3Udf_IL_d6TIlq738tj_MKtt/view?usp=sharing)]

* Towards continual and compositional few-shot learning.
    * Stanford University. Stanford, CA, USA. 2020/10.
    [[slides](https://drive.google.com/file/d/1Y8jXp0wTlWqn9pBE97btRJX7FutQOqP1/view?usp=sharing)]
    * Brown University. Providence, RI, USA. 2020/09.
    [[slides](https://drive.google.com/file/d/1GjiRkDnMol3PdoxLKb5q7Oy4rDMnC0kT/view?usp=sharing)]
    * MIT. Cambridge, MA, USA. 2020/09.
    [[slides](https://drive.google.com/file/d/16GXux_cX6AahqQ2yLQIWtEP8AdpDKezA/view?usp=sharing)]
    [[video](https://www.youtube.com/watch?v=PhKBAkINm40)]
    * Mila. Montréal, QC, Canada. 2020/08.
    [[slides](https://drive.google.com/file/d/1LNXPTJEPhzK-wNPJrev-9EaButZrYRfr/view?usp=sharing)]

* Meta-learning for more human-like learning algorithms.
    * Columbia University. New York, NY, USA. 2019/10.
    [[slides](https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing)]
    * INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07.
    [[slides](https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing)]
    * Max Planck Institute for Intelligent Systems. Tübingen, Germany. 2018/06.
    [[slides](https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing)]
    * NEC Laboratories America. Princeton, NJ, USA. 2018/06.
    [[slides](https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing)]

<!-- * A tutorial on few-shot learning and unsupervised representation learning.
Vector Institute. Toronto, ON, Canada. 2021/01. -->

<!--
## <a name="talks">Talks</a>

* A tutorial on few-shot learning and unsupervised representation learning.
Vector Institute. Toronto, ON, Canada. 2021/01.

* How can we apply few-shot learning?
Vector Institute. Toronto, ON, Canada. 2020/10.

* Towards continual and compositional few-shot learning.
Stanford University. Stanford, CA, USA. 2020/10.

* Towards continual and compositional few-shot learning.
Brown University. Providence, RI, USA. 2020/09.

* Towards continual and compositional few-shot learning.
MIT. Cambridge, MA, USA. 2020/09.

* Towards continual and compositional few-shot learning.
Mila. Montréal, Québec, Canada. 2020/08.

* Towards continual and compositional few-shot learning.
Uber ATG. Toronto, Ontario, Canada. 2020/08.

* Wandering within a world: Online contextualized few-shot learning.
Google Brain. Montréal, Québec, Canada. 2020/08.

* Wandering within a world: Online contextualized few-shot learning.
ICML 2020 Lifelong Learning Workshop. Virtual webinar. 2020/07.
[[slides](https://drive.google.com/file/d/1SJusk2ILF-I3q3RGSU2nEz_6BYG6GY_Q/view?usp=sharing)]

* Wandering within a world: Online contextualized few-shot learning.
ICML 2020 Continual Learning Workshop. Virtual webinar. 2020/07.
[[slides](https://drive.google.com/file/d/1HhXSVx7pJsSp1LO7W880BqMZfFSo48od/view?usp=sharing)]

* Jointly learnable behavior and trajectory planning for self-driving vehicles.
IROS 2019. Macau, China. 2019/11.
[[slides](https://drive.google.com/file/d/1QzrgV5uHaoEpEMUrbPrlHE_nERHfuNvT/view?usp=sharing)]

* Meta-learning for more human-like learning algorithms.
Columbia University, Department of Statistics. New York, NY, USA. 2019/10.
[[slides](https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing)]

* Learning to reweight examples for robust deep learning.
CIFAR deep learning and reinforcement learning summer school. Toronto, Ontario, Canada. 2018/08.
[[slides](https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing)]

* Meta-learning for weakly supervised learning.
INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07.
[[slides](https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing)]

* Learning to reweight examples for robust deep learning. ICML 2018. Stockholm, Sweden. 2018/07.
[[slides](https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing)]
[[video](https://vimeo.com/287808016)]

* Meta-learning and learning to reweight examples.
Max Planck Institute for Intelligent Systems. Tübingen, Germany. 2018/06.
[[slides](https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing)]

* Meta-learning for weakly supervised learning.
NEC Laboratories America. Princeton, NJ, USA. 2018/06.
[[slides](https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing)]

* SBNet: Sparse blocks network for fast inference.
Borealis AI Lab. Toronto, ON, Canada. 2018/02.
[[slides](https://docs.google.com/presentation/d/1mTo8Dv3BjQwh2lNerLnwQgsa4YTrDb-O8kkAN4lcCI4/edit?usp=sharing)]

* Meta-learning for semi-supervised few-shot classification.
Vector Institute. Toronto, ON, Canada. 2017/11.
[[slides](https://docs.google.com/presentation/d/16im80t2tl1mJHyvTrgMmqqWbCvPBBCmALK4tbTJFq-o/edit?usp=sharing)]

* End-to-end instance segmentation with recurrent attention.
CVPR 2017. Honolulu, HI, USA. 2017/07.
[[video](https://www.youtube.com/watch?v=oHgUowLph7E)]

* Sequence-to-sequence deep learning with recurrent attention.
Queen's University. Kingston, ON, Canada. 2017/05.
[[slides](https://docs.google.com/presentation/d/1lAKvNL4RWk00Ad4aAInniAkiWxaEwJn7cLoCFVwQuMs/edit?usp=sharing)]

* Recurrent neural networks. CSC 2541 Guest Lecture.
University of Toronto. Toronto, ON, Canada. 2017/01.
[[slides](https://docs.google.com/presentation/d/1cTfhrPa5EFtRsbKXSKv4AAmAi9lZoe0vq0Yt4oZtElc/edit?usp=sharing)]

* Deep dashboard tutorial. University of Toronto. 2016/02.
University of Guelph. Guelph, ON, Canada. 2016/03.
[[slides](https://docs.google.com/presentation/d/1hWINp0UY6aAINjgmWqHmYg_Qtt13DsHL8X6J6xGq1jc/edit?usp=sharing)]

* Exploring data and models for image question answering.
ICML 2015 Deep Learning Workshop. Lille, France. 2015/07.
[[slides](https://docs.google.com/presentation/d/1jEtaqod5-QgHuK09pQv2U1HrkCxFHiRbBL7a-2C6dfw/edit?usp=sharing)]
-->

<!--
------------------------------------------------------------------------------->

<!--## <a name="service">Service</a>

* Journal reviewer:
[IEEE TPAMI](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)
[IEEE TIP](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83)
[IEEE TNNLS](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385)
[IEEE TCI](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852)
[Optim Method Softw](https://www.tandfonline.com/toc/goms20/current)

* Conference reviewer:
[NIPS/NeurIPS](https://nips.cc/) 2016-2020,
[ICML](https://icml.cc/) 2017-2020,
[ICLR](https://iclr.cc/) 2018-2020,
[CVPR](https://www.thecvf.com/) 2018-2020,
[ICCV](https://www.thecvf.com/) 2019,
[ECCV](https://eccv2020.eu/) 2020,
[AAAI](https://www.aaai.org/) 2018,
[UAI](http://auai.org/) 2018
-->

<!--
------------------------------------------------------------------------------->

<!--
## <a name="media">Media</a>

* Autonomous vehicles: U of T researchers make advances with new algorithm.  Nina Haikara. U of T News. 2018/06/21.
[[link](https://www.utoronto.ca/news/autonomous-vehicles-u-t-researchers-make-advances-new-algorithm)]

* Industry | Uber proposed SBNet: Leveraging Activation Block Sparsity for Speeding up Convolutional Neural Networks 业界 | Uber提出SBNet：利用激活的稀疏性加速卷积网络 (Article in Chinese). Synced. 2018/01/18. [[link](https://mp.weixin.qq.com/s/xCzS7sYMFmk5K4ClB1I2YQ)]

* SBNet: Leveraging Activation Block Sparsity for Speeding up Convolutional Neural Networks. Uber Engineering Blog. 2018/01/16. [[link](http://eng.uber.com/sbnet)]
-->
<div class="ribbon"></div>
