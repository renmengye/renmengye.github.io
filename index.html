<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<body>
    <title>
Mengye Ren
</title>
<div class="ribbon">
 
</div>
<div class="headdiv">
<div class="txt-panel">
<h1>
Mengye Ren
</h1>
<p>
<span class="title">Assistant Professor</span> <br/> New York University
</p>
<p>
<span class="title">Visiting Researcher</span> <br/> Google Brain
</p>
<!--Department of Computer Science<br/>
University of Toronto</p>-->
<p>
Email: mengye@cs.nyu.edu
</p>
<p>
<a href="https://www.linkedin.com/in/mengye-ren-593b3546">LinkedIn</a>  <!--<a href="https://twitter.com/mengyer">Twitter</a>&nbsp;--> <a href="https://github.com/renmengye">GitHub</a>  <a href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google Scholar</a>  <a href="cv/cv_mengye_ren.pdf">CV</a>  <!-- <a href="http://blog.mengyer.com">Blog</a> -->
</p>
</div>
<div class="img-panel">
<img class="round-pic" src="img/profile_pic3.jpg" />
</div>
</div>
</div>
<hr />
<div class="nav-bar">
<p><a href="#bio">Bio</a> | <a href="#research">Research</a> | <a href="#news">News</a> | <!--<a href="#preprints">Preprints</a> |--> <a href="#papers">Selected Papers</a> | <a href="#teaching">Teaching</a> | <!--<a href="#soft">Software</a> |--> <a href="#talks">Selected Talks</a> <!--<a href="#service">Service</a> --> <!--<a href="#media">Media</a>--></p>
</div>
<hr />
<h2 id="bio"><a name="bio">Bio</a></h2>
<p>Mengye Ren is an assistant professor of computer science and data science at New York University. He is also a visiting researcher at Google Brain Toronto working with Prof. <a href="https://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>. He received B.A.Sc. in Engineering Science (2015), and M.Sc. (2017) and Ph.D. (2021) in Computer Science from the University of Toronto, advised by Prof.  <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a> and Prof. <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a>. From 2017 to 2021, he was also a senior research scientist at Uber Advanced Technologies Group (ATG) and Waabi. His research focuses on making machine learning more natural and human-like, in order for AIs to continually learn, adapt, and reason in naturalistic environments.</p>
<hr />
<h2 id="research"><a name="research">Research</a></h2>
<p>Areas: machine learning, computer vision, meta-learning, representation learning, few-shot learning, brain &amp; cognitively inspired learning, robot learning, self-driving vehicles</p>
<p>My key research question is: how do we enable human-like, agent-based machine intelligence to continually learn, adapt, and reason in naturalistic environments? Towards this goal of building a more general and flexible AI, my research has centered on developing <em>meta-learning</em> and <em>representation learning</em> algorithms.</p>
<p>Some recent research highlights include:</p>
<ul>
<li><p>Naturalistic paradigms for learning novel classes &amp; attributes with very few examples, i.e. few-shot learning (FSL): <a href="https://arxiv.org/abs/1803.00676">semi-supervised FSL</a>, <a href="https://arxiv.org/abs/1810.07218">incremental FSL</a>, <a href="https://arxiv.org/abs/2007.04546">online contextualized FSL</a>, <a href="https://arxiv.org/abs/2012.05895">attribute FSL</a></p></li>
<li><p>Meta-learning algorithms: <a href="https://arxiv.org/abs/2007.04546">contextual prototypical memory</a>, <a href="https://arxiv.org/abs/1810.07218">learning regularization functions</a>, <a href="https://arxiv.org/abs/1803.09050">learning to reweight examples</a>, <a href="https://arxiv.org/abs/1810.05749">graph hypernetworks</a></p></li>
<li><p>Brain and cognitively inspired representation learning: <a href="https://arxiv.org/abs/2009.04806">learning to imitate drawing</a>, <a href="https://arxiv.org/abs/2101.06553">self-supervised learning from video</a>, <a href="https://arxiv.org/abs/2008.01342">local unsupervised learning</a>, <a href="https://arxiv.org/abs/1605.09410">recurrent attention</a>, <a href="https://arxiv.org/abs/1611.04520">divisive normalization</a></p></li>
</ul>
<hr />
<h2 id="news"><a name="news">News</a></h2>
<ul>
<li><p>2021/11: I will visit the University of Oxford and give a talk on Nov 17, 2021.</p></li>
<li><p>2021/10: I will visit Stanford University and give a talk on Oct 20, 2021.</p></li>
<li><p>2021/10: I defended my Ph.D. thesis <a href="https://drive.google.com/file/d/11ZbCKTNCb9blzkijaXA-QpTXlK7TTPQP/view?usp=sharing">“Open World Machine Learning with Limited Labeled Data”</a> on Oct 19, 2021.</p></li>
<li><p>2021/09: Two papers [<a href="https://arxiv.org/abs/2104.03956">1</a>, <a href="https://arxiv.org/abs/2101.06784">2</a>] are accepted at CoRL 2021.</p></li>
<li><p>2021/07: Two papers [<a href="https://arxiv.org/abs/2101.06553">1</a>, <a href="https://arxiv.org/abs/2101.06560">2</a>] are accepted at ICCV 2021.</p></li>
<li><p>2021/05: I will join as an assistant professor at <a href="https://cs.nyu.edu/home/index.html">NYU Courant Computer Science</a> and <a href="https://cds.nyu.edu">Center for Data Science</a> starting Sept 2022.</p></li>
<li><p>2021/05: One <a href="https://arxiv.org/abs/2009.04806">paper</a> is accepted at ICML 2021.</p></li>
<li><p>2021/02: Two papers [<a href="https://arxiv.org/abs/2101.06549">1</a>, <a href="https://arxiv.org/abs/2101.06541">2</a>] are accepted at CVPR 2021.</p></li>
<li><p>2021/02: One <a href="https://arxiv.org/abs/2011.01153">paper</a> is accepted at ICRA 2021.</p></li>
<li><p>2021/01: Two papers [<a href="https://arxiv.org/abs/2007.04546">1</a>, <a href="https://arxiv.org/abs/2010.07140">2</a>] are accepted at ICLR 2021.</p></li>
<li><p>2020/10: One <a href="https://arxiv.org/abs/2011.05289">paper</a> is accepted at CoRL 2020.</p></li>
<li><p>2020/09: One <a href="https://arxiv.org/abs/2008.01342">paper</a> is accepted at NeurIPS 2020.</p></li>
<li><p>2020/09: I will visit Stanford University and give a talk on Oct 12, 2020.</p></li>
<li><p>2020/09: I will visit Brown University and give a talk on Sept 25, 2020.</p></li>
<li><p>2020/08: I will visit <a href="https://sites.google.com/view/visionseminar">MIT</a> and give a talk on Sept 22, 2020.</p></li>
<li><p>2020/08: I will give a talk at <a href="https://mila.quebec/en/cours/rdv">Mila</a> on Aug 28, 2020.</p></li>
</ul>
<!-- * 2020/07: One [paper](https://arxiv.org/abs/2008.05930) is accepted at ECCV 2020.

* 2020/07: One [paper](https://arxiv.org/abs/2008.05927) is accepted at IROS 2020.

* 2020/06: One [paper](https://arxiv.org/abs/2007.05096) is accepted at ICML 2020.

* 2020/02: One [paper](https://arxiv.org/abs/2004.00543) is accepted at CVPR 2020.

* 2019/09: One [paper](https://arxiv.org/abs/1910.11296) is accepted at CoRL 2019.

* 2019/09: One [paper](https://arxiv.org/abs/1810.07218) is accepted at NeurIPS 2019.

* 2019/09: I will visit [Columbia University](http://stat.columbia.edu/student-seminar-fall-2019) in
  NYC on Oct 9, 2019.

* 2019/06: One [paper](https://arxiv.org/abs/1910.04586) is accepted at IROS 2019.

* 2018/12: One [paper](https://arxiv.org/abs/1810.05749) is accepted at ICLR 2019.

* 2018/10: I will be teaching CSC 411 (Machine Learning and Data Mining) in the winter semester of 2019. [[course website](teach/csc411_19s)]

* 2018/06: I will visit INRIA Grenoble Rhône-Alpes and give a talk on July 19, 2018.

* 2018/06: I will visit TU Berlin on July 16, 2018.

* 2018/05: I will visit NEC lab in Princeton, NJ and give a talk on June 4, 2018.

* 2018/04: I will visit the University of Tübingen and MPI for Intelligent Systems from June 25 to
  July 20, 2018.
 -->
<hr />
<h2 id="selected-papers"><a name="papers">Selected Papers</a></h2>
<p>[<a href="research">Full List</a>] [<a href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google Scholar</a>] [<a href="https://dblp.org/pers/hd/r/Ren:Mengye">dblp</a>]</p>
<ul>
<li><p>Online unsupervised learning of visual representations and categories. Mengye Ren, Tyler R. Scott, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel. <em>arXiv preprint 2109.05675</em>, 2021. [<a href="https://arxiv.org/abs/2109.05675">arxiv</a>]</p></li>
<li><p>Self-supervised representation learning from flow equivariance. Yuwen Xiong, Mengye Ren, Wenyuan Zeng, Raquel Urtasun. <em>ICCV</em>, 2021. [<a href="https://arxiv.org/abs/2101.06553">arxiv</a>]</p></li>
<li><p>SketchEmbedNet: Learning novel concepts by imitating drawings. Alexander Wang<code>*</code>, Mengye Ren<code>*</code>, Richard Zemel. <em>ICML</em>, 2021. [<a href="https://arxiv.org/abs/2009.04806">arxiv</a>]</p></li>
<li><p>Wandering within a world: Online contextualized few-shot learning. Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel. <em>ICLR</em>, 2021. [<a href="https://arxiv.org/abs/2007.04546">arxiv</a>] [<a href="https://github.com/renmengye/oc-fewshot-public">code</a>] [<a href="https://slideslive.com/38931573/wandering-within-a-world-online-contextualized-fewshot-learning">video</a>]</p></li>
<li><p>Probing few-shot generalization with attributes. Mengye Ren<code>*</code>, Eleni Triantafillou<code>*</code>, Kuan-Chieh Wang<code>*</code>, James Lucas<code>*</code>, Jake Snell, Xaq Pitkow, Andreas S. Tolias, Richard Zemel. <em>arXiv preprint 2012.05895</em>, 2020. [<a href="https://arxiv.org/abs/2012.05895">arxiv</a>] [<a href="https://slideslive.at/38941548/flexible-fewshot-learning-of-contextual-similarities">video</a>]</p></li>
<li><p>LoCo: Local contrastive representation learning. Yuwen Xiong, Mengye Ren, Raquel Urtasun. <em>NeurIPS</em>, 2020. [<a href="https://arxiv.org/abs/2008.01342">arxiv</a>] [<a href="https://slideslive.com/38936405/loco-local-contrastive-representation-learning">video</a>]</p></li>
<li><p>Multi-agent routing value iteration networks. Quinlan Sykora<code>*</code>, Mengye Ren<code>*</code>, Raquel Urtasun. <em>ICML</em>, 2020. [<a href="https://arxiv.org/abs/2007.05096">arxiv</a>] [<a href="https://github.com/uber-research/MARVIN">code</a>] [<a href="https://slideslive.com/38927801/multiagent-routing-value-iteration-network-marvin">video</a>]</p></li>
<li><p>Incremental few-shot learning with attention attractor networks. Mengye Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel. <em>NeurIPS</em>, 2019. [<a href="https://arxiv.org/abs/1810.07218">arxiv</a>] [<a href="https://github.com/renmengye/inc-few-shot-attractor-public">code</a>]</p></li>
<li><p>Graph hypernetworks for neural architecture search. Chris Zhang, Mengye Ren, Raquel Urtasun. <em>ICLR</em>, 2019. [<a href="https://arxiv.org/abs/1810.05749">arxiv</a>]</p></li>
<li><p>Learning to reweight examples for robust deep learning. Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun. <em>ICML</em>, 2018. [<a href="https://arxiv.org/abs/1803.09050">arxiv</a>] [<a href="https://github.com/uber-research/learning-to-reweight-examples">code</a>] [<a href="https://vimeo.com/287808016">video</a>]</p></li>
<li><p>Meta-learning for semi-supervised few-shot classification. Mengye Ren, Eleni Triantafillou<code>*</code>, Sachin Ravi<code>*</code>, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel. <em>ICLR</em>, 2018. [<a href="research/fewshotssl/index.html">link</a>] [<a href="https://arxiv.org/abs/1803.00676">arxiv</a>] [<a href="https://github.com/renmengye/few-shot-ssl-public">code</a>]</p></li>
<li><p>End-to-end instance segmentation with recurrent attention. Mengye Ren, Richard S. Zemel. <em>CVPR</em>, 2017. [<a href="research/recattend/index.html">link</a>] [<a href="https://arxiv.org/abs/1605.09410">arxiv</a>] [<a href="https://github.com/renmengye/rec-attend-public">code</a>] [<a href="https://www.youtube.com/watch?v=oHgUowLph7E">video</a>]</p></li>
<li><p>Exploring models and data for image question answering. Mengye Ren, Ryan Kiros, Richard S. Zemel. <em>NIPS</em>, 2015. [<a href="research/imageqa/index.html">link</a>] [<a href="https://arxiv.org/abs/1505.02074">arxiv</a>] [<a href="research/imageqa/results">results</a>] [<a href="research/imageqa/data/cocoqa">dataset</a>] [<a href="https://github.com/renmengye/imageqa-public">code</a>] [<a href="https://github.com/renmengye/imageqa-qgen">question generation</a>]</p></li>
</ul>
<hr />
<h2 id="teaching"><a name="teaching">Teaching</a></h2>
<!--Course instructor:-->
<!--* Vector Institute: Deep learning II (2020 fall)-->
<ul>
<li>UofT CSC 411: Machine learning and data mining (2019 winter) [<a href="teach/csc411_19s">course website</a>]</li>
</ul>
<!--
Teaching assistant:

* UofT ECE 521: Inference algorithms (2017 winter)

* UofT CSC 401/2511: Natural language computing (2016 winter)

* UofT CSC 411/2515: Introduction to machine learning (2015/2016 fall)
  * Naive Bayes and Gaussian Bayes classifier tutorial
[[slides](teach/csc411_15f/nb_gbc_tutorial.pdf)]
[[code](teach/csc411_15f/nb_gbc_tutorial_code.zip)]

* UofT CSC 190: Data structure and algorithm (engineering science) (2014 winter)
-->
<!--
------------------------------------------------------------------------------->
<!--
## <a name="soft">Software</a>
* Forward-mode automatic differentiation for TensorFlow.
[[github]](https://github.com/renmengye/tensorflow-forward-ad)

* Python-based light weight pipeline scheduler for slurm jobs.
[[github]](https://github.com/renmengye/pysched)

* Deep Dashboard: Visualize training process in real time.
[[github](https://github.com/renmengye/deep-dashboard)]
-->
<hr />
<h2 id="selected-talks"><a name="talks">Selected Talks</a></h2>
<p>[<a href="talks">Full List</a>]</p>
<ul>
<li>Visual learning in the open world.
<ul>
<li>University of Oxford. Oxford, UK. 2021/11. [<a href="https://drive.google.com/file/d/10_vWl_ETc_dNXFNcyt6Ft-4uvRyxaLAM/view?usp=sharing">slides</a>]</li>
<li>Google Brain. Toronto, ON, Canada. 2021/11. [<a href="https://drive.google.com/file/d/10AQdRPe6va2-FxCrPM3bhqKW3FMRvMHU/view?usp=sharing">slides</a>]</li>
<li>Stanford University. Stanford, CA, USA. 2021/10. [<a href="https://drive.google.com/file/d/10-WWd-GQ3Udf_IL_d6TIlq738tj_MKtt/view?usp=sharing">slides</a>]</li>
</ul></li>
<li>Towards continual and compositional few-shot learning.
<ul>
<li>Stanford University. Stanford, CA, USA. 2020/10. [<a href="https://drive.google.com/file/d/1Y8jXp0wTlWqn9pBE97btRJX7FutQOqP1/view?usp=sharing">slides</a>]</li>
<li>Brown University. Providence, RI, USA. 2020/09. [<a href="https://drive.google.com/file/d/1GjiRkDnMol3PdoxLKb5q7Oy4rDMnC0kT/view?usp=sharing">slides</a>]</li>
<li>MIT. Cambridge, MA, USA. 2020/09. [<a href="https://drive.google.com/file/d/16GXux_cX6AahqQ2yLQIWtEP8AdpDKezA/view?usp=sharing">slides</a>] [<a href="https://www.youtube.com/watch?v=PhKBAkINm40">video</a>]</li>
<li>Mila. Montréal, QC, Canada. 2020/08. [<a href="https://drive.google.com/file/d/1LNXPTJEPhzK-wNPJrev-9EaButZrYRfr/view?usp=sharing">slides</a>]</li>
</ul></li>
<li>Meta-learning for more human-like learning algorithms.
<ul>
<li>Columbia University. New York, NY, USA. 2019/10. [<a href="https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing">slides</a>]</li>
<li>INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07. [<a href="https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing">slides</a>]</li>
<li>Max Planck Institute for Intelligent Systems. Tübingen, Germany. 2018/06. [<a href="https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing">slides</a>]</li>
<li>NEC Laboratories America. Princeton, NJ, USA. 2018/06. [<a href="https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing">slides</a>]</li>
</ul></li>
</ul>
<!-- * A tutorial on few-shot learning and unsupervised representation learning.
Vector Institute. Toronto, ON, Canada. 2021/01. -->
<!--
## <a name="talks">Talks</a>

* A tutorial on few-shot learning and unsupervised representation learning.
Vector Institute. Toronto, ON, Canada. 2021/01.

* How can we apply few-shot learning?
Vector Institute. Toronto, ON, Canada. 2020/10.

* Towards continual and compositional few-shot learning.
Stanford University. Stanford, CA, USA. 2020/10.

* Towards continual and compositional few-shot learning.
Brown University. Providence, RI, USA. 2020/09.

* Towards continual and compositional few-shot learning.
MIT. Cambridge, MA, USA. 2020/09.

* Towards continual and compositional few-shot learning.
Mila. Montréal, Québec, Canada. 2020/08.

* Towards continual and compositional few-shot learning.
Uber ATG. Toronto, Ontario, Canada. 2020/08.

* Wandering within a world: Online contextualized few-shot learning.
Google Brain. Montréal, Québec, Canada. 2020/08.

* Wandering within a world: Online contextualized few-shot learning.
ICML 2020 Lifelong Learning Workshop. Virtual webinar. 2020/07.
[[slides](https://drive.google.com/file/d/1SJusk2ILF-I3q3RGSU2nEz_6BYG6GY_Q/view?usp=sharing)]

* Wandering within a world: Online contextualized few-shot learning.
ICML 2020 Continual Learning Workshop. Virtual webinar. 2020/07.
[[slides](https://drive.google.com/file/d/1HhXSVx7pJsSp1LO7W880BqMZfFSo48od/view?usp=sharing)]

* Jointly learnable behavior and trajectory planning for self-driving vehicles.
IROS 2019. Macau, China. 2019/11.
[[slides](https://drive.google.com/file/d/1QzrgV5uHaoEpEMUrbPrlHE_nERHfuNvT/view?usp=sharing)]

* Meta-learning for more human-like learning algorithms.
Columbia University, Department of Statistics. New York, NY, USA. 2019/10.
[[slides](https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing)]

* Learning to reweight examples for robust deep learning.
CIFAR deep learning and reinforcement learning summer school. Toronto, Ontario, Canada. 2018/08.
[[slides](https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing)]

* Meta-learning for weakly supervised learning.
INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07.
[[slides](https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing)]

* Learning to reweight examples for robust deep learning. ICML 2018. Stockholm, Sweden. 2018/07.
[[slides](https://drive.google.com/file/d/1jWGJHjpFwjMeHrAtx6vMJ2yg973_hx3t/view?usp=sharing)]
[[video](https://vimeo.com/287808016)]

* Meta-learning and learning to reweight examples.
Max Planck Institute for Intelligent Systems. Tübingen, Germany. 2018/06.
[[slides](https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing)]

* Meta-learning for weakly supervised learning.
NEC Laboratories America. Princeton, NJ, USA. 2018/06.
[[slides](https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing)]

* SBNet: Sparse blocks network for fast inference.
Borealis AI Lab. Toronto, ON, Canada. 2018/02.
[[slides](https://docs.google.com/presentation/d/1mTo8Dv3BjQwh2lNerLnwQgsa4YTrDb-O8kkAN4lcCI4/edit?usp=sharing)]

* Meta-learning for semi-supervised few-shot classification.
Vector Institute. Toronto, ON, Canada. 2017/11.
[[slides](https://docs.google.com/presentation/d/16im80t2tl1mJHyvTrgMmqqWbCvPBBCmALK4tbTJFq-o/edit?usp=sharing)]

* End-to-end instance segmentation with recurrent attention.
CVPR 2017. Honolulu, HI, USA. 2017/07.
[[video](https://www.youtube.com/watch?v=oHgUowLph7E)]

* Sequence-to-sequence deep learning with recurrent attention.
Queen's University. Kingston, ON, Canada. 2017/05.
[[slides](https://docs.google.com/presentation/d/1lAKvNL4RWk00Ad4aAInniAkiWxaEwJn7cLoCFVwQuMs/edit?usp=sharing)]

* Recurrent neural networks. CSC 2541 Guest Lecture.
University of Toronto. Toronto, ON, Canada. 2017/01.
[[slides](https://docs.google.com/presentation/d/1cTfhrPa5EFtRsbKXSKv4AAmAi9lZoe0vq0Yt4oZtElc/edit?usp=sharing)]

* Deep dashboard tutorial. University of Toronto. 2016/02.
University of Guelph. Guelph, ON, Canada. 2016/03.
[[slides](https://docs.google.com/presentation/d/1hWINp0UY6aAINjgmWqHmYg_Qtt13DsHL8X6J6xGq1jc/edit?usp=sharing)]

* Exploring data and models for image question answering.
ICML 2015 Deep Learning Workshop. Lille, France. 2015/07.
[[slides](https://docs.google.com/presentation/d/1jEtaqod5-QgHuK09pQv2U1HrkCxFHiRbBL7a-2C6dfw/edit?usp=sharing)]
-->
<!--
------------------------------------------------------------------------------->
<!--## <a name="service">Service</a>

* Journal reviewer:
[IEEE TPAMI](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)
[IEEE TIP](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83)
[IEEE TNNLS](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385)
[IEEE TCI](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852)
[Optim Method Softw](https://www.tandfonline.com/toc/goms20/current)

* Conference reviewer:
[NIPS/NeurIPS](https://nips.cc/) 2016-2020,
[ICML](https://icml.cc/) 2017-2020,
[ICLR](https://iclr.cc/) 2018-2020,
[CVPR](https://www.thecvf.com/) 2018-2020,
[ICCV](https://www.thecvf.com/) 2019,
[ECCV](https://eccv2020.eu/) 2020,
[AAAI](https://www.aaai.org/) 2018,
[UAI](http://auai.org/) 2018
-->
<!--
------------------------------------------------------------------------------->
<!--
## <a name="media">Media</a>

* Autonomous vehicles: U of T researchers make advances with new algorithm.  Nina Haikara. U of T News. 2018/06/21.
[[link](https://www.utoronto.ca/news/autonomous-vehicles-u-t-researchers-make-advances-new-algorithm)]

* Industry | Uber proposed SBNet: Leveraging Activation Block Sparsity for Speeding up Convolutional Neural Networks 业界 | Uber提出SBNet：利用激活的稀疏性加速卷积网络 (Article in Chinese). Synced. 2018/01/18. [[link](https://mp.weixin.qq.com/s/xCzS7sYMFmk5K4ClB1I2YQ)]

* SBNet: Leveraging Activation Block Sparsity for Speeding up Convolutional Neural Networks. Uber Engineering Blog. 2018/01/16. [[link](http://eng.uber.com/sbnet)]
-->
<div class="ribbon">

</div>

</body>
</html>
