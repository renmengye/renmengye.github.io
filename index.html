<!DOCTYPE html>
<html>
<head>
<meta id="testViewport" name="viewport" content="width=380, initial-scale=0.75, maximum-scale=1, user-scalable=no">
<script>
var sw = screen.width;
var sh = screen.height;
if ( window.matchMedia("(orientation: landscape)").matches ) {
  var fw = sh;
} else {
  var fw = sw;
}
if (fw < 768) {
    var mvp = document.getElementById("testViewport");
    mvp.setAttribute("content","width=380,initial-scale=0.75");
} else {
    var mvp = document.getElementById("testViewport");
    mvp.setAttribute("content","width=device-width,initial-scale=1");
}
</script>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="style.css">
<script>
       (function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){
           (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
       ga("create", "UA-7905505-5", "auto");
       ga("send", "pageview");
</script>
<meta charset="UTF-8">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4CCK7L860L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4CCK7L860L');
</script>
<body>
    <title>
Mengye Ren
</title>
<div class="ribbon">
 
</div>
<div class="headdiv">
<div class="txt-panel">
<h1>
Mengye Ren
</h1>
<p>
<span class="title">Assistant Professor</span> <br/> Department of
Computer Science <br/> Courant Institute of Mathematical Sciences <br/>
Center for Data Science (joint) <br/> New York University
</p>
<p>
Email: mengye@nyu.edu <br/>Tel: +1 (212) 998-3369 <br/>Office: 60 5th
Ave, Rm 508, New York, NY, 10011
</p>
<p>
<a href="https://www.linkedin.com/in/mengye-ren-593b3546">LinkedIn</a> 
<a href="https://twitter.com/mengyer">Twitter</a> 
<!-- <a href="https://github.com/renmengye">GitHub</a>&nbsp; -->
<a href="https://www.youtube.com/@mengyetalks">YouTube</a> 
<!-- <a href="https://mengyeren.substack.com">Substack</a>&nbsp; -->
<a href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google
Scholar</a>  <a href="cv/cv_mengye_ren.pdf">CV</a> 
<!-- <a href="http://blog.mengyer.com">Blog</a> -->
</p>
</div>
<div class="img-panel">
<img class="round-pic" src="img/profile_pic3.jpg" />
</div>
</div>
</div>
<hr />
<div class="nav-bar">
<p><a href="#bio">Bio</a> | <a href="#research">Research</a> |
<a href="#teaching">Teaching</a> | <a href="#news">News</a> |
<a href="#group">Group</a> | <a href="#preprints">Preprints</a> |
<a href="#papers">Papers</a> | <a href="#talks">Talks</a></p>
</div>
<hr />
<h2 id="bio"><a name="bio">Bio</a></h2>
<p>Mengye Ren is an assistant professor of computer science and data
science at New York University (NYU). Before joining NYU, he was a
visiting faculty researcher at Google Brain Toronto working with
Prof. <a href="https://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>.
He received B.A.Sc. in Engineering Science (2015), and M.Sc. (2017) and
Ph.D. (2022) in Computer Science from the University of Toronto, advised
by Prof. <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
and Prof. <a href="http://www.cs.toronto.edu/~urtasun/">Raquel
Urtasun</a>. From 2017 to 2021, he was also a senior research scientist
at Uber Advanced Technologies Group (ATG) and Waabi, working on
self-driving vehicles. His research focuses on making machine learning
more natural and human-like, in order for AIs to continually learn,
adapt, and reason in naturalistic environments.</p>
<hr />
<h2 id="research"><a name="research">Research</a></h2>
<p>Areas: machine learning, computer vision, representation learning,
meta-learning, few-shot learning, brain &amp; cognitively inspired
learning, robot learning, self-driving vehicles</p>
<p>My key research question is: how do we enable human-like, agent-based
machine intelligence to continually learn, adapt, and reason in
naturalistic environments? I am interested in the emergence of
intelligence by learning from a point-of-view experience. Current
research topics in my group are:</p>
<ul>
<li><p>Memorization and forgetting in sequentially changing
environments</p></li>
<li><p>Visual representation learning in the wild using egocentric
videos</p></li>
<li><p>Few-shot learning, reasoning, and abstraction in vision and
language</p></li>
<li><p>Human-AI alignment in personalized AI</p></li>
</ul>
<!-- Towards this goal of building a more general and flexible AI, my
research has centered on developing *representation learning* and
*meta-learning* algorithms. -->
<!-- Some recent research highlights include:

* Naturalistic paradigms for learning representations, classes, and attributes
  in an online continual data stream and very few labeled examples (few-shot 
  learning FSL): 
  [semi-supervised FSL](https://arxiv.org/abs/1803.00676), 
  [incremental FSL](https://arxiv.org/abs/1810.07218), 
  [online contextualized FSL](https://arxiv.org/abs/2007.04546),
  [attribute FSL](https://arxiv.org/abs/2012.05895),
  [online self-supervised learning](https://arxiv.org/abs/2109.05675)

* Meta-learning algorithms:
  [contextual prototypical memory](https://arxiv.org/abs/2007.04546),
  [unsupervised prototypical memory](https://arxiv.org/abs/2109.05675),
  [learning regularization functions](https://arxiv.org/abs/1810.07218),
  [learning to reweight examples](https://arxiv.org/abs/1803.09050),
  [graph hypernetworks](https://arxiv.org/abs/1810.05749)

* Brain and cognitively inspired representation learning:
  [local activity perturbation](https://arxiv.org/abs/2210.03310),
  [local self-supervised learning](https://arxiv.org/abs/2008.01342),
  [self-supervised learning from video](https://arxiv.org/abs/2101.06553),
  [recurrent attention](https://arxiv.org/abs/1605.09410),
  [learning to imitate drawing](https://arxiv.org/abs/2009.04806),
  [divisive normalization](https://arxiv.org/abs/1611.04520) -->
<hr />
<h2 id="teaching"><a name="teaching">Teaching</a></h2>
<ul>
<li><p>NYU DS-GA 1008 / CSCI-GA 2572: Deep Learning [<a
href="https://drive.google.com/drive/folders/12OMYhwWql5EH_jOrmqI3j2QY9GjNLX6N?usp=sharing">2024
spring</a>]</p></li>
<li><p>NYU CSCI-GA 2565: Machine Learning [<a
href="https://nyu-cs2565.github.io/2023-fall">2023 fall</a>]</p></li>
<li><p>NYU DS-GA 1003: Machine Learning [<a
href="https://nyu-ds1003.github.io/spring2023">2023 spring</a>]</p></li>
<li><p>Vector Institute: Deep Learning II [2020 fall]</p></li>
<li><p>UofT CSC 411: Machine Learning and Data Mining [<a
href="teach/csc411_19s">2019 winter</a>]</p></li>
</ul>
<hr />
<h2 id="news"><a name="news">News</a></h2>
<ul>
<li><p>2024/04: I will give an invited <a
href="https://www.simonsfoundation.org/event/machine-learning-at-the-flatiron-institute-seminar-mengye-ren/">talk</a>
at Simons Flatiron Institute in New York.</p></li>
<li><p>2024/04: One paper is accepted at CoLLAs 2024.</p></li>
<li><p>2024/04: One <a href="https://arxiv.org/abs/2402.00300">paper</a>
is accepted at CogSci 2024.</p></li>
<li><p>2024/04: I gave an invited <a
href="https://www.northamerica.uaruhr.de/nyc/events/2024/event00228.html.en">talk</a>
at the German Consulate General in New York.</p></li>
<li><p>2023/09: Congrats <a href="https://alexnwang.github.io/">Alex
Wang</a> on getting the NSERC PGS-D award!</p></li>
<li><p>2023/06: I am co-organizing <a
href="https://sites.google.com/view/localized-learning-workshop">Localized
Learning Workshop</a> at ICML 2023.</p></li>
<li><p>2023/04: One <a
href="research/2023/multitask-learning-via-interleaving-a-neural-network-investigation/mayo-2023-multitask.pdf">paper</a>
is accepted at CogSci 2023.</p></li>
<li><p>2023/02: One <a href="https://arxiv.org/abs/2311.02007">paper</a>
is accepted at CVPR 2023.</p></li>
<li><p>2023/01: Two papers [<a
href="https://arxiv.org/abs/2210.03310">1</a>, <a
href="research/2023/learning-in-temporally-structured-environments/jones-2023-learning.pdf">2</a>]
are accepted at ICLR 2023.</p></li>
<li><p>2022/12: I gave an invited <a
href="https://youtu.be/bYZ_lO8nNf0">talk</a> at NeurIPS 2022 Meta-Learn
workshop.</p></li>
<li><p>2022/10: New <a
href="https://arxiv.org/abs/2210.03310">preprint</a> on biologically
plausible learning with local activity perturbation.</p></li>
<li><p>2022/10: One <a href="https://arxiv.org/abs/2210.02615">paper</a>
is accepted at MATH-AI workshop at NeurIPS.</p></li>
<li><p>2022/10: One <a
href="research/2022/neural-network-online-training-with-sensitivity-to-multiscale-temporal-structure/jones-2022-neural.pdf">paper</a>
is accepted at MemARI workshop at NeurIPS.</p></li>
<li><p>2022/09: I have moved to New York and officially joined
NYU.</p></li>
</ul>
<!-- * 2022/07: One [paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990259.pdf) accepted at ECCV 2022.

* 2022/01: I started working at Google Brain Toronto as a visiting faculty researcher.

* 2021/11: I will visit the University of Oxford and give a talk on Nov 17, 2021.

* 2021/10: I will visit Stanford University and give a talk on Oct 20, 2021.

* 2021/10: I defended my Ph.D. thesis ["Open World Machine Learning with
  Limited Labeled
  Data"](https://tspace.library.utoronto.ca/bitstream/1807/123215/2/Ren_Mengye_202206_PhD_thesis.pdf)
  on Oct 19, 2021.

* 2021/09: Two papers [[1](https://arxiv.org/abs/2104.03956),
  [2](https://arxiv.org/abs/2101.06784)] are accepted at CoRL 2021.

* 2021/07: Two papers [[1](https://arxiv.org/abs/2101.06553),
  [2](https://arxiv.org/abs/2101.06560)] are accepted at ICCV 2021.

* 2021/05: I will join as an assistant professor at [NYU Courant Computer
  Science](https://cs.nyu.edu/home/index.html) and [Center for Data
  Science](https://cds.nyu.edu) starting Sept 2022.

* 2021/05: One [paper](https://arxiv.org/abs/2009.04806) is accepted at ICML 2021.

* 2021/02: Two papers [[1](https://arxiv.org/abs/2101.06549),
  [2](https://arxiv.org/abs/2101.06541)] are accepted at CVPR 2021.

* 2021/02: One [paper](https://arxiv.org/abs/2011.01153) is accepted at ICRA 2021.

* 2021/01: Two papers [[1](https://arxiv.org/abs/2007.04546),
  [2](https://arxiv.org/abs/2010.07140)] are accepted at ICLR 2021.

* 2020/10: One [paper](https://arxiv.org/abs/2011.05289) is accepted at CoRL 2020.

* 2020/09: One [paper](https://arxiv.org/abs/2008.01342) is accepted at NeurIPS 2020.

* 2020/09: I will visit Stanford University and give a talk on Oct 12, 2020.

* 2020/09: I will visit Brown University and give a talk on Sept 25, 2020.

* 2020/08: I will visit [MIT](https://sites.google.com/view/visionseminar) and
  give a talk on Sept 22, 2020.

* 2020/08: I will give a talk at [Mila](https://mila.quebec/en/cours/rdv) on
  Aug 28, 2020. -->
<!-- * 2020/07: One [paper](https://arxiv.org/abs/2008.05930) is accepted at ECCV 2020.

* 2020/07: One [paper](https://arxiv.org/abs/2008.05927) is accepted at IROS 2020.

* 2020/06: One [paper](https://arxiv.org/abs/2007.05096) is accepted at ICML 2020.

* 2020/02: One [paper](https://arxiv.org/abs/2004.00543) is accepted at CVPR 2020.

* 2019/09: One [paper](https://arxiv.org/abs/1910.11296) is accepted at CoRL 2019.

* 2019/09: One [paper](https://arxiv.org/abs/1810.07218) is accepted at NeurIPS 2019.

* 2019/09: I will visit [Columbia University](http://stat.columbia.edu/student-seminar-fall-2019) in
  NYC on Oct 9, 2019.

* 2019/06: One [paper](https://arxiv.org/abs/1910.04586) is accepted at IROS 2019.

* 2018/12: One [paper](https://arxiv.org/abs/1810.05749) is accepted at ICLR 2019.

* 2018/10: I will be teaching CSC 411 (Machine Learning and Data Mining) in the winter semester of 2019. [[course website](teach/csc411_19s)]

* 2018/06: I will visit INRIA Grenoble Rhône-Alpes and give a talk on July 19, 2018.

* 2018/06: I will visit TU Berlin on July 16, 2018.

* 2018/05: I will visit NEC lab in Princeton, NJ and give a talk on June 4, 2018.

* 2018/04: I will visit the University of Tübingen and MPI for Intelligent Systems from June 25 to
  July 20, 2018.
 -->
<hr />
<h2 id="group"><a name="group">Group</a></h2>
<ul>
<li><a href="https://2016choang.github.io/">Chris Hoang</a> (2023-)</li>
<li><a href="https://jacklu0831.github.io/">Jack Lu</a> (2023-)</li>
<li><a href="https://rteehas.github.io/">Ryan Teehan</a> (2022-)</li>
<li><a href="https://alexnwang.github.io/">Alex Wang</a> (2022-)</li>
<li><a href="https://yanlai00.github.io/">Yanlai Yang</a> (2022-)</li>
</ul>
<hr />
<h2 id="recent-preprints"><a name="preprints">Recent Preprints</a></h2>
<ul>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2403.15362">CoLLEGe: Concept embedding
generation for large language models</a>.</span> Ryan Teehan, Brenden M.
Lake, Mengye Ren. <em>arXiv preprint 2403.15362</em>, 2024. [<a
href="https://arxiv.org/abs/2403.15362">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2403.09613">Reawakening knowledge:
Anticipatory recovery from catastrophic interference via structured
training</a>.</span> Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye
Ren. <em>arXiv preprint 2403.09613</em>, 2024. [<a
href="https://arxiv.org/abs/2403.09613">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2402.00300">Self-supervised learning of
video representations from a child’s perspective</a>.</span> Emin Orhan,
Wentao Wang, Alex N. Wang, Mengye Ren, Brenden M. Lake. <em>arXiv
preprint 2402.00300</em>, 2024. [<a
href="https://arxiv.org/abs/2402.00300">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2312.12736">Learning and forgetting unsafe
examples in large language models</a>.</span> Jiachen Zhao, Zhun Deng,
David Madras, James Zou, Mengye Ren. <em>arXiv preprint 2312.12736</em>,
2023. [<a href="https://arxiv.org/abs/2312.12736">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2312.05269">LifelongMemory: Leveraging LLMs
for answering queries in egocentric videos</a>.</span> Ying Wang, Yanlai
Yang, Mengye Ren. <em>arXiv preprint 2312.05269</em>, 2023. [<a
href="https://lifelongmemory.github.io/">webpage</a>] [<a
href="https://arxiv.org/abs/2312.05269">arxiv</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/2311.17218">BIM: Block-wise self-supervised
learning with masked image modeling</a>.</span> Yixuan Luo, Mengye Ren,
Sai Qian Zhang. <em>arXiv preprint 2311.17218</em>, 2023. [<a
href="https://arxiv.org/abs/2311.17218">arxiv</a>]</p></li>
</ul>
<hr />
<h2 id="selected-papers"><a name="papers">Selected Papers</a></h2>
<p>[<a href="research">Full List</a>] [<a
href="https://scholar.google.com/citations?user=XcQ9WqMAAAAJ">Google
Scholar</a>] [<a
href="https://dblp.org/pers/hd/r/Ren:Mengye">dblp</a>]</p>
<ul>
<li><p><span class="paper-title"><a
href="research/2023/scaling-forward-gradient-with-local-losses">Scaling
forward gradient with local losses</a>.</span> Mengye Ren, Simon
Kornblith, Renjie Liao, Geoffrey Hinton. <em>ICLR</em>, 2023. [<a
href="https://arxiv.org/abs/2210.03310">arxiv</a>] [<a
href="research/2023/scaling-forward-gradient-with-local-losses/ren-2023-scaling.pdf">pdf</a>]
[<a
href="https://github.com/google-research/google-research/tree/master/local_forward_gradient">code</a>]
[<a
href="research/2023/scaling-forward-gradient-with-local-losses">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2022/online-unsupervised-learning-of-visual-representations-and-categories">Online
unsupervised learning of visual representations and
categories</a>.</span> Mengye Ren, Tyler R. Scott, Michael L. Iuzzolino,
Michael C. Mozer, Richard Zemel. <em>arXiv preprint 2109.05675</em>,
2022. [<a href="https://arxiv.org/abs/2109.05675">arxiv</a>] [<a
href="research/2022/online-unsupervised-learning-of-visual-representations-and-categories/ren-2022-online.pdf">pdf</a>]
[<a href="https://github.com/renmengye/online-unsup-proto-net">code</a>]
[<a
href="research/2022/online-unsupervised-learning-of-visual-representations-and-categories">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2021/self-supervised-representation-learning-from-flow-equivariance">Self-supervised
representation learning from flow equivariance</a>.</span> Yuwen Xiong,
Mengye Ren, Wenyuan Zeng, Raquel Urtasun. <em>ICCV</em>, 2021. [<a
href="https://arxiv.org/abs/2101.06553">arxiv</a>] [<a
href="research/2021/self-supervised-representation-learning-from-flow-equivariance/xiong-2021-self.pdf">pdf</a>]
[<a
href="research/2021/self-supervised-representation-learning-from-flow-equivariance">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings">SketchEmbedNet:
Learning novel concepts by imitating drawings</a>.</span> Alexander
Wang<code>*</code>, Mengye Ren<code>*</code>, Richard Zemel.
<em>ICML</em>, 2021. [<a
href="https://arxiv.org/abs/2009.04806">arxiv</a>] [<a
href="research/2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings/wang-2021-sketch.pdf">pdf</a>]
[<a href="https://github.com/alexnwang/SketchEmbedNet-public">code</a>]
[<a
href="research/2021/sketch-embed-net-learning-novel-concepts-by-imitating-drawings">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2021/wandering-within-a-world-online-contextualized-few-shot-learning">Wandering
within a world: Online contextualized few-shot learning</a>.</span>
Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard Zemel.
<em>ICLR</em>, 2021. [<a
href="https://arxiv.org/abs/2007.04546">arxiv</a>] [<a
href="research/2021/wandering-within-a-world-online-contextualized-few-shot-learning/ren-2021-wandering.pdf">pdf</a>]
[<a href="https://github.com/renmengye/oc-fewshot-public">code</a>] [<a
href="https://slideslive.com/38931573/wandering-within-a-world-online-contextualized-fewshot-learning">video</a>]
[<a
href="research/2021/wandering-within-a-world-online-contextualized-few-shot-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2020/probing-few-shot-generalization-with-attributes">Probing
few-shot generalization with attributes</a>.</span> Mengye
Ren<code>*</code>, Eleni Triantafillou<code>*</code>, Kuan-Chieh
Wang<code>*</code>, James Lucas<code>*</code>, Jake Snell, Xaq Pitkow,
Andreas S. Tolias, Richard Zemel. <em>arXiv preprint 2012.05895</em>,
2020. [<a href="https://arxiv.org/abs/2012.05895">arxiv</a>] [<a
href="2022/probing-few-shot-generalization-with-attributes/ren-2022-probing.pdf">pdf</a>]
[<a
href="https://slideslive.at/38941548/flexible-fewshot-learning-of-contextual-similarities">video</a>]
[<a
href="research/2022/probing-few-shot-generalization-with-attributes">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2020/loco-local-contrastive-representation-learning">LoCo:
Local contrastive representation learning</a>.</span> Yuwen Xiong,
Mengye Ren, Raquel Urtasun. <em>NeurIPS</em>, 2020. [<a
href="https://arxiv.org/abs/2008.01342">arxiv</a>] [<a
href="research/2020/loco-local-contrastive-representation-learning/xiong-2020-loco.pdf">pdf</a>]
[<a
href="https://slideslive.com/38936405/loco-local-contrastive-representation-learning">video</a>]
[<a
href="research/2020/loco-local-contrastive-representation-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2020/multi-agent-routing-value-iteration-network">Multi-agent
routing value iteration network</a>.</span> Quinlan
Sykora<code>*</code>, Mengye Ren<code>*</code>, Raquel Urtasun.
<em>ICML</em>, 2020. [<a
href="https://arxiv.org/abs/2007.05096">arxiv</a>] [<a
href="research/2020/multi-agent-routing-value-iteration-network/sykora-2020-multi.pdf">pdf</a>]
[<a href="https://github.com/uber-research/MARVIN">code</a>] [<a
href="https://slideslive.com/38927801/multiagent-routing-value-iteration-network-marvin">video</a>]
[<a
href="research/2020/multi-agent-routing-value-iteration-network">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2019/incremental-few-shot-learning-with-attention-attractor-networks">Incremental
few-shot learning with attention attractor networks</a>.</span> Mengye
Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel. <em>NeurIPS</em>,
2019. [<a href="https://arxiv.org/abs/1810.07218">arxiv</a>] [<a
href="https://github.com/renmengye/inc-few-shot-attractor-public">code</a>]
[<a
href="research/2019/incremental-few-shot-learning-with-attention-attractor-networks">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2019/graph-hypernetworks-for-neural-architecture-search">Graph
hypernetworks for neural architecture search</a>.</span> Chris Zhang,
Mengye Ren, Raquel Urtasun. <em>ICLR</em>, 2019. [<a
href="https://arxiv.org/abs/1810.05749">arxiv</a>] [<a
href="research/2019/graph-hypernetworks-for-neural-architecture-search">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2018/learning-to-reweight-examples-for-robust-deep-learning">Learning
to reweight examples for robust deep learning</a>.</span> Mengye Ren,
Wenyuan Zeng, Bin Yang, Raquel Urtasun. <em>ICML</em>, 2018. [<a
href="https://arxiv.org/abs/1803.09050">arxiv</a>] [<a
href="https://github.com/uber-research/learning-to-reweight-examples">code</a>]
[<a href="https://vimeo.com/287808016">video</a>] [<a
href="research/2018/learning-to-reweight-examples-for-robust-deep-learning">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="research/2018/meta-learning-for-semi-supervised-few-shot-classification">Meta-learning
for semi-supervised few-shot classification</a>.</span> Mengye Ren,
Eleni Triantafillou<code>*</code>, Sachin Ravi<code>*</code>, Jake
Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S.
Zemel. <em>ICLR</em>, 2018. [<a
href="research/fewshotssl/index.html">link</a>] [<a
href="https://arxiv.org/abs/1803.00676">arxiv</a>] [<a
href="https://github.com/renmengye/few-shot-ssl-public">code</a>] [<a
href="research/2018/meta-learning-for-semi-supervised-few-shot-classification">html</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/1605.09410">End-to-end instance segmentation
with recurrent attention</a>.</span> Mengye Ren, Richard S. Zemel.
<em>CVPR</em>, 2017. [<a href="research/recattend/index.html">link</a>]
[<a href="https://arxiv.org/abs/1605.09410">arxiv</a>] [<a
href="https://github.com/renmengye/rec-attend-public">code</a>] [<a
href="https://www.youtube.com/watch?v=oHgUowLph7E">video</a>]</p></li>
<li><p><span class="paper-title"><a
href="https://arxiv.org/abs/1505.02074">Exploring models and data for
image question answering</a>.</span> Mengye Ren, Ryan Kiros, Richard S.
Zemel. <em>NIPS</em>, 2015. [<a
href="research/imageqa/index.html">link</a>] [<a
href="https://arxiv.org/abs/1505.02074">arxiv</a>] [<a
href="research/imageqa/results">results</a>] [<a
href="research/imageqa/data/cocoqa">dataset</a>] [<a
href="https://github.com/renmengye/imageqa-public">code</a>] [<a
href="https://github.com/renmengye/imageqa-qgen">question
generation</a>]</p></li>
</ul>
<hr />
<h2 id="selected-talks"><a name="talks">Selected Talks</a></h2>
<p>[<a href="talks">Full List</a>]</p>
<ul>
<li>Lifelong learning in structured environments.
<ul>
<li>American Statistical Association. Statistical Learning and Data
Science Webinar. 2023/10. [<a
href="https://drive.google.com/file/d/11vaeustbBEDfFpc9LH_M2Ke3M6UJuJ68/view?usp=sharing">slides</a>]
[<a href="https://youtu.be/XRro25Am0JE">video</a>]</li>
</ul></li>
<li>Visual learning in the open world.
<ul>
<li>NeurIPS 2022 MetaLearn. New Orleans, LA, USA. 2022/12. [<a
href="https://drive.google.com/file/d/1gA968oKiO1ufAtX3ogGsQbqJVkW0ztry/view?usp=sharing">slides</a>]
[<a href="https://youtu.be/bYZ_lO8nNf0">video</a>]</li>
<li>University of Oxford. Oxford, UK. 2021/11. [<a
href="https://drive.google.com/file/d/10_vWl_ETc_dNXFNcyt6Ft-4uvRyxaLAM/view?usp=sharing">slides</a>]</li>
<li>Google Brain. Toronto, ON, Canada. 2021/11. [<a
href="https://drive.google.com/file/d/10AQdRPe6va2-FxCrPM3bhqKW3FMRvMHU/view?usp=sharing">slides</a>]</li>
<li>Stanford University. Stanford, CA, USA. 2021/10. [<a
href="https://drive.google.com/file/d/10-WWd-GQ3Udf_IL_d6TIlq738tj_MKtt/view?usp=sharing">slides</a>]</li>
</ul></li>
<li>Towards continual and compositional few-shot learning.
<ul>
<li>Stanford University. Stanford, CA, USA. 2020/10. [<a
href="https://drive.google.com/file/d/1Y8jXp0wTlWqn9pBE97btRJX7FutQOqP1/view?usp=sharing">slides</a>]</li>
<li>Brown University. Providence, RI, USA. 2020/09. [<a
href="https://drive.google.com/file/d/1GjiRkDnMol3PdoxLKb5q7Oy4rDMnC0kT/view?usp=sharing">slides</a>]</li>
<li>MIT. Cambridge, MA, USA. 2020/09. [<a
href="https://drive.google.com/file/d/16GXux_cX6AahqQ2yLQIWtEP8AdpDKezA/view?usp=sharing">slides</a>]
[<a href="https://www.youtube.com/watch?v=PhKBAkINm40">video</a>]</li>
<li>Mila. Montréal, QC, Canada. 2020/08. [<a
href="https://drive.google.com/file/d/1LNXPTJEPhzK-wNPJrev-9EaButZrYRfr/view?usp=sharing">slides</a>]</li>
</ul></li>
<li>Meta-learning for more human-like learning algorithms.
<ul>
<li>Columbia University. New York, NY, USA. 2019/10. [<a
href="https://drive.google.com/file/d/1S6HgdAMx8_QYz5hcSf4B7tj_ZzwDd1t_/view?usp=sharing">slides</a>]</li>
<li>INRIA Grenoble Rhône-Alpes. Grenoble, France. 2018/07. [<a
href="https://drive.google.com/file/d/1ePaNOzThOL_F7B5SZPPNWpj2IkXcNdkE/view?usp=sharing">slides</a>]</li>
<li>Max Planck Institute for Intelligent Systems. Tübingen, Germany.
2018/06. [<a
href="https://drive.google.com/file/d/1nUqYGh1QKv5eyXsEStBo4bf5pQRbhFsF/view?usp=sharing">slides</a>]</li>
<li>NEC Laboratories America. Princeton, NJ, USA. 2018/06. [<a
href="https://drive.google.com/file/d/14_H34NgmQ6NN8XJkn_lwK_awrypUdQvv/view?usp=sharing">slides</a>]</li>
</ul></li>
</ul>
<div class="ribbon">

</div>

</body>
</html>
